{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a96a9962-e070-49ef-a152-34d76f76264c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-05 17:44:50 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2021-05-05 17:44:50 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | ../../../K...ize/gsd.pt |\n",
      "| pos       | ../../../K...pos/gsd.pt |\n",
      "| lemma     | ../../../K...mma/gsd.pt |\n",
      "| depparse  | ../../../K...rse/gsd.pt |\n",
      "=======================================\n",
      "\n",
      "2021-05-05 17:44:50 INFO: Use device: gpu\n",
      "2021-05-05 17:44:50 INFO: Loading: tokenize\n",
      "2021-05-05 17:44:50 INFO: Loading: pos\n",
      "2021-05-05 17:44:50 INFO: Loading: lemma\n",
      "2021-05-05 17:44:50 INFO: Loading: depparse\n",
      "2021-05-05 17:44:51 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# stanza.download('zh-hant')\n",
    "\n",
    "config = {\n",
    "    'processors': 'tokenize,pos,lemma,depparse', # Comma-separated list of processors to use\n",
    "    'lang': 'zh', # Language code for the language to build the Pipeline in\n",
    "    'tokenize_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/tokenize/gsd.pt', # Processor-specific arguments are set with keys \"{processor_name}_{argument_name}\"\n",
    "    'pos_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pos/gsd.pt',\n",
    "    'pos_pretrain_path':'../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pretrain/gsd.pt',\n",
    "    'lemma_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/lemma/gsd.pt',\n",
    "    'depparse_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/depparse/gsd.pt',\n",
    "    'depparse_pretrain_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pretrain/gsd.pt',\n",
    "}\n",
    "\n",
    "nlp = stanza.Pipeline(**config) # Initialize the pipeline using a configuration dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97175b36-7d95-443f-9a9b-864ef60adf48",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc489881-a0a8-4cc3-adc2-3a740689abbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-06 02:01:59 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2021-05-06 02:01:59 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | ../../../K...ize/gsd.pt |\n",
      "| pos       | ../../../K...pos/gsd.pt |\n",
      "| lemma     | ../../../K...mma/gsd.pt |\n",
      "| depparse  | ../../../K...rse/gsd.pt |\n",
      "=======================================\n",
      "\n",
      "2021-05-06 02:01:59 INFO: Use device: gpu\n",
      "2021-05-06 02:01:59 INFO: Loading: tokenize\n",
      "2021-05-06 02:01:59 INFO: Loading: pos\n",
      "2021-05-06 02:02:00 INFO: Loading: lemma\n",
      "2021-05-06 02:02:00 INFO: Loading: depparse\n",
      "2021-05-06 02:02:00 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# stanza.download('zh-hant')\n",
    "\n",
    "config = {\n",
    "    'processors': 'tokenize,pos,lemma,depparse', # Comma-separated list of processors to use\n",
    "    'lang': 'zh', # Language code for the language to build the Pipeline in\n",
    "    'tokenize_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/tokenize/gsd.pt', # Processor-specific arguments are set with keys \"{processor_name}_{argument_name}\"\n",
    "    'pos_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pos/gsd.pt',\n",
    "    'pos_pretrain_path':'../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pretrain/gsd.pt',\n",
    "    'lemma_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/lemma/gsd.pt',\n",
    "    'depparse_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/depparse/gsd.pt',\n",
    "    'depparse_pretrain_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pretrain/gsd.pt',\n",
    "}\n",
    "\n",
    "nlp = stanza.Pipeline(**config) # Initialize the pipeline using a configuration dict\n",
    "\n",
    "# doc = nlp(\"古往今來，能飾演古龍小說人物楚留香的，無一不是娛樂圈公認的美男子，2011年，36歲的張智堯在，楚留香新傳，裡飾演楚留香，依舊帥得讓人無法自拔\")\n",
    "# print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7233087-66f1-4d4a-bec8-f062a5991623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1 word: 古往 head id: 2 head: 今來 deprel: nmod\n",
      "id: 2 word: 今來 head id: 5 head: 飾演 deprel: nmod:tmod\n",
      "id: 3 word: ， head id: 5 head: 飾演 deprel: punct\n",
      "id: 4 word: 能 head id: 5 head: 飾演 deprel: aux\n",
      "id: 5 word: 飾演 head id: 0 head: root deprel: root\n",
      "id: 6 word: 古龍 head id: 8 head: 人物 deprel: nmod\n",
      "id: 7 word: 小說 head id: 8 head: 人物 deprel: nmod\n",
      "id: 8 word: 人物 head id: 5 head: 飾演 deprel: obj\n",
      "id: 9 word: 楚 head id: 8 head: 人物 deprel: appos\n",
      "id: 10 word: 留香 head id: 9 head: 楚 deprel: flat:name\n",
      "id: 11 word: 的 head id: 5 head: 飾演 deprel: discourse\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"古埃及中王國時期。青銅器廣泛應用，開發法雍湖地區，修建卡爾奈克神廟\")\n",
    "\n",
    "edges = []\n",
    "edges_and_dependencies = []\n",
    "edges_list = []\n",
    "dependencies_list = []\n",
    "object_index = []\n",
    "subject_index = []\n",
    "predicate_index = []\n",
    "result_list = []\n",
    "\n",
    "\n",
    "# for token in doc:\n",
    "#     for child in token.children:\n",
    "#         # append token to construct graph\n",
    "#         edges.append(('{0}'.format(token.lower_),\n",
    "#                               '{0}'.format(child.lower_)))\n",
    "#         edges_and_dependencies.append(('{0}'.format((token.lower_, token.dep_)),\n",
    "#                                                '{0}'.format((child.lower_, child.dep_))))\n",
    "        \n",
    "        \n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(f\"id: {word.id}\", f\"word: {word.text}\", f\"head id: {word.head}\", \n",
    "              f\"head: {sent.words[word.head-1].text if word.head > 0 else 'root'}\", f\"deprel: {word.deprel}\")\n",
    "        edges.append(('{0}'.format(word.id),\n",
    "                              '{0}'.format(word.head)))\n",
    "\n",
    "\n",
    "# # append data for graph searching\n",
    "# edges_list.append(str(token))\n",
    "# dependencies_list.append(token.dep_)\n",
    "\n",
    "    \n",
    "# print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
