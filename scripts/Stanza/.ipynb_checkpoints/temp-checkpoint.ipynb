{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a96a9962-e070-49ef-a152-34d76f76264c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-05 17:44:50 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2021-05-05 17:44:50 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | ../../../K...ize/gsd.pt |\n",
      "| pos       | ../../../K...pos/gsd.pt |\n",
      "| lemma     | ../../../K...mma/gsd.pt |\n",
      "| depparse  | ../../../K...rse/gsd.pt |\n",
      "=======================================\n",
      "\n",
      "2021-05-05 17:44:50 INFO: Use device: gpu\n",
      "2021-05-05 17:44:50 INFO: Loading: tokenize\n",
      "2021-05-05 17:44:50 INFO: Loading: pos\n",
      "2021-05-05 17:44:50 INFO: Loading: lemma\n",
      "2021-05-05 17:44:50 INFO: Loading: depparse\n",
      "2021-05-05 17:44:51 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# stanza.download('zh-hant')\n",
    "\n",
    "config = {\n",
    "    'processors': 'tokenize,pos,lemma,depparse', # Comma-separated list of processors to use\n",
    "    'lang': 'zh', # Language code for the language to build the Pipeline in\n",
    "    'tokenize_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/tokenize/gsd.pt', # Processor-specific arguments are set with keys \"{processor_name}_{argument_name}\"\n",
    "    'pos_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pos/gsd.pt',\n",
    "    'pos_pretrain_path':'../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pretrain/gsd.pt',\n",
    "    'lemma_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/lemma/gsd.pt',\n",
    "    'depparse_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/depparse/gsd.pt',\n",
    "    'depparse_pretrain_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pretrain/gsd.pt',\n",
    "}\n",
    "\n",
    "nlp = stanza.Pipeline(**config) # Initialize the pipeline using a configuration dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97175b36-7d95-443f-9a9b-864ef60adf48",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc489881-a0a8-4cc3-adc2-3a740689abbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-05 18:05:30 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2021-05-05 18:05:30 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | ../../../K...ize/gsd.pt |\n",
      "| pos       | ../../../K...pos/gsd.pt |\n",
      "| lemma     | ../../../K...mma/gsd.pt |\n",
      "| depparse  | ../../../K...rse/gsd.pt |\n",
      "=======================================\n",
      "\n",
      "2021-05-05 18:05:30 INFO: Use device: gpu\n",
      "2021-05-05 18:05:30 INFO: Loading: tokenize\n",
      "2021-05-05 18:05:32 INFO: Loading: pos\n",
      "2021-05-05 18:05:32 INFO: Loading: lemma\n",
      "2021-05-05 18:05:32 INFO: Loading: depparse\n",
      "2021-05-05 18:05:33 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: 古往 \tlemma: 古往\n",
      "word: 今來 \tlemma: 今來\n",
      "word: ， \tlemma: ，\n",
      "word: 能 \tlemma: 能\n",
      "word: 飾演 \tlemma: 飾演\n",
      "word: 古龍 \tlemma: 古龍\n",
      "word: 小說 \tlemma: 小說\n",
      "word: 人物 \tlemma: 人物\n",
      "word: 楚 \tlemma: 楚\n",
      "word: 留香 \tlemma: 留香\n",
      "word: 的 \tlemma: 的\n",
      "word: ， \tlemma: ，\n",
      "word: 無 \tlemma: 無\n",
      "word: 一 \tlemma: 一\n",
      "word: 不是 \tlemma: 不是\n",
      "word: 娛樂 \tlemma: 娛樂\n",
      "word: 圈 \tlemma: 圈\n",
      "word: 公認 \tlemma: 公認\n",
      "word: 的 \tlemma: 的\n",
      "word: 美 \tlemma: 美\n",
      "word: 男子 \tlemma: 男子\n",
      "word: ， \tlemma: ，\n",
      "word: 2011 \tlemma: 2011\n",
      "word: 年 \tlemma: 年\n",
      "word: ， \tlemma: ，\n",
      "word: 36 \tlemma: 36\n",
      "word: 歲 \tlemma: 歲\n",
      "word: 的 \tlemma: 的\n",
      "word: 張 \tlemma: 張\n",
      "word: 智堯 \tlemma: 智堯\n",
      "word: 在 \tlemma: 在\n",
      "word: ， \tlemma: ，\n",
      "word: 楚 \tlemma: 楚\n",
      "word: 留香 \tlemma: 留香\n",
      "word: 新傳 \tlemma: 新傳\n",
      "word: ， \tlemma: ，\n",
      "word: 裡 \tlemma: 裡\n",
      "word: 飾演 \tlemma: 飾演\n",
      "word: 楚 \tlemma: 楚\n",
      "word: 留香 \tlemma: 留香\n",
      "word: ， \tlemma: ，\n",
      "word: 依舊 \tlemma: 依舊\n",
      "word: 帥得 \tlemma: 帥得\n",
      "word: 讓 \tlemma: 讓\n",
      "word: 人 \tlemma: 人\n",
      "word: 無法 \tlemma: 無法\n",
      "word: 自拔 \tlemma: 自拔\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# stanza.download('zh-hant')\n",
    "\n",
    "config = {\n",
    "    'processors': 'tokenize,pos,lemma,depparse', # Comma-separated list of processors to use\n",
    "    'lang': 'zh', # Language code for the language to build the Pipeline in\n",
    "    'tokenize_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/tokenize/gsd.pt', # Processor-specific arguments are set with keys \"{processor_name}_{argument_name}\"\n",
    "    'pos_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pos/gsd.pt',\n",
    "    'pos_pretrain_path':'../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pretrain/gsd.pt',\n",
    "    'lemma_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/lemma/gsd.pt',\n",
    "    'depparse_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/depparse/gsd.pt',\n",
    "    'depparse_pretrain_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pretrain/gsd.pt',\n",
    "}\n",
    "\n",
    "nlp = stanza.Pipeline(**config) # Initialize the pipeline using a configuration dict\n",
    "\n",
    "doc = nlp(\"古往今來，能飾演古龍小說人物楚留香的，無一不是娛樂圈公認的美男子，2011年，36歲的張智堯在，楚留香新傳，裡飾演楚留香，依舊帥得讓人無法自拔\")\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7233087-66f1-4d4a-bec8-f062a5991623",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a7e1895be759>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# append token to construct graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Document' object is not iterable"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"古往今來，能飾演古龍小說人物楚留香的\")\n",
    "\n",
    "edges = []\n",
    "edges_and_dependencies = []\n",
    "edges_list = []\n",
    "dependencies_list = []\n",
    "object_index = []\n",
    "subject_index = []\n",
    "predicate_index = []\n",
    "result_list = []\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    for child in token.children:\n",
    "        # append token to construct graph\n",
    "        edges.append(('{0}'.format(token.lower_),\n",
    "                              '{0}'.format(child.lower_)))\n",
    "        edges_and_dependencies.append(('{0}'.format((token.lower_, token.dep_)),\n",
    "                                               '{0}'.format((child.lower_, child.dep_))))\n",
    "        \n",
    "        \n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(f\"id: {word.id}\", f\"word: {word.text}\", f\"head id: {word.head}\", \n",
    "              f\"head: {sent.words[word.head-1].text if word.head > 0 else 'root'}\", f\"deprel: {word.deprel}\")\n",
    "        edges.append(('{0}'.format(word.id),\n",
    "                              '{0}'.format(word.head)))\n",
    "\n",
    "\n",
    "# # append data for graph searching\n",
    "# edges_list.append(str(token))\n",
    "# dependencies_list.append(token.dep_)\n",
    "\n",
    "    \n",
    "# print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
