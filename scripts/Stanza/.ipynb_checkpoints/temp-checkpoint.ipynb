{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96a9962-e070-49ef-a152-34d76f76264c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-08 15:33:28 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2021-05-08 15:33:28 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | ../../../K...ize/gsd.pt |\n",
      "| pos       | ../../../K...pos/gsd.pt |\n",
      "| lemma     | ../../../K...mma/gsd.pt |\n",
      "| depparse  | ../../../K...rse/gsd.pt |\n",
      "=======================================\n",
      "\n",
      "2021-05-08 15:33:28 INFO: Use device: gpu\n",
      "2021-05-08 15:33:28 INFO: Loading: tokenize\n",
      "2021-05-08 15:33:31 INFO: Loading: pos\n",
      "2021-05-08 15:33:31 INFO: Loading: lemma\n",
      "2021-05-08 15:33:31 INFO: Loading: depparse\n",
      "2021-05-08 15:33:32 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# stanza.download('zh-hant')\n",
    "\n",
    "config = {\n",
    "    'processors': 'tokenize,pos,lemma,depparse', # Comma-separated list of processors to use\n",
    "    'lang': 'zh', # Language code for the language to build the Pipeline in\n",
    "    'tokenize_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/tokenize/gsd.pt', # Processor-specific arguments are set with keys \"{processor_name}_{argument_name}\"\n",
    "    'pos_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pos/gsd.pt',\n",
    "    'pos_pretrain_path':'../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pretrain/gsd.pt',\n",
    "    'lemma_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/lemma/gsd.pt',\n",
    "    'depparse_model_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/depparse/gsd.pt',\n",
    "    'depparse_pretrain_path': '../../../KnowledgeGraph_materials/stanza_resources/zh-hant/pretrain/gsd.pt',\n",
    "}\n",
    "\n",
    "nlp = stanza.Pipeline(**config) # Initialize the pipeline using a configuration dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97175b36-7d95-443f-9a9b-864ef60adf48",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc489881-a0a8-4cc3-adc2-3a740689abbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-06 02:01:59 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2021-05-06 02:01:59 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | ../../../K...ize/gsd.pt |\n",
      "| pos       | ../../../K...pos/gsd.pt |\n",
      "| lemma     | ../../../K...mma/gsd.pt |\n",
      "| depparse  | ../../../K...rse/gsd.pt |\n",
      "=======================================\n",
      "\n",
      "2021-05-06 02:01:59 INFO: Use device: gpu\n",
      "2021-05-06 02:01:59 INFO: Loading: tokenize\n",
      "2021-05-06 02:01:59 INFO: Loading: pos\n",
      "2021-05-06 02:02:00 INFO: Loading: lemma\n",
      "2021-05-06 02:02:00 INFO: Loading: depparse\n",
      "2021-05-06 02:02:00 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# doc = nlp(\"古往今來，能飾演古龍小說人物楚留香的，無一不是娛樂圈公認的美男子，2011年，36歲的張智堯在，楚留香新傳，裡飾演楚留香，依舊帥得讓人無法自拔\")\n",
    "# print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7233087-66f1-4d4a-bec8-f062a5991623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1 word: 古 head id: 2 head: 埃及 deprel: case:pref\n",
      "id: 2 word: 埃及 head id: 14 head: 開始 deprel: nsubj\n",
      "id: 3 word: 、 head id: 4 head: 西南亞 deprel: punct\n",
      "id: 4 word: 西南亞 head id: 2 head: 埃及 deprel: conj\n",
      "id: 5 word: 、 head id: 6 head: 南歐 deprel: punct\n",
      "id: 6 word: 南歐 head id: 2 head: 埃及 deprel: conj\n",
      "id: 7 word: 、 head id: 8 head: 中歐 deprel: punct\n",
      "id: 8 word: 中歐 head id: 2 head: 埃及 deprel: conj\n",
      "id: 9 word: 和 head id: 10 head: 中國 deprel: cc\n",
      "id: 10 word: 中國 head id: 2 head: 埃及 deprel: conj\n",
      "id: 11 word: 等 head id: 2 head: 埃及 deprel: acl\n",
      "id: 12 word: 地 head id: 2 head: 埃及 deprel: appos\n",
      "id: 13 word: 先後 head id: 14 head: 開始 deprel: advmod\n",
      "id: 14 word: 開始 head id: 0 head: root deprel: root\n",
      "id: 15 word: 用 head id: 14 head: 開始 deprel: xcomp\n",
      "id: 16 word: 礦石 head id: 17 head: 煉銅 deprel: nmod\n",
      "id: 17 word: 煉銅 head id: 15 head: 用 deprel: obj\n",
      "id: 18 word: 。 head id: 14 head: 開始 deprel: punct\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"古埃及、西南亞、南歐、中歐和中國等地先後開始用礦石煉銅。\")\n",
    "\n",
    "\n",
    "\n",
    "# for token in doc:\n",
    "#     for child in token.children:\n",
    "#         # append token to construct graph\n",
    "#         edges.append(('{0}'.format(token.lower_),\n",
    "#                               '{0}'.format(child.lower_)))\n",
    "#         edges_and_dependencies.append(('{0}'.format((token.lower_, token.dep_)),\n",
    "#                                                '{0}'.format((child.lower_, child.dep_))))\n",
    "        \n",
    "        \n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(f\"id: {word.id}\", f\"word: {word.text}\", f\"head id: {word.head}\", \n",
    "              f\"head: {sent.words[word.head-1].text if word.head > 0 else 'root'}\", f\"deprel: {word.deprel}\")\n",
    "        edges.append(('{0}'.format(word.id),\n",
    "                              '{0}'.format(word.head)))\n",
    "\n",
    "\n",
    "# # append data for graph searching\n",
    "# edges_list.append(str(token))\n",
    "# dependencies_list.append(token.dep_)\n",
    "\n",
    "    \n",
    "# print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c005aab5-9d99-4b26-85de-2ebfb5569eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: 古\tupos: PART\txpos: PFA\tfeats: _\n",
      "word: 希臘\tupos: PROPN\txpos: NNP\tfeats: _\n",
      "word: 哲學\tupos: NOUN\txpos: NN\tfeats: _\n",
      "word: 家\tupos: PART\txpos: SFN\tfeats: _\n",
      "word: 阿那克西曼德\tupos: PROPN\txpos: NNP\tfeats: _\n",
      "word: 在\tupos: VERB\txpos: VV\tfeats: _\n",
      "word: 世\tupos: NOUN\txpos: NN\tfeats: _\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"古希臘哲學家阿那克西曼德在世\")\n",
    "\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
