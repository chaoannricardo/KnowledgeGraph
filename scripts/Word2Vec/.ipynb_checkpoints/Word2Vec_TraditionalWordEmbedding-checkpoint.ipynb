{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc5af2d",
   "metadata": {},
   "source": [
    "# Traditional Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a07c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "  Welcome to MONPA: Multi-Objective NER POS Annotator for Chinese\n",
      "+---------------------------------------------------------------------+\n",
      "已找到 model檔。Found model file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph\\\\scripts\\\\Word2Vec'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "from gensim.models import word2vec\n",
    "from monpa import utils\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import csv\n",
    "import logging\n",
    "import monpa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efa437",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b4cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ratio = 0.5\n",
    "data_entity = pd.read_csv(\"../../results/210408_result/210408_dataset_entity_result_MONPA.csv\", sep=\",\", encoding=\"utf8\")\n",
    "data_relation = pd.read_csv(\"../../results/210408_result/210408_dataset_relation_result_MONPA.csv\", sep=\",\", encoding=\"utf8\")\n",
    "output_token_for_word2vec = \"../../results/210408_result/210408_token_sentences_for_word2vec.csv\"\n",
    "model_save_path = \"../../models/210408_word2vec_skipgram.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20bee5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONPA\n",
    "relation_dependencies_possible_list = []\n",
    "relation_pos_possible_list = [\"VH\", \"VC\", \"VJ\", \"VA\"]\n",
    "relation_pos_re = \"^[V]\"\n",
    "entity_dependencies_possible_list = []\n",
    "entity_pos_possible_list = [\"Na\", \"Nv\", \"Neu\", \"Nes\", \"Nf\", \"Ng\", \"Nh\", \"Neqa\", \"Nep\", \"Ncd\", \"FW\", \"DE\"]\n",
    "entity_pos_re = \"^[N]\"\n",
    "splitter_pos_list = [\"COMMACATEGORY\", \"PERIODCATEGORY\"]\n",
    "\n",
    "# spaCy\n",
    "# relation_dependencies_possible_list = [\"ROOT\", \"nmod:prep\", \"prep\", \"agent\", ]\n",
    "# relation_pos_possible_list = [\"VERB\"]\n",
    "# relation_pos_re = \"^[V]\"\n",
    "# entity_dependencies_possible_list = [\"compound:nn\", \"nsubj\", \"dep\", \"dobj\"]\n",
    "# entity_pos_possible_list = [\"NOUN\", \"PROPN\"]\n",
    "# entity_pos_re = \"^[N]\"\n",
    "# bracket_pos_list = []\n",
    "\n",
    "# Common, usually entity\n",
    "sentences_splitter = [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\", \"；\", \";\", \".\"] + [\"\\r\\n\" * i for i in range(0, 100)]\n",
    "bracket_entity_list_first = [\"(\", \"（\", \"[\", \"［\", \"{\", \"｛\", \"<\", \"＜\", \"〔\", \"【\", \"〖\", \"《\", \"〈\"]\n",
    "bracket_entity_list_last = [\")\", \"）\", \"]\", \"］\", \"}\", \"｝\", \">\", \"＞\", \"〕\", \"】\", \"〗\", \"》\", \"〉\"]\n",
    "punct_entity_list = [\" \" * i for i in range(0, 100)]\n",
    "conjuction_entity_list = [\"的\", \"、\", \"之\", \"及\", \"與\"]\n",
    "not_entity_relation_list = [\"的\", \"、\", \"之\", \"及\", \"與\", \"\\r\\n \\r\\n \", \"\\r\\n \\r\\n  \"] +\\\n",
    "[\" \" * i for i in range(0, 100)] + [\"\\n\" * i for i in range(0, 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c8892",
   "metadata": {},
   "source": [
    "# Import stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d52286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\ipykernel_launcher.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203299 \n",
      "\n",
      " [\"t's\", 'twice', 'unfortunately', 'unless', 'unlikely', 'unto', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'value', 'various', 'viz', 'vs', 'want', 'wants', \"wasn't\", 'way', \"we'd\", 'welcome', \"we'll\", 'went', \"we're\", \"weren't\", \"we've\", \"what's\", \"where's\", \"who's\", 'willing', 'wish', 'wonder', \"won't\", \"wouldn't\", 'yes', \"you'd\", \"you'll\", \"you're\", \"you've\", 'zero', 'zt', 'ZT', 'zz', 'ZZ', '準備', '覆雜', '心裏', '註意', '裏面']\n"
     ]
    }
   ],
   "source": [
    "stopword_list = []\n",
    "\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"../stopwords/\")):\n",
    "    if fileElement[-2:] != \"md\":\n",
    "        data_temp = pd.read_csv(\"../stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n",
    "        stopword_list += data_temp.iloc[:, 0].tolist()\n",
    "        \n",
    "stopword_list = list(dict.fromkeys(stopword_list))\n",
    "        \n",
    "print(len(stopword_list), \"\\n\\n\", stopword_list[-50:])     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9c185",
   "metadata": {},
   "source": [
    "# Create User-Defined Dict by entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b21378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5304 1610\n",
      "6914\n",
      "['晶圓', '技術', '製程', '晶片', '產品', '成本', '表面', '測品', '過程', '設備'] ['進化', '通稱為', '通道', '速率', '複合', '製造為', '連絡', '製造到', '進 ', '進一步']\n"
     ]
    }
   ],
   "source": [
    "print(int(np.around(len(data_entity) * selected_ratio, decimals=0)), int(np.around(len(data_relation) * selected_ratio, decimals=0)))\n",
    "data_dict = pd.concat([data_entity.iloc[:int(np.around(len(data_entity) * selected_ratio, decimals=0)),:],\\\n",
    "                    data_relation.iloc[:int(np.around(len(data_relation) * selected_ratio, decimals=0)),:]], ignore_index=True)\n",
    "print(len(data_dict))\n",
    "\n",
    "entity_list = data_dict.iloc[:, 0].tolist()\n",
    "\n",
    "print(entity_list[:10], entity_list[-10:])\n",
    "\n",
    "# remove blank inside entity element\n",
    "for elementIndex, element in enumerate(entity_list):\n",
    "    entity_list[elementIndex] = str(entity_list[elementIndex]).replace(\" \", \"\")\n",
    "    entity_list[elementIndex] = str(entity_list[elementIndex]).replace(\"  \", \"\")\n",
    "    entity_list[elementIndex] = str(entity_list[elementIndex]).replace(\"\\\"\", \"\")\n",
    "\n",
    "for i in range(len(entity_list)):\n",
    "    if type(data_dict.loc[i, \"Ratio\"]) == pd.Series:\n",
    "        print(data_dict.loc[i, \"Ratio\"])\n",
    "    \n",
    "    \n",
    "data_dict_output = pd.DataFrame({\n",
    "    \"0\": entity_list,\n",
    "    \"1\": [int(1000000000 * data_dict.loc[i, \"Ratio\"]) for i in range(len(entity_list))],\n",
    "    \"2\": [\"UserEntity\" for i in range(len(entity_list))],\n",
    "})\n",
    "\n",
    "data_dict_output.to_csv(\"../dicts/monpa_entity_dict.txt\", encoding=\"utf8\", sep=\" \", index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc80950d",
   "metadata": {},
   "source": [
    "# Import and Segment data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84d15e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:00<00:00, 872.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Length:  271186  \n",
      "\n",
      " \n",
      "\n",
      "氣附著管外殼上。蒸氣可在管外凝結成一層薄膜後，再排\n",
      "\n",
      "至儲存槽，或排出後做適當之處置。在接觸式冷凝器中，\n",
      "\n",
      "則噴灑冷的液體以冷凝廢氣中之揮發性成份。 \n",
      "\n",
      "5 -30 \n",
      "\n",
      "\f",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "textElement = \"\"\n",
    "\n",
    "for fileIndex, fileElement in enumerate(tqdm(os.listdir(\"../../data/\"))):\n",
    "    file = codecs.open(\"../../data/\" + fileElement, 'r', encoding='utf8', errors='ignore')\n",
    "    for textIndex, textLines in enumerate(file):\n",
    "        textElement += textLines\n",
    "        \n",
    "    \n",
    "print(\"Text Length: \", len(textElement), \" \\n\\n\", textElement[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91e15b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in entity dictionary\n",
    "\n",
    "monpa.load_userdict(\"../dicts/monpa_entity_dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c898d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create splitted tect lit by monpa\n",
    "\n",
    "textList = utils.short_sentence(textElement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d24177ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1735/1735 [03:41<00:00,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['12', '吋', '晶圓', '後段', '製程', '之', '發展', '趨勢', '探討', '前瞻', '封裝', '系列', '200', '2年', '08月', '05日', '星期一', '瀏覽', '人次', '：', '台灣', '半導體', '產業', '結構', '完整'], ['以', '其', '緊密', '延伸', '的', '晶片', '製造', '合作', '模式'], ['在', '過', '去', '數', '年', '間', '已', '建立', '起', '完整', '的', '半導體', '製程', '供應', '鏈'], ['並', '持續', '朝向', '高', '附加', '價值', '的', '高階', '製程', '發展'], ['而', '基於', '成本', '效益', '與', '縮短', '產品', '上市', '時', '程', '的', '考量']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# divide sentences and segment by monpa\n",
    "cutResultList = []\n",
    "\n",
    "\n",
    "for textIndex, textElement in enumerate(tqdm(textList)):\n",
    "    cutElements = []\n",
    "    cutPOSs = []\n",
    "    # pseg function\n",
    "    cutResult = monpa.pseg(textElement)\n",
    "    # append entity and pos seperately\n",
    "    for cutIndex, cutElement in enumerate(cutResult):\n",
    "        cutElements.append(cutElement[0])\n",
    "        cutPOSs.append(cutElement[1])\n",
    "    # eliminate entity between two brackets\n",
    "    for elementIndex, element in enumerate(cutElements):\n",
    "        # delete elements that's between two bracket\n",
    "        if cutElements[elementIndex] in bracket_entity_list_first:\n",
    "            left_bracket_index = bracket_entity_list_first.index(cutElements[elementIndex])\n",
    "            findingLimit =  (elementIndex+11) if (elementIndex+11) <= (len(cutElements)) else len(cutElements)\n",
    "            for findingLeftIndex in range(elementIndex+1, findingLimit):\n",
    "                if cutElements[findingLeftIndex] == bracket_entity_list_last[left_bracket_index]:\n",
    "                    for removalIndex in range(elementIndex, findingLeftIndex+1):\n",
    "                        cutElements[removalIndex] = \"\"\n",
    "                    break\n",
    "        elif element in punct_entity_list:\n",
    "            cutElements[elementIndex] = \"\"\n",
    "        elif cutResult[elementIndex][1] in splitter_pos_list or cutResult[elementIndex][0] in sentences_splitter:\n",
    "            # appending subsentences\n",
    "            cutElements[elementIndex] = \"\"\n",
    "            subCutElements = cutElements[:elementIndex]\n",
    "            subCutElements =  list(filter((\"\").__ne__, subCutElements))\n",
    "            cutResultList.append(subCutElements)\n",
    "            for appendingIndex in range(0, elementIndex):\n",
    "                cutElements[appendingIndex] = \"\"\n",
    "    cutElements = list(filter((\"\").__ne__, cutElements))\n",
    "    cutResultList.append(cutElements)\n",
    "\n",
    "print(cutResultList[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "473816f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11859/11859 [04:01<00:00, 49.11it/s] \n"
     ]
    }
   ],
   "source": [
    "# create data for word2vec training\n",
    "\n",
    "lineList = []\n",
    "\n",
    "for sentencesIndex, sentencesElementList in enumerate(tqdm(cutResultList)):\n",
    "    tokenElement = \"\"\n",
    "    for elementIndex, element in enumerate(sentencesElementList):\n",
    "        if element not in stopword_list:\n",
    "            tokenElement += (element + \" \")\n",
    "   \n",
    "    lineList.append(tokenElement)\n",
    "    \n",
    "data_output_word2vec = pd.DataFrame({\n",
    "    \"Token Sentence\":lineList\n",
    "})\n",
    "\n",
    "data_output_word2vec = data_output_word2vec.applymap(lambda x: str(x).replace(' ', u\"\\u00A0\"))\n",
    "\n",
    "data_output_word2vec.to_csv(output_token_for_word2vec, encoding=\"utf8\", sep=\" \", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a8187",
   "metadata": {},
   "source": [
    "# Word2Vec Model Training\n",
    "* Reference: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "* Reference: https://radimrehurek.com/gensim/auto_examples/tutorials/run_annoy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ae0a269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph\\\\scripts\\\\Word2Vec'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "from gensim.models import word2vec\n",
    "from monpa import utils\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import logging\n",
    "import monpa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ce246c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word2vec.LineSentence(output_token_for_word2vec)\n",
    "\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# parameter tuning 1: https://datascience.stackexchange.com/questions/51404/word2vec-how-to-choose-the-embedding-size-parameter\n",
    "# parameter tuning 2: https://stackoverflow.com/questions/29939984/word2vec-and-gensim-parameters-equivalence\n",
    "model = word2vec.Word2Vec(sentences=sentences,\n",
    "    corpus_file=None,\n",
    "    vector_size=600,\n",
    "    alpha=0.025,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    max_vocab_size=None,\n",
    "    sample=0.001,\n",
    "    seed=1,\n",
    "    workers=3,\n",
    "    min_alpha=0.0001,\n",
    "    sg=1,\n",
    "    hs=0,\n",
    "    negative=5,\n",
    "    ns_exponent=0.75,\n",
    "    cbow_mean=1,\n",
    "    epochs=1000,\n",
    "    null_word=0,\n",
    "    trim_rule=None,\n",
    "    sorted_vocab=1,\n",
    "    batch_words=10000,\n",
    "    compute_loss=False,\n",
    "    callbacks=(),\n",
    "    max_final_vocab=None)\n",
    "\n",
    "model.save(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
