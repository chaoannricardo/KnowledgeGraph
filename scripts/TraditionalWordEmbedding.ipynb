{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "homeless-delivery",
   "metadata": {},
   "source": [
    "# Traditional Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "directed-herald",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "  Welcome to MONPA: Multi-Objective NER POS Annotator for Chinese\n",
      "+---------------------------------------------------------------------+\n",
      "已找到 model檔。Found model file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Windows_Storage\\\\Storage\\\\Github\\\\KnowledgeGraph\\\\scripts'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "from gensim.models import word2vec\n",
    "from monpa import utils\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import csv\n",
    "import logging\n",
    "import monpa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-richardson",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "promotional-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ratio = 0.5\n",
    "data_entity = pd.read_csv(\"../results/210408_result/210408_dataset_entity_result_MONPA.csv\", sep=\",\", encoding=\"utf8\")\n",
    "data_relation = pd.read_csv(\"../results/210408_result/210408_dataset_relation_result_MONPA.csv\", sep=\",\", encoding=\"utf8\")\n",
    "output_token_for_word2vec = \"../results/210408_result/210408_token_sentences_for_word2vec.csv\"\n",
    "model_save_path = \"../models/210408_word2vec.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "elder-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONPA\n",
    "relation_dependencies_possible_list = []\n",
    "relation_pos_possible_list = [\"VH\", \"VC\", \"VJ\", \"VA\"]\n",
    "relation_pos_re = \"^[V]\"\n",
    "entity_dependencies_possible_list = []\n",
    "entity_pos_possible_list = [\"Na\", \"Nv\", \"Neu\", \"Nes\", \"Nf\", \"Ng\", \"Nh\", \"Neqa\", \"Nep\", \"Ncd\", \"FW\", \"DE\"]\n",
    "entity_pos_re = \"^[N]\"\n",
    "splitter_pos_list = [\"COMMACATEGORY\", \"PERIODCATEGORY\"]\n",
    "\n",
    "# spaCy\n",
    "# relation_dependencies_possible_list = [\"ROOT\", \"nmod:prep\", \"prep\", \"agent\", ]\n",
    "# relation_pos_possible_list = [\"VERB\"]\n",
    "# relation_pos_re = \"^[V]\"\n",
    "# entity_dependencies_possible_list = [\"compound:nn\", \"nsubj\", \"dep\", \"dobj\"]\n",
    "# entity_pos_possible_list = [\"NOUN\", \"PROPN\"]\n",
    "# entity_pos_re = \"^[N]\"\n",
    "# bracket_pos_list = []\n",
    "\n",
    "# Common, usually entity\n",
    "sentences_splitter = [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\", \"；\", \";\", \".\"] + [\"\\r\\n\" * i for i in range(0, 100)]\n",
    "bracket_entity_list_first = [\"(\", \"（\", \"[\", \"［\", \"{\", \"｛\", \"<\", \"＜\", \"〔\", \"【\", \"〖\", \"《\", \"〈\"]\n",
    "bracket_entity_list_last = [\")\", \"）\", \"]\", \"］\", \"}\", \"｝\", \">\", \"＞\", \"〕\", \"】\", \"〗\", \"》\", \"〉\"]\n",
    "punct_entity_list = [\" \" * i for i in range(0, 100)]\n",
    "conjuction_entity_list = [\"的\", \"、\", \"之\", \"及\", \"與\"]\n",
    "not_entity_relation_list = [\"的\", \"、\", \"之\", \"及\", \"與\", \"\\r\\n \\r\\n \", \"\\r\\n \\r\\n  \"] +\\\n",
    "[\" \" * i for i in range(0, 100)] + [\"\\n\" * i for i in range(0, 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-territory",
   "metadata": {},
   "source": [
    "# Import stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hydraulic-florida",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3249 \n",
      "\n",
      " ['where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', \"who's\", 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', \"won't\", 'would', \"wouldn't\", 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'zero', 'zt', 'ZT', 'zz', 'ZZ', '準備', '覆雜', '心裏', '註意', '裏面']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-ecedad700bed>:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data_temp = pd.read_csv(\"./stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n"
     ]
    }
   ],
   "source": [
    "stopword_list = []\n",
    "\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"./stopwords/\")):\n",
    "    if fileElement[-2:] != \"md\":\n",
    "        data_temp = pd.read_csv(\"./stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n",
    "        stopword_list += data_temp.iloc[:, 0].tolist()\n",
    "        \n",
    "stopword_list = list(dict.fromkeys(stopword_list))\n",
    "        \n",
    "print(len(stopword_list), \"\\n\\n\", stopword_list[-50:])     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-taylor",
   "metadata": {},
   "source": [
    "# Create User-Defined Dict by entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "august-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4462 1404\n",
      "5866\n",
      "['晶圓', '技術', '製程', '晶片', '產品', '設備', '表面', '資源', '成本', '測品'] ['設計業', '連絡', '進 ', '進出', '進化', '設立', '進流水', '設法', '設有', '設施']\n"
     ]
    }
   ],
   "source": [
    "print(int(np.around(len(data_entity) * selected_ratio, decimals=0)), int(np.around(len(data_relation) * selected_ratio, decimals=0)))\n",
    "data_dict = pd.concat([data_entity.iloc[:int(np.around(len(data_entity) * selected_ratio, decimals=0)),:],\\\n",
    "                    data_relation.iloc[:int(np.around(len(data_relation) * selected_ratio, decimals=0)),:]], ignore_index=True)\n",
    "print(len(data_dict))\n",
    "\n",
    "entity_list = data_dict.iloc[:, 0].tolist()\n",
    "\n",
    "print(entity_list[:10], entity_list[-10:])\n",
    "\n",
    "# remove blank inside entity element\n",
    "for elementIndex, element in enumerate(entity_list):\n",
    "    entity_list[elementIndex] = str(entity_list[elementIndex]).replace(\" \", \"\")\n",
    "    entity_list[elementIndex] = str(entity_list[elementIndex]).replace(\"  \", \"\")\n",
    "    entity_list[elementIndex] = str(entity_list[elementIndex]).replace(\"\\\"\", \"\")\n",
    "\n",
    "for i in range(len(entity_list)):\n",
    "    if type(data_dict.loc[i, \"Ratio\"]) == pd.Series:\n",
    "        print(data_dict.loc[i, \"Ratio\"])\n",
    "    \n",
    "    \n",
    "data_dict_output = pd.DataFrame({\n",
    "    \"0\": entity_list,\n",
    "    \"1\": [int(1000000000 * data_dict.loc[i, \"Ratio\"]) for i in range(len(entity_list))],\n",
    "    \"2\": [\"UserEntity\" for i in range(len(entity_list))],\n",
    "})\n",
    "\n",
    "data_dict_output.to_csv(\"./dicts/monpa_entity_dict.txt\", encoding=\"utf8\", sep=\" \", index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-character",
   "metadata": {},
   "source": [
    "# Import and Segment data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "diverse-village",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 245.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Length:  207715  \n",
      "\n",
      " 管中流過，凝結下來之蒸\n",
      "\n",
      "氣附著管外殼上。蒸氣可在管外凝結成一層薄膜後，再排\n",
      "\n",
      "至儲存槽，或排出後做適當之處置。在接觸式冷凝器中，\n",
      "\n",
      "則噴灑冷的液體以冷凝廢氣中之揮發性成份。 \n",
      "\n",
      "5 -30 \n",
      "\n",
      "\f",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "textElement = \"\"\n",
    "\n",
    "for fileIndex, fileElement in enumerate(tqdm(os.listdir(\"../data/\"))):\n",
    "    file = codecs.open(\"../data/\" + fileElement, 'r', encoding='utf8', errors='ignore')\n",
    "    for textIndex, textLines in enumerate(file):\n",
    "        textElement += textLines\n",
    "        \n",
    "    \n",
    "print(\"Text Length: \", len(textElement), \" \\n\\n\", textElement[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dutch-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in entity dictionary\n",
    "\n",
    "monpa.load_userdict(\"./dicts/monpa_entity_dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "entitled-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create splitted tect lit by monpa\n",
    "\n",
    "textList = utils.short_sentence(textElement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prescribed-casting",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1332/1332 [12:43<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['安森美', '半導體'], ['今天', '發表', '新', '的', 'R', 'D', 'M', '系列', '矽', '光電', '倍增', '管', '陣列'], ['將', '光學', '雷達', '感測器', '能力', '擴', '展', '到', '其', '廣泛', '的', '智慧', '感測', '方案', '陣容'], ['ArrayR', 'D', 'M', '-', '0112', 'A', '20', '-', 'QFN', '是', '市場', '上', '首款', '符合', '車規', '的', 'SiP', 'M', '產品'], ['以', '滿足', '汽車', '產業', '及', '其他', '領域', 'Li', 'D', 'AR', '應', '用', '中', '不斷', '增長', '的', '需求']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# divide sentences and segment by monpa\n",
    "cutResultList = []\n",
    "\n",
    "\n",
    "for textIndex, textElement in enumerate(tqdm(textList)):\n",
    "    cutElements = []\n",
    "    cutPOSs = []\n",
    "    # pseg function\n",
    "    cutResult = monpa.pseg(textElement)\n",
    "    # append entity and pos seperately\n",
    "    for cutIndex, cutElement in enumerate(cutResult):\n",
    "        cutElements.append(cutElement[0])\n",
    "        cutPOSs.append(cutElement[1])\n",
    "    # eliminate entity between two brackets\n",
    "    for elementIndex, element in enumerate(cutElements):\n",
    "        # delete elements that's between two bracket\n",
    "        if cutElements[elementIndex] in bracket_entity_list_first:\n",
    "            left_bracket_index = bracket_entity_list_first.index(cutElements[elementIndex])\n",
    "            findingLimit =  (elementIndex+11) if (elementIndex+11) <= (len(cutElements)) else len(cutElements)\n",
    "            for findingLeftIndex in range(elementIndex+1, findingLimit):\n",
    "                if cutElements[findingLeftIndex] == bracket_entity_list_last[left_bracket_index]:\n",
    "                    for removalIndex in range(elementIndex, findingLeftIndex+1):\n",
    "                        cutElements[removalIndex] = \"\"\n",
    "                    break\n",
    "        elif element in punct_entity_list:\n",
    "            cutElements[elementIndex] = \"\"\n",
    "        elif cutResult[elementIndex][1] in splitter_pos_list or cutResult[elementIndex][0] in sentences_splitter:\n",
    "            # appending subsentences\n",
    "            cutElements[elementIndex] = \"\"\n",
    "            subCutElements = cutElements[:elementIndex]\n",
    "            subCutElements =  list(filter((\"\").__ne__, subCutElements))\n",
    "            cutResultList.append(subCutElements)\n",
    "            for appendingIndex in range(0, elementIndex):\n",
    "                cutElements[appendingIndex] = \"\"\n",
    "    cutElements = list(filter((\"\").__ne__, cutElements))\n",
    "    cutResultList.append(cutElements)\n",
    "\n",
    "print(cutResultList[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "diverse-centre",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9362/9362 [00:10<00:00, 934.55it/s] \n"
     ]
    }
   ],
   "source": [
    "# create data for word2vec training\n",
    "\n",
    "lineList = []\n",
    "\n",
    "for sentencesIndex, sentencesElementList in enumerate(tqdm(cutResultList)):\n",
    "    tokenElement = \"\"\n",
    "    for elementIndex, element in enumerate(sentencesElementList):\n",
    "        if element not in stopword_list:\n",
    "            tokenElement += (element + \" \")\n",
    "   \n",
    "    lineList.append(tokenElement)\n",
    "    \n",
    "data_output_word2vec = pd.DataFrame({\n",
    "    \"Token Sentence\":lineList\n",
    "})\n",
    "\n",
    "data_output_word2vec = data_output_word2vec.applymap(lambda x: str(x).replace(' ', u\"\\u00A0\"))\n",
    "\n",
    "data_output_word2vec.to_csv(output_token_for_word2vec, encoding=\"utf8\", sep=\" \", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-cholesterol",
   "metadata": {},
   "source": [
    "# Word2Vec Model Training\n",
    "* Reference: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "* Reference: https://radimrehurek.com/gensim/auto_examples/tutorials/run_annoy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "novel-dressing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Windows_Storage\\\\Storage\\\\Github\\\\KnowledgeGraph\\\\scripts'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "from gensim.models import word2vec\n",
    "from monpa import utils\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import logging\n",
    "import monpa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "greek-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word2vec.LineSentence(output_token_for_word2vec)\n",
    "\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "model = word2vec.Word2Vec(sentences=sentences,\n",
    "    corpus_file=None,\n",
    "    size=600,\n",
    "    alpha=0.025,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    max_vocab_size=None,\n",
    "    sample=0.001,\n",
    "    seed=1,\n",
    "    workers=3,\n",
    "    min_alpha=0.0001,\n",
    "    sg=0,\n",
    "    hs=0,\n",
    "    negative=5,\n",
    "    ns_exponent=0.75,\n",
    "    cbow_mean=1,\n",
    "    iter=1000,\n",
    "    null_word=0,\n",
    "    trim_rule=None,\n",
    "    sorted_vocab=1,\n",
    "    batch_words=10000,\n",
    "    compute_loss=False,\n",
    "    callbacks=(),\n",
    "    max_final_vocab=None)\n",
    "\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-capacity",
   "metadata": {},
   "source": [
    "# Some opertation needed but would make kernel down\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"../models/210408_word2vec.model\")\n",
    "\n",
    "print(model.wv.similarity(\"晶圓\", \"價值\"))\n",
    "print(model.wv.most_similar([\"晶圓\", \"矽\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monpa",
   "language": "python",
   "name": "monpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
