{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "preceding-queue",
   "metadata": {},
   "source": [
    "# Traditional Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "missing-peeing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "  Welcome to MONPA: Multi-Objective NER POS Annotator for Chinese\n",
      "+---------------------------------------------------------------------+\n",
      "已找到 model檔。Found model file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Windows_Storage\\\\Storage\\\\Github\\\\KnowledgeGraph\\\\scripts'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "from gensim.models import word2vec\n",
    "from monpa import utils\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import logging\n",
    "import monpa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-directory",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "explicit-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ratio = 0.5\n",
    "data_entity = pd.read_csv(\"../results/210408_result/210408_dataset_entity_result_MONPA.csv\", sep=\",\", encoding=\"utf8\")\n",
    "data_relation = pd.read_csv(\"../results/210408_result/210408_dataset_relation_result_MONPA.csv\", sep=\",\", encoding=\"utf8\")\n",
    "output_token_for_word2vec = \"../results/210408_result/210408_token_sentences_for_word2vec.csv\"\n",
    "model_save_path = \"../models/210408_word2vec.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "modular-pricing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONPA\n",
    "relation_dependencies_possible_list = []\n",
    "relation_pos_possible_list = [\"VH\", \"VC\", \"VJ\", \"VA\"]\n",
    "relation_pos_re = \"^[V]\"\n",
    "entity_dependencies_possible_list = []\n",
    "entity_pos_possible_list = [\"Na\", \"Nv\", \"Neu\", \"Nes\", \"Nf\", \"Ng\", \"Nh\", \"Neqa\", \"Nep\", \"Ncd\", \"FW\", \"DE\"]\n",
    "entity_pos_re = \"^[N]\"\n",
    "bracket_pos_list = [\"PARENTHESISCATEGORY\"]\n",
    "punct_pos_list = [\"COMMACATEGORY\", \"PERIODCATEGORY\"]\n",
    "\n",
    "# spaCy\n",
    "# relation_dependencies_possible_list = [\"ROOT\", \"nmod:prep\", \"prep\", \"agent\", ]\n",
    "# relation_pos_possible_list = [\"VERB\"]\n",
    "# relation_pos_re = \"^[V]\"\n",
    "# entity_dependencies_possible_list = [\"compound:nn\", \"nsubj\", \"dep\", \"dobj\"]\n",
    "# entity_pos_possible_list = [\"NOUN\", \"PROPN\"]\n",
    "# entity_pos_re = \"^[N]\"\n",
    "# bracket_pos_list = []\n",
    "# punct_pos_list = [\"PUNCT\"]\n",
    "\n",
    "# Common, usually entity\n",
    "sentences_splitter = [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\"]\n",
    "bracket_entity_list = [\"(\", \")\", \"（\", \"）\"]\n",
    "punct_entity_list = [\" \" * i for i in range(0, 100)]\n",
    "conjuction_entity_list = [\"的\", \"、\", \"之\", \"及\", \"與\"]\n",
    "not_entity_relation_list = [\"的\", \"、\", \"之\", \"及\", \"與\", \"\\r\\n \\r\\n \", \"\\r\\n \\r\\n  \"] +\\\n",
    "[\" \" * i for i in range(0, 100)] + [\"\\n\" * i for i in range(0, 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-spiritual",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pressing-russian",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 202.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Length:  207715  \n",
      "\n",
      " 管中流過，凝結下來之蒸\n",
      "\n",
      "氣附著管外殼上。蒸氣可在管外凝結成一層薄膜後，再排\n",
      "\n",
      "至儲存槽，或排出後做適當之處置。在接觸式冷凝器中，\n",
      "\n",
      "則噴灑冷的液體以冷凝廢氣中之揮發性成份。 \n",
      "\n",
      "5 -30 \n",
      "\n",
      "\f",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "textElement = \"\"\n",
    "\n",
    "for fileIndex, fileElement in enumerate(tqdm(os.listdir(\"../data/\"))):\n",
    "    file = codecs.open(\"../data/\" + fileElement, 'r', encoding='utf8', errors='ignore')\n",
    "    for textIndex, textLines in enumerate(file):\n",
    "        textElement += textLines\n",
    "        \n",
    "    \n",
    "print(\"Text Length: \", len(textElement), \" \\n\\n\", textElement[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-renaissance",
   "metadata": {},
   "source": [
    "# Import stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "registered-captain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3249 \n",
      "\n",
      " ['where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', \"who's\", 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', \"won't\", 'would', \"wouldn't\", 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'zero', 'zt', 'ZT', 'zz', 'ZZ', '準備', '覆雜', '心裏', '註意', '裏面']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-caa2e0aa211a>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data_temp = pd.read_csv(\"./stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n"
     ]
    }
   ],
   "source": [
    "stopword_list = []\n",
    "\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"./stopwords/\")):\n",
    "    if fileElement[-2:] != \"md\":\n",
    "        data_temp = pd.read_csv(\"./stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n",
    "        stopword_list += data_temp.iloc[:, 0].tolist()\n",
    "        \n",
    "stopword_list = list(dict.fromkeys(stopword_list))\n",
    "        \n",
    "print(len(stopword_list), \"\\n\\n\", stopword_list[-50:])     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-marketplace",
   "metadata": {},
   "source": [
    "# Create User-Defined Dict by entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "danish-alfred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4291 1337\n",
      "5628\n",
      "['晶圓', '技術', '製程', '晶片', '產品', '表面', '成本', '設備', '市場', '測品'] ['力球', '類機', '類表', '漏油', '切出', '逸散', '逸 出', '精度', '精確', '見下']\n"
     ]
    }
   ],
   "source": [
    "print(int(np.around(len(data_entity) * selected_ratio, decimals=0)), int(np.around(len(data_relation) * selected_ratio, decimals=0)))\n",
    "data_dict = pd.concat([data_entity.iloc[:int(np.around(len(data_entity) * selected_ratio, decimals=0)),:],\\\n",
    "                    data_relation.iloc[:int(np.around(len(data_relation) * selected_ratio, decimals=0)),:]], ignore_index=True)\n",
    "print(len(data_dict))\n",
    "\n",
    "entity_list = data_dict.iloc[:, 0].tolist()\n",
    "\n",
    "print(entity_list[:10], entity_list[-10:])\n",
    "\n",
    "# remove blank inside entity element\n",
    "for elementIndex, element in enumerate(entity_list):\n",
    "    entity_list[elementIndex] = str(entity_list[elementIndex]).replace(\" \", \"\")\n",
    "    entity_list[elementIndex] = str(entity_list[elementIndex]).replace(\"  \", \"\")\n",
    "    entity_list[elementIndex] = str(entity_list[elementIndex]).replace(\"\\\"\", \"\")\n",
    "\n",
    "for i in range(len(entity_list)):\n",
    "    if type(data_dict.loc[i, \"Ratio\"]) == pd.Series:\n",
    "        print(data_dict.loc[i, \"Ratio\"])\n",
    "    \n",
    "    \n",
    "data_dict_output = pd.DataFrame({\n",
    "    \"0\": entity_list,\n",
    "    \"1\": [int(1000000000 * data_dict.loc[i, \"Ratio\"]) for i in range(len(entity_list))],\n",
    "    \"2\": [\"UserEntity\" for i in range(len(entity_list))],\n",
    "})\n",
    "\n",
    "data_dict_output.to_csv(\"./dicts/monpa_entity_dict.txt\", encoding=\"utf8\", sep=\" \", index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-sensitivity",
   "metadata": {},
   "source": [
    "# Segment data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "naked-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in entity dictionary\n",
    "\n",
    "monpa.load_userdict(\"./dicts/monpa_entity_dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "infinite-effort",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('蒸氣', 'UserEntity'), ('可', 'D'), ('在', 'P'), ('管', 'VC'), ('外', 'Ng'), ('凝結', 'UserEntity'), ('成', 'UserEntity'), ('一層', 'UserEntity'), ('薄膜', 'UserEntity'), ('後', 'Ng'), ('，', 'COMMACATEGORY'), ('再', 'D'), ('排', 'VC'), ('  至', 'PER'), ('儲存', 'UserEntity'), ('槽', 'Na'), ('，', 'COMMACATEGORY'), ('或', 'Caa'), ('排出', 'UserEntity'), ('後', 'Ng'), ('做', 'VC'), ('適當', 'VH'), ('之', 'Nh'), ('處置', 'UserEntity'), ('。', 'PERIODCATEGORY'), ('在', 'P'), ('接觸', 'UserEntity'), ('式', 'Na'), ('冷凝', 'UserEntity'), ('器', 'Na'), ('中', 'Ng'), ('，', 'COMMACATEGORY'), (' ', 'FW'), (' ', 'FW'), ('則', 'D'), ('噴灑', 'VC'), ('冷', 'FW'), ('的', 'T'), ('液體', 'UserEntity'), ('以', 'P'), ('冷凝', 'UserEntity'), ('廢氣', 'UserEntity'), ('中', 'Ng'), ('之', 'DE'), ('揮發', 'UserEntity'), ('性', 'Na'), ('成', 'UserEntity'), ('份', 'Na'), ('。', 'PERIODCATEGORY'), (' ', 'FW'), (' ', 'FW'), (' ', 'FW'), ('5', 'Neu'), (' ', 'FW'), ('-', 'DASHCATEGORY'), ('30', 'Neu')]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# divide sentences and segment by monpa\n",
    "textList = utils.short_sentence(textElement)\n",
    "\n",
    "cutResultList = []\n",
    "\n",
    "for textIndex, textElement in enumerate(tqdm(textList)):\n",
    "    cutResult = monpa.pseg(textElement)\n",
    "    for elementIndex, element in enumerate(cutResult):\n",
    "        if element in stopword_list:\n",
    "            cutResult[elementIndex] = \"\"\n",
    "            \n",
    "        \n",
    "    cutResultList.append(cutResult)\n",
    "\n",
    "print(cutResultList[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-devices",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "necessary-enterprise",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1332/1332 [00:06<00:00, 219.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# create data for word2vec training\n",
    "\n",
    "lineList = []\n",
    "\n",
    "for sentencesIndex, sentencesElementList in enumerate(tqdm(cutResultList)):\n",
    "    tokenElement = \"\"\n",
    "    for elementIndex, element in enumerate(sentencesElementList):\n",
    "        if element not in stopword_list:\n",
    "            tokenElement += (element + \" \")\n",
    "    lineList.append(tokenElement)\n",
    "    \n",
    "data_output_word2vec = pd.DataFrame({\n",
    "    \"Token Sentence\":lineList\n",
    "})\n",
    "\n",
    "data_output_word2vec.to_csv(output_token_for_word2vec, encoding=\"utf8\", sep=\" \", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-adams",
   "metadata": {},
   "source": [
    "# Word2Vec Model Training\n",
    "* Reference: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "* Reference: https://radimrehurek.com/gensim/auto_examples/tutorials/run_annoy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "professional-victorian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Windows_Storage\\\\Storage\\\\Github\\\\KnowledgeGraph\\\\scripts'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "from gensim.models import word2vec\n",
    "from monpa import utils\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import logging\n",
    "import monpa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dried-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word2vec.LineSentence(output_token_for_word2vec)\n",
    "model = word2vec.Word2Vec(sentences, size=240)\n",
    "\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-growing",
   "metadata": {},
   "source": [
    "# Some opertation needed but would make kernel down\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"../models/210408_word2vec.model\")\n",
    "\n",
    "print(model.wv.similarity(\"晶圓\", \"價值\"))\n",
    "print(model.wv.most_similar([\"晶圓\", \"矽\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monpa",
   "language": "python",
   "name": "monpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
