{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "phantom-biodiversity",
   "metadata": {},
   "source": [
    "# Traditional Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vocal-fault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "  Welcome to MONPA: Multi-Objective NER POS Annotator for Chinese\n",
      "+---------------------------------------------------------------------+\n",
      "已找到 model檔。Found model file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Windows_Storage\\\\Storage\\\\Github\\\\KnowledgeGraph\\\\scripts'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "from gensim.models import word2vec\n",
    "from monpa import utils\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import logging\n",
    "import monpa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bored-broadcasting",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pointed-semester",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_ratio = 0.5\n",
    "data_entity = pd.read_csv(\"../results/210408_result/210408_dataset_entity_result_MONPA.csv\", sep=\",\", encoding=\"utf8\")\n",
    "data_relation = pd.read_csv(\"../results/210408_result/210408_dataset_relation_result_MONPA.csv\", sep=\",\", encoding=\"utf8\")\n",
    "output_token_for_word2vec = \"../results/210408_result/210408_token_sentences_for_word2vec.csv\"\n",
    "model_save_path = \"../models/210408_word2vec.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deadly-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONPA\n",
    "relation_dependencies_possible_list = []\n",
    "relation_pos_possible_list = [\"VH\", \"VC\", \"VJ\", \"VA\"]\n",
    "relation_pos_re = \"^[V]\"\n",
    "entity_dependencies_possible_list = []\n",
    "entity_pos_possible_list = [\"Na\", \"Nv\", \"Neu\", \"Nes\", \"Nf\", \"Ng\", \"Nh\", \"Neqa\", \"Nep\", \"Ncd\", \"FW\", \"DE\"]\n",
    "entity_pos_re = \"^[N]\"\n",
    "bracket_pos_list = [\"PARENTHESISCATEGORY\"]\n",
    "punct_pos_list = [\"COMMACATEGORY\", \"PERIODCATEGORY\"]\n",
    "\n",
    "# spaCy\n",
    "# relation_dependencies_possible_list = [\"ROOT\", \"nmod:prep\", \"prep\", \"agent\", ]\n",
    "# relation_pos_possible_list = [\"VERB\"]\n",
    "# relation_pos_re = \"^[V]\"\n",
    "# entity_dependencies_possible_list = [\"compound:nn\", \"nsubj\", \"dep\", \"dobj\"]\n",
    "# entity_pos_possible_list = [\"NOUN\", \"PROPN\"]\n",
    "# entity_pos_re = \"^[N]\"\n",
    "# bracket_pos_list = []\n",
    "# punct_pos_list = [\"PUNCT\"]\n",
    "\n",
    "# Common, usually entity\n",
    "sentences_splitter = [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\"]\n",
    "bracket_entity_list = [\"(\", \")\", \"（\", \"）\"]\n",
    "punct_entity_list = [\" \" * i for i in range(0, 100)]\n",
    "conjuction_entity_list = [\"的\", \"、\", \"之\", \"及\", \"與\"]\n",
    "not_entity_relation_list = [\"的\", \"、\", \"之\", \"及\", \"與\", \"\\r\\n \\r\\n \", \"\\r\\n \\r\\n  \"] +\\\n",
    "[\" \" * i for i in range(0, 100)] + [\"\\n\" * i for i in range(0, 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-aluminum",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "martial-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 202.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Length:  207715  \n",
      "\n",
      " 管中流過，凝結下來之蒸\n",
      "\n",
      "氣附著管外殼上。蒸氣可在管外凝結成一層薄膜後，再排\n",
      "\n",
      "至儲存槽，或排出後做適當之處置。在接觸式冷凝器中，\n",
      "\n",
      "則噴灑冷的液體以冷凝廢氣中之揮發性成份。 \n",
      "\n",
      "5 -30 \n",
      "\n",
      "\f",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "textElement = \"\"\n",
    "\n",
    "for fileIndex, fileElement in enumerate(tqdm(os.listdir(\"../data/\"))):\n",
    "    file = codecs.open(\"../data/\" + fileElement, 'r', encoding='utf8', errors='ignore')\n",
    "    for textIndex, textLines in enumerate(file):\n",
    "        textElement += textLines\n",
    "        \n",
    "    \n",
    "print(\"Text Length: \", len(textElement), \" \\n\\n\", textElement[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-fleet",
   "metadata": {},
   "source": [
    "# Import stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "infrared-somerset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3249 \n",
      "\n",
      " ['where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', \"who's\", 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', \"won't\", 'would', \"wouldn't\", 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'zero', 'zt', 'ZT', 'zz', 'ZZ', '準備', '覆雜', '心裏', '註意', '裏面']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-caa2e0aa211a>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data_temp = pd.read_csv(\"./stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n"
     ]
    }
   ],
   "source": [
    "stopword_list = []\n",
    "\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"./stopwords/\")):\n",
    "    if fileElement[-2:] != \"md\":\n",
    "        data_temp = pd.read_csv(\"./stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n",
    "        stopword_list += data_temp.iloc[:, 0].tolist()\n",
    "        \n",
    "stopword_list = list(dict.fromkeys(stopword_list))\n",
    "        \n",
    "print(len(stopword_list), \"\\n\\n\", stopword_list[-50:])     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-pointer",
   "metadata": {},
   "source": [
    "# Create User-Defined Dict by entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "challenging-emperor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4291 1337\n",
      "5628\n",
      "['晶圓', '技術', '製程', '晶片', '產品', '表面', '成本', '設備', '市場', '測品'] ['力球', '類機', '類表', '漏油', '切出', '逸散', '逸 出', '精度', '精確', '見下']\n"
     ]
    }
   ],
   "source": [
    "print(int(np.around(len(data_entity) * selected_ratio, decimals=0)), int(np.around(len(data_relation) * selected_ratio, decimals=0)))\n",
    "data_dict = pd.concat([data_entity.iloc[:int(np.around(len(data_entity) * selected_ratio, decimals=0)),:],\\\n",
    "                    data_relation.iloc[:int(np.around(len(data_relation) * selected_ratio, decimals=0)),:]], ignore_index=True)\n",
    "print(len(data_dict))\n",
    "\n",
    "entity_list = data_dict.iloc[:, 0].tolist()\n",
    "\n",
    "print(entity_list[:10], entity_list[-10:])\n",
    "\n",
    "# remove blank inside entity element\n",
    "for elementIndex, element in enumerate(entity_list):\n",
    "    entity_list[elementIndex] = str(entity_list[elementIndex]).replace(\" \", \"\")\n",
    "    entity_list[elementIndex] = str(entity_list[elementIndex]).replace(\"  \", \"\")\n",
    "    entity_list[elementIndex] = str(entity_list[elementIndex]).replace(\"\\\"\", \"\")\n",
    "\n",
    "for i in range(len(entity_list)):\n",
    "    if type(data_dict.loc[i, \"Ratio\"]) == pd.Series:\n",
    "        print(data_dict.loc[i, \"Ratio\"])\n",
    "    \n",
    "    \n",
    "data_dict_output = pd.DataFrame({\n",
    "    \"0\": entity_list,\n",
    "    \"1\": [int(1000000000 * data_dict.loc[i, \"Ratio\"]) for i in range(len(entity_list))],\n",
    "    \"2\": [\"UserEntity\" for i in range(len(entity_list))],\n",
    "})\n",
    "\n",
    "data_dict_output.to_csv(\"./dicts/monpa_entity_dict.txt\", encoding=\"utf8\", sep=\" \", index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-packet",
   "metadata": {},
   "source": [
    "# Segment data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "working-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in entity dictionary\n",
    "\n",
    "monpa.load_userdict(\"./dicts/monpa_entity_dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "regional-blocking",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1332/1332 [07:10<00:00,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['安森美', '半導體', '', 'ON Semiconductor', '', '', '', '發表', '新', '', 'RD', 'M', '系列', '矽', '光電', '倍增', '', ' ', '', 'SiP', 'M', '', ' ', '陣列', '', '', '光學', '雷達', ' ', '', 'LiDAR', '', ' ', '感測器', '能力', '擴', '展', '', '', '', '', '智慧', '感測', '方案', '陣容', '', 'ArrayRD', 'M', '', '01', '12', 'A20-QFN', '', '市場', '', '首款', '符合', '車規', '', 'SiP', 'M', '產品', '', '', '', '汽車', '產業', '', '', '領域', 'LiDAR', '應', '', '中', '', '增長', '', '需求', ''], ['ArrayRD', 'M', '', '01', '12', 'A20-QFN', '', '單片', '', '', '12', 'SiP', 'M', '', '素', '陣列', '', '', '安森美', '半導體', '領先', '市場', '', 'RD', 'M', '製程', '', '', '', '', '', '紅', '外 ', '', 'NIR', '', ' ', '', '', '高', '靈敏度', '', '', '', '905', '奈米', '', 'nm', '', '處', '', '領先', '業界', '', '18.5', '', '', '光子', '偵測', '效率', ' ', '', 'PDE', '', '', 'SiP', 'M', '', '高', '內部', '增益', '', '', '靈敏度', '', '', '單光子', '水', '準', '', '', '功能', '', '高', 'PDE', '', '', '', '', '', '檢測', '', '微弱', '', '返回', '信號', '', '', '', '', '', '低', '反射', '目', '標', '', '', '', '偵測到', '', '遠', '', '距離', ''], ['', '', '', '產品', '符合', 'AEC-Q', '10', '', '', '國際', '汽車', '電子', '協會', '', '', '', ' IATF', '16', '949', '', '國際', '汽車', '推動', '', '組', '', '', '技術', '規範', '開發', '', '    ', 'SiP', 'M', '技術', '近年', '', '蓬勃', '發展', '', '', '', '獨特', '', '功能', '組', '', '', '成', '', '', '市場', '深度', '感測', '應', '', '', '首選', '感測器', '', 'SiP', 'M', '', '', '明亮', '', '陽光', '條件', '', '進行長', '距離', '測', '', '時', '提供', '最佳', '', '訊噪', '', '性能', '', '', '優勢', '包括', '', '低', '', '電', '源', '偏置', '', '', '低', '', '溫度', '變化', '敏感', '性', '', '', '', '成', '', '', '', '傳統', '雪崩', '光電', '二極體', '', 'APD', '', '', '系統', '', '理想', '升級', '產品', ''], ['SiP', 'M', '採用', '大批量', 'C', 'M', 'OS', '製程', '生產', '', '', '', '', '低', '', '偵測器', '成本', '', '', '', '應', '用於', '', '市場', '', 'LiDAR', '方案', '', ' ', '   ', '', '', '激光', '測量', '物體', '', '距離', '', '跨越', '', '汽車', '', '消費', '', '工業', '應', '', '領域', '', '', '汽車', '領域', '', 'LiDAR', '', '用於', '提升', '安全性', '', '駕駛', '輔助', '系統', '', 'ADAS', '', '', '通', '', '', '', '感知', '模式', '互補和', '提供', '冗餘', '', '輔助', '', '車道', '', '和交', '通', '擁堵', '輔助', '', '功能', '', 'LiDAR', '正', '', '用於', '全', '自動', '駕駛', '', '', '', '案例', '', '', '機器', '', '運輸', '', '', '安全地', '實時', '導航', '環境', ''], ['受益', '', 'ArrayRD', 'M', '', '01', '12', 'A20-QFN', '', '高', 'PDE', '', '支持', '', '功能', '', 'LiDAR', '系統', '', '', '證明', '', '', '300', '米', '', '', '距離', '測距', '', '', '遠', '', '距離', '', '車輛', '', '', '', '時間', '', '應', '', '意外', '障礙', '', '    安森美', '半導體', '汽車', '感測', '部門', '資深', '總監', 'Wade Appelman', '表', '示', '', 'LiDAR', '提供', '', '高', '解析', '度', '深度', '數據', '', '', '充滿', '挑戰', '', '微光', '條件', '下即時', '準確', '', '識別物體', '']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# divide sentences and segment by monpa\n",
    "textList = utils.short_sentence(textElement)\n",
    "\n",
    "cutResultList = []\n",
    "\n",
    "for textIndex, textElement in enumerate(tqdm(textList)):\n",
    "    cutResult = monpa.pseg(textElement)\n",
    "    for elementIndex, element in enumerate(cutResult):\n",
    "        if element in stopword_list:\n",
    "            cutResult[elementIndex] = \"\"\n",
    "    cutResultList.append(cutResult)\n",
    "\n",
    "print(cutResultList[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-nepal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "affiliated-hostel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1332/1332 [00:06<00:00, 219.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# create data for word2vec training\n",
    "\n",
    "lineList = []\n",
    "\n",
    "for sentencesIndex, sentencesElementList in enumerate(tqdm(cutResultList)):\n",
    "    tokenElement = \"\"\n",
    "    for elementIndex, element in enumerate(sentencesElementList):\n",
    "        if element not in stopword_list:\n",
    "            tokenElement += (element + \" \")\n",
    "    lineList.append(tokenElement)\n",
    "    \n",
    "data_output_word2vec = pd.DataFrame({\n",
    "    \"Token Sentence\":lineList\n",
    "})\n",
    "\n",
    "data_output_word2vec.to_csv(output_token_for_word2vec, encoding=\"utf8\", sep=\" \", header=None, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-setting",
   "metadata": {},
   "source": [
    "# Word2Vec Model Training\n",
    "* Reference: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py\n",
    "* Reference: https://radimrehurek.com/gensim/auto_examples/tutorials/run_annoy.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "altered-glucose",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Windows_Storage\\\\Storage\\\\Github\\\\KnowledgeGraph\\\\scripts'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import packages\n",
    "from gensim.models import word2vec\n",
    "from monpa import utils\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import logging\n",
    "import monpa\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extended-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word2vec.LineSentence(output_token_for_word2vec)\n",
    "model = word2vec.Word2Vec(sentences, size=240)\n",
    "\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-jersey",
   "metadata": {},
   "source": [
    "# Some opertation needed but would make kernel down\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"../models/210408_word2vec.model\")\n",
    "\n",
    "print(model.wv.similarity(\"晶圓\", \"價值\"))\n",
    "print(model.wv.most_similar([\"晶圓\", \"矽\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monpa",
   "language": "python",
   "name": "monpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
