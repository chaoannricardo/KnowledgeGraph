{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a90ca4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Desktop\\Ricardo\\KnowledgeGraph\\scripts\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487fd8b9",
   "metadata": {},
   "source": [
    "# Some Reference\n",
    "\n",
    "* Regular Expression Tester: https://regex101.com/\n",
    "* Some Re Tutorials: https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7646eec",
   "metadata": {},
   "source": [
    "# Processing Steps\n",
    "* Divide paragraphs into sentences by Chinese punctuations (“，”, “。”, “、”, “？”, “?”).\n",
    "* Segment sentences into tokens with relative dependencies and POSs (Part of Speech).\n",
    "* Remove tokens with certain type of dependency and POS (i.e. PUNCT, x, PARENTHESISCATEGORY)\n",
    "* Remove unnecessary tokens\n",
    "  * punctuation\n",
    "  * word between two bracket\n",
    "* Extract possible tokens as candidates of entities and relations by its dependency and POS category,\n",
    "  * with regular expression (Entity, POS, dependencies): “^[V]”, “^[N]”,\n",
    "  * with certain dependencies and POS type,\n",
    "  * with given entity dictionaries (entity, POS, dependencies),\n",
    "  * with certain conjuction chars ([\"的\", \"、\", \"之\", \"及\", \"與\"]) ,\n",
    "* Clean up candidate list again by removing unnecessary tokens (conjunction characters). \n",
    "  * 晶圓的 -> 晶圓, 的製程 -> 製程, issues caused by adjectives.\n",
    "* Remove stopwords\n",
    "* Remove entity and relation with only one and more than 10 chars\n",
    "* Caculate TFiDF and sort\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Optional\n",
    "* Conclude frequency of occurrence of each node/edge candidate, keeping the first 20 ranks only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c3ec5",
   "metadata": {},
   "source": [
    "# Configurations\n",
    "### Extract possible tokens as candidates of entities and relations by its dependency and POS category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f506a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "token_path = \"../results/210408_result/210408_dataset_monpaResult.csv\"\n",
    "entity_saving_path = \"../results/210408_result/210408_dataset_entity_result_MONPA.csv\"\n",
    "relation_saving_path = \"../results/210408_result/210408_dataset_relation_result_MONPA.csv\"\n",
    "main_result_saving_path = \"../results/210408_result/210408_dataset_main_result_MONPA.csv\"\n",
    "filtered_result_path = \"../results/210408_result/210408_dataset_filtered_result_MONPA.csv\"\n",
    "\n",
    "# token_path = \"../results/210330_result/210330_dataset_spaCyResult.csv\"\n",
    "# entity_saving_path = \"../results/210330_result/210330_dataset_entity_result_spaCy.csv\"\n",
    "# relation_saving_path = \"../results/210330_result/210330_dataset_relation_result_spaCy.csv\"\n",
    "# main_result_saving_path = \"../results/210330_result/210330_dataset_main_result_spaCy.csv\"\n",
    "# filtered_result_path = \"../results/210408_result/210408_dataset_filtered_result_spaCy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a4e57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONPA\n",
    "relation_dependencies_possible_list = []\n",
    "relation_pos_possible_list = [\"VH\", \"VC\", \"VJ\", \"VA\"]\n",
    "relation_pos_re = \"^[V]\"\n",
    "entity_dependencies_possible_list = []\n",
    "entity_pos_possible_list = [\"Na\", \"Nv\", \"Neu\", \"Nes\", \"Nf\", \"Ng\", \"Nh\", \"Neqa\", \"Nep\", \"Ncd\", \"FW\", \"DE\"]\n",
    "entity_pos_re = \"^[N]\"\n",
    "splitter_pos_list = [\"COMMACATEGORY\", \"PERIODCATEGORY\"]\n",
    "\n",
    "# spaCy\n",
    "# relation_dependencies_possible_list = [\"ROOT\", \"nmod:prep\", \"prep\", \"agent\", ]\n",
    "# relation_pos_possible_list = [\"VERB\"]\n",
    "# relation_pos_re = \"^[V]\"\n",
    "# entity_dependencies_possible_list = [\"compound:nn\", \"nsubj\", \"dep\", \"dobj\"]\n",
    "# entity_pos_possible_list = [\"NOUN\", \"PROPN\"]\n",
    "# entity_pos_re = \"^[N]\"\n",
    "# punct_pos_list = [\"PUNCT\"]\n",
    "\n",
    "# Common, usually entity\n",
    "sentences_splitter = [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\", \"；\", \";\", \".\"] + [\"\\r\\n\" * i for i in range(0, 100)]\n",
    "bracket_entity_list_first = [\"(\", \"（\", \"[\", \"［\", \"{\", \"｛\", \"<\", \"＜\", \"〔\", \"【\", \"〖\", \"《\", \"〈\"]\n",
    "bracket_entity_list_last = [\")\", \"）\", \"]\", \"］\", \"}\", \"｝\", \">\", \"＞\", \"〕\", \"】\", \"〗\", \"》\", \"〉\"]\n",
    "punct_entity_list = [\" \" * i for i in range(0, 100)]\n",
    "conjuction_entity_list = [\"的\", \"、\", \"之\", \"及\", \"與\"]\n",
    "not_entity_relation_list = [\"的\", \"、\", \"之\", \"及\", \"與\", \"\\r\\n \\r\\n \", \"\\r\\n \\r\\n  \"] +\\\n",
    "[\" \" * i for i in range(0, 100)] + [\"\\n\" * i for i in range(0, 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd9b8bc",
   "metadata": {},
   "source": [
    "# Read in Tokens (Preparation beforehand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3723a742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143131\n",
      "  Segmented Element  POS  Dependecies\n",
      "0                12  Neu          NaN\n",
      "1                 吋   Nf          NaN\n",
      "2                晶圓   Na          NaN\n",
      "3                後段  Ncd          NaN\n",
      "4                製程   Na          NaN\n"
     ]
    }
   ],
   "source": [
    "# read in tokens\n",
    "\n",
    "dataToken = pd.read_csv(token_path, encoding=\"utf8\")\n",
    "\n",
    "# append a column for data if dependencies column does not exist (monpa case)\n",
    "if len(dataToken.columns) < 3:\n",
    "    dataToken.loc[:, \"Dependecies\"] = np.nan\n",
    "\n",
    "print(len(dataToken))\n",
    "print(dataToken.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494cb33",
   "metadata": {},
   "source": [
    "# Divide by Chinese Seperators\n",
    "# Segment sentences into tokens with relative dependencies and POSs (Part of Speech).\n",
    "# Remove tokens with certain type of dependency and POS (i.e. PUNCT, x, PARENTHESISCATEGORY)\n",
    "# Remove unnecessary tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d33cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list length (Sentences):  9887 9887 9887\n",
      "['12', '吋', '晶圓', '後段', '製程', '之', '發展', '趨勢', '探討', '前瞻', '封裝', '系列', '2002年', '08月', '05日', '星期一', '瀏覽', '人次', '：', '台灣', '半導體', '產業', '結構', '完整'] ['Neu', 'Nf', 'Na', 'Ncd', 'Na', 'DE', 'Nv', 'Na', 'VE', 'FW', 'FW', 'FW', 'Nv', 'Na', 'Na', 'FW', 'FW', 'FW', 'FW', 'FW', 'Nd', 'Nd', 'Nd', 'FW', 'Nd', 'Na', 'FW', 'VC', 'Na', 'FW', 'FW', 'FW', 'FW', 'FW', 'FW', 'FW', 'FW', 'LOC', 'Na', 'Na', 'Na', 'VH'] [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "list length:  9887 9887 9887\n"
     ]
    }
   ],
   "source": [
    "# devide token by seperators\n",
    "total_entity_list = []\n",
    "sentence_entity_list = []\n",
    "total_label_list = []\n",
    "sentence_label_list = []\n",
    "total_dependencies_list = []\n",
    "sentence_dependencies_list = []\n",
    "\n",
    "\n",
    "# devide tokens by chinese punctuations\n",
    "for index, entityElement in enumerate(dataToken.iloc[:, 0]):\n",
    "    if entityElement not in sentences_splitter:\n",
    "        sentence_entity_list.append(entityElement)\n",
    "        sentence_label_list.append(dataToken.iloc[index, 1])\n",
    "        sentence_dependencies_list.append(dataToken.iloc[index, 2])\n",
    "    else:\n",
    "        total_entity_list.append(sentence_entity_list)\n",
    "        total_label_list.append(sentence_label_list)\n",
    "        total_dependencies_list.append(sentence_dependencies_list)\n",
    "        sentence_entity_list = []\n",
    "        sentence_label_list = []\n",
    "        sentence_dependencies_list = []\n",
    "    \n",
    "print(\"list length (Sentences): \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))\n",
    "\n",
    "\n",
    "# Clean up tokens and eliminate unneccesary tokens\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    for tokenIndex, tokens in enumerate(sentences):\n",
    "        # delete elements that's between two bracket\n",
    "        if sentences[tokenIndex] in bracket_entity_list_first:\n",
    "            left_bracket_index = bracket_entity_list_first.index(sentences[tokenIndex])\n",
    "            findingLimit =  (tokenIndex+11) if (tokenIndex+11) <= (len(sentences)) else len(sentences)\n",
    "            for findingLeftIndex in range(tokenIndex+1, findingLimit):\n",
    "                if sentences[findingLeftIndex] == bracket_entity_list_last[left_bracket_index]:\n",
    "                    for removalIndex in range(tokenIndex, findingLeftIndex+1):\n",
    "#                         print(total_entity_list[sentenceIndex][removalIndex])\n",
    "                        total_entity_list[sentenceIndex][removalIndex] = \"\"\n",
    "                        total_label_list[sentenceIndex][removalIndex]= \"\"\n",
    "                        total_dependencies_list[sentenceIndex][removalIndex] = \"\"\n",
    "                    break\n",
    "        elif sentences[tokenIndex] in punct_entity_list or sentences[tokenIndex] in sentences_splitter:\n",
    "            # set token that fit certain POS type to \"\"\n",
    "            total_entity_list[sentenceIndex][tokenIndex] = \"\"\n",
    "            \n",
    "    total_entity_list[sentenceIndex] = list(filter((\"\").__ne__, total_entity_list[sentenceIndex]))\n",
    "    total_label_list[sentenceIndex]  = list(filter((\"\").__ne__, total_label_list[sentenceIndex]))\n",
    "    total_dependencies_list[sentenceIndex] = list(filter((\"\").__ne__, total_dependencies_list[sentenceIndex]))\n",
    "                \n",
    "print(total_entity_list[0] , total_label_list[0], total_dependencies_list[0])\n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95a392",
   "metadata": {},
   "source": [
    "# Extract possible tokens as candidates of entities and relations by its dependency and POS category.\n",
    "#  Clean up candidate list again by removing unnecessary tokens (conjunction characters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c30205e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9887 9887 9887 9887\n",
      "['12吋晶圓後段製程之發展趨勢', '前瞻封裝系列2002年08月05日星期一瀏覽人次：台灣半導體產業結構完整'] [7, 23] ['探討'] [8]\n",
      "['其', '晶片製造合作模式'] [1, 8] ['緊密', '延伸'] [2, 3]\n",
      "['數年間', '半導體製程供應鏈'] [3, 11] ['過去', '建立', '完整'] [1, 5, 7]\n",
      "['附加價值', '製程發展'] [6, 9] ['持續', '朝向', '高'] [1, 2, 3]\n",
      "['成本效益與縮短產品', '時程的考量'] [6, 10] ['上市'] [7]\n",
      "['12吋晶圓廠', '晶圓代工下一波的競爭主力'] [2, 12] ['成為'] [4]\n",
      "['國際整合元件大廠', '產能', '趨勢下'] [4, 7, 12] ['委外', '代工'] [8, 9]\n",
      "['晶圓', '', '表現'] [3, 5, 7] ['造就', '代工', '亮麗'] [0, 4, 6]\n",
      "['半導體產業的競爭優勢'] [7] ['建立'] [1]\n",
      "['國內晶圓', '兩', '龍頭', '積體電路', '科學園區', '科學園區', '12吋晶圓代工製造廠'] [1, 3, 5, 9, 15, 18, 27] ['代工', '大', '開始', '投資', '興建'] [2, 4, 20, 21, 22]\n",
      "['小部份產能'] [3] ['進行', '試產'] [1, 4]\n",
      "['國際級的整合元件大廠', '12吋晶圓的發展趨勢'] [5, 14] ['前進'] [15]\n",
      "['後段的封裝製造廠', '整合性', '服務'] [4, 7, 10] ['使得', '提供', '化'] [0, 6, 9]\n",
      "['製程技術'] [4] ['強化'] [2]\n",
      "['晶圓尺寸增加後'] [5] ['滿足'] [1]\n"
     ]
    }
   ],
   "source": [
    "# get entity pairs and relations\n",
    "\n",
    "# get relations\n",
    "all_relations_list = []\n",
    "all_relations_index = []\n",
    "all_element_flatten = []\n",
    "all_relation_flatten = []\n",
    "\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    relation_list = []\n",
    "    relation_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1: \n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in relation_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in relation_pos_possible_list or\\\n",
    "            re.match(relation_pos_re, str(total_label_list[sentenceIndex][tokenIndex]), flags=re.IGNORECASE) != None:\n",
    "                relation_list.append(token)\n",
    "                relation_index_list.append(tokenIndex)\n",
    "    all_relations_list.append(relation_list)\n",
    "    all_relations_index.append(relation_index_list)\n",
    "\n",
    "\n",
    "# get entities\n",
    "all_reformatted_entities = []\n",
    "all_reformatted_index_list = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    entity_index_list = []\n",
    "    possible_entities = []\n",
    "    reformatted_entities = []\n",
    "    reformatted_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1:\n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in entity_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in entity_pos_possible_list or\\\n",
    "            re.match(entity_pos_re, str(total_label_list[sentenceIndex][tokenIndex]), flags=re.IGNORECASE) != None or\\\n",
    "            token in conjuction_entity_list:\n",
    "                entity_index_list.append(tokenIndex)\n",
    "                possible_entities.append(token)\n",
    "    # combine token if situate next to each other\n",
    "#     print(entity_index_list, possible_entities)\n",
    "    if len(possible_entities) > 0:\n",
    "        combine_entity_name = possible_entities[0]\n",
    "        for possibleIndex, possibleElement in enumerate(entity_index_list):\n",
    "#             print(possible_entities)\n",
    "            \n",
    "            isContinuous = False\n",
    "            if possibleIndex != 0:\n",
    "                if possibleElement == entity_index_list[possibleIndex - 1] + 1:\n",
    "                    isContinuous = True\n",
    "                    combine_entity_name += str(possible_entities[possibleIndex])\n",
    "                else:\n",
    "                    isContinuous = False\n",
    "            \n",
    "                if isContinuous == False:\n",
    "                    reformatted_entities.append(combine_entity_name)\n",
    "                    reformatted_index_list.append(entity_index_list[possibleIndex-1])\n",
    "                    combine_entity_name = possible_entities[possibleIndex]\n",
    "                    \n",
    "            if possibleIndex == (len(entity_index_list) - 1):\n",
    "                reformatted_entities.append(combine_entity_name)\n",
    "                reformatted_index_list.append(possibleElement)\n",
    "                \n",
    "#             print(combine_entity_name)     \n",
    "#     print(reformatted_entities)\n",
    "    all_reformatted_entities.append(reformatted_entities)\n",
    "    all_reformatted_index_list.append(reformatted_index_list)\n",
    "    \n",
    "# flatten section\n",
    "# remove unneccessary conjuction words\n",
    "for index, element in enumerate(all_reformatted_entities):\n",
    "    \n",
    "    for elementIndex, elementSingle in enumerate(element):\n",
    "        # remove single conjuction char in first or last position\n",
    "        if elementSingle[0] in conjuction_entity_list:\n",
    "            element[elementIndex] = elementSingle[1:]\n",
    "        elif elementSingle[-1] in conjuction_entity_list:\n",
    "            element[elementIndex] = elementSingle[:-1]\n",
    "            \n",
    "        all_element_flatten.append(element[elementIndex])\n",
    "        \n",
    "    for relationIndex, relationSingle in enumerate(all_relations_list[index]):\n",
    "        # remove single conjuction char in first or last position\n",
    "        if relationSingle[0] in conjuction_entity_list:\n",
    "            all_relations_list[index][relationIndex] = relationSingle[1:]\n",
    "        elif relationSingle[-1] in conjuction_entity_list:\n",
    "            all_relations_list[index][relationIndex] = relationSingle[:-1]\n",
    "            \n",
    "        all_relation_flatten.append(all_relations_list[index][relationIndex])\n",
    "                      \n",
    "        \n",
    "# print(all_relations_list, len(all_relations_list), \"\\n\")\n",
    "# print(all_reformatted_entities, len(all_reformatted_entities), )\n",
    "print(len(all_relations_list), len(all_relations_index), len(all_reformatted_entities), len(all_reformatted_index_list))\n",
    "\n",
    "# print out parsing result\n",
    "for index, element in enumerate(all_reformatted_entities[:15]):\n",
    "    print(element, all_reformatted_index_list[index], all_relations_list[index], all_relations_index[index])\n",
    "    \n",
    "data_result = pd.DataFrame({\n",
    "    \"Entity\":all_reformatted_entities,\n",
    "    \"EntityIndex\":all_reformatted_index_list,\n",
    "    \"Relation\":all_relations_list,\n",
    "    \"RelationIndex\":all_relations_index\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f977dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conculde dataframe with all tokens\n",
    "        \n",
    "for notIndex, notElement in enumerate(not_entity_relation_list):\n",
    "    all_element_flatten = list(filter((notElement).__ne__, all_element_flatten))\n",
    "    all_relation_flatten = list(filter((notElement).__ne__, all_relation_flatten))\n",
    "        \n",
    "data_entity = pd.DataFrame({\n",
    "    \"Element\":all_element_flatten,\n",
    "})\n",
    "\n",
    "data_relation = pd.DataFrame({\n",
    "    \"Element\":all_relation_flatten,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469f110a",
   "metadata": {},
   "source": [
    "# Remove stopwords\n",
    "# Remove entity and relation with only one char and too many chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a882fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\ipykernel_launcher.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "停用詞數目： 2003299 \n",
      "\n",
      " [\"t's\", 'twice', 'unfortunately', 'unless', 'unlikely', 'unto', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'value', 'various', 'viz', 'vs', 'want', 'wants', \"wasn't\", 'way', \"we'd\", 'welcome', \"we'll\", 'went', \"we're\", \"weren't\", \"we've\", \"what's\", \"where's\", \"who's\", 'willing', 'wish', 'wonder', \"won't\", \"wouldn't\", 'yes', \"you'd\", \"you'll\", \"you're\", \"you've\", 'zero', 'zt', 'ZT', 'zz', 'ZZ', '準備', '覆雜', '心裏', '註意', '裏面']\n"
     ]
    }
   ],
   "source": [
    "# import stopwords list\n",
    "\n",
    "stopword_list = []\n",
    "\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"./stopwords/\")):\n",
    "    if fileElement[-2:] != \"md\":\n",
    "        data_temp = pd.read_csv(\"./stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n",
    "        stopword_list += data_temp.iloc[:, 0].tolist()\n",
    "        \n",
    "stopword_list = list(dict.fromkeys(stopword_list))\n",
    "        \n",
    "print(\"停用詞數目：\", len(stopword_list), \"\\n\\n\", stopword_list[-50:])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ec444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1779/23302 [00:36<07:24, 48.39it/s]"
     ]
    }
   ],
   "source": [
    "# remove stop words and entity & relation that only occur once\n",
    "\n",
    "data_list = [data_entity, data_relation]\n",
    "\n",
    "for dataIndex, dataElement in enumerate(data_list):\n",
    "    drop_index = []\n",
    "    for rowIndex, rowElement in enumerate(tqdm(dataElement.iloc[:, 0])):\n",
    "        if rowElement in stopword_list or len(str(rowElement)) <= 1 or len(str(rowElement)) >= 8:\n",
    "            drop_index.append(rowIndex)\n",
    "    # drop entity/relations that are inside stopword list\n",
    "    print(\"Original Length:\", len(dataElement))\n",
    "    dataElement.drop(index=drop_index, inplace=True)\n",
    "    print(\"After Removal:\", len(dataElement))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683122c",
   "metadata": {},
   "source": [
    "# Caculate TFiDF and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe49ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conculde a list with frequency\n",
    "\n",
    "def ratiolize(x):\n",
    "    x = str(np.around(x * 100, decimals=2)) + \"%\"\n",
    "    return x\n",
    "\n",
    "data_entity_value_count = data_entity.value_counts(ascending=False).to_frame()\n",
    "data_entity_value_count.reset_index(inplace=True)\n",
    "data_entity_value_count = data_entity_value_count.rename(columns = {'index':'Entity'})\n",
    "data_entity_value_count = data_entity_value_count.rename(columns = {0:'Count'})\n",
    "data_entity_value_count.loc[:, \"Ratio\"] = np.around(data_entity_value_count.iloc[:, 1] / len(data_entity_value_count),\n",
    "                                                    decimals=4)\n",
    "# data_entity_value_count.loc[:, \"Ratio\"] = data_entity_value_count.loc[:, \"Ratio\"].apply(ratiolize)\n",
    "\n",
    "data_relation_value_count = data_relation.value_counts(ascending=False).to_frame()\n",
    "data_relation_value_count.reset_index(inplace=True)\n",
    "data_relation_value_count = data_relation_value_count.rename(columns = {'index':'Relation'})\n",
    "data_relation_value_count = data_relation_value_count.rename(columns = {0:'Count'})\n",
    "data_relation_value_count.loc[:, \"Ratio\"] = np.around(data_relation_value_count.iloc[:, 1] / len(data_relation_value_count),\n",
    "                                                      decimals=4)\n",
    "# data_relation_value_count.loc[:, \"Ratio\"] = data_relation_value_count.loc[:, \"Ratio\"].apply(ratiolize)\n",
    "\n",
    "\n",
    "'''\n",
    "calculate tfidf\n",
    "'''\n",
    "documentNum = len(os.listdir(\"../data/\"))\n",
    "data_all = [data_entity_value_count, data_relation_value_count]\n",
    "\n",
    "# gather all document\n",
    "textDocuments = []\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"../data/\")):\n",
    "    textElement = \"\"\n",
    "    file = codecs.open(\"../data/\" + fileElement, 'r', encoding='utf8', errors='ignore')\n",
    "    for textIndex, textLines in enumerate(file):\n",
    "        textElement += textLines\n",
    "    textDocuments.append(textElement)\n",
    "\n",
    "\n",
    "for dataIndex, dataElement in enumerate(data_all):\n",
    "    for rowIndex, rowElement in enumerate(tqdm(dataElement.iloc[:, 0])):\n",
    "        document_count = 0\n",
    "        for textIndex, textElement in enumerate(textDocuments):\n",
    "            # check if entity/relation exists in the doucument\n",
    "            if rowElement in textElement:\n",
    "                document_count += 1\n",
    "        # assign document frequency\n",
    "        dataElement.loc[rowIndex, \"DocumentFrequency\"] = document_count\n",
    "        # assign tfidf\n",
    "        dataElement.loc[rowIndex, \"tfiDF\"] = np.around(dataElement.loc[rowIndex, \"Count\"] * \\\n",
    "            math.log(documentNum / (dataElement.loc[rowIndex, \"DocumentFrequency\"] + 1), 10), decimals=4)\n",
    "        \n",
    "# data_entity_value_count.sort_values(by=[\"tfiDF\"], ascending=False, inplace=True)\n",
    "# data_relation_value_count.sort_values(by=[\"tfiDF\"], ascending=False, inplace=True)\n",
    "\n",
    "print(len(data_entity), data_entity_value_count[:10], \"\\n\")\n",
    "print(len(data_relation), data_relation_value_count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6252722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving result without filtering\n",
    "data_entity_value_count.to_csv(entity_saving_path, encoding=\"utf8\", index=None, quoting=False)\n",
    "data_relation_value_count.to_csv(relation_saving_path, encoding=\"utf8\", index=None, quoting=False)\n",
    "data_result.to_csv(main_result_saving_path, encoding=\"utf8\", index=None, quoting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d142a4",
   "metadata": {},
   "source": [
    "# Filter out most frequent edge and relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18144228",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_entity_value_count = pd.read_csv(entity_saving_path, encoding=\"utf8\")\n",
    "data_relation_value_count = pd.read_csv(relation_saving_path, encoding=\"utf8\")\n",
    "data_result = pd.read_csv(main_result_saving_path, encoding=\"utf8\")\n",
    "\n",
    "# take only first 20 places\n",
    "data_entity_filter = data_entity_value_count.loc[:, \"Element\"].tolist()[:20]\n",
    "data_relation_filter = data_relation_value_count.loc[:, \"Element\"].tolist()[:20]\n",
    "\n",
    "abandonIndexList = []\n",
    "\n",
    "for rowIndex, rowItem in enumerate(data_result.iloc[:, 0]):\n",
    "    need_to_abandon = True\n",
    "    entityList = rowItem.replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")[:-1]\n",
    "    relationList = data_result.iloc[rowIndex, 2].replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")[:-1]\n",
    "    \n",
    "    for entityIndex, entityElement in enumerate(entityList):\n",
    "        if entityElement in data_entity_filter:\n",
    "            need_to_abandon = False\n",
    "            break\n",
    "    \n",
    "    for relationIndex, relationElement in enumerate(relationList):\n",
    "        if need_to_abandon == False:\n",
    "            break\n",
    "        if relationElement in data_relation_filter:\n",
    "            need_to_abandon = False\n",
    "            break\n",
    "            \n",
    "    if need_to_abandon == True:\n",
    "        abandonIndexList.append(rowIndex)\n",
    "            \n",
    "data_result.drop(index=abandonIndexList, inplace=True)\n",
    "print(data_result)\n",
    "\n",
    "# export data\n",
    "data_result.to_csv(filtered_result_path, encoding=\"utf8\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
