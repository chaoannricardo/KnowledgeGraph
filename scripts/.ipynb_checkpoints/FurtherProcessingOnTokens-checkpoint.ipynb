{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "maritime-tennessee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Windows_Storage\\Storage\\Github\\KnowledgeGraph\\scripts\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-alfred",
   "metadata": {},
   "source": [
    "# Some Reference\n",
    "\n",
    "* Regular Expression Tester: https://regex101.com/\n",
    "* Some Re Tutorials: https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-sculpture",
   "metadata": {},
   "source": [
    "# Processing Steps\n",
    "* Divide paragraphs into sentences by Chinese punctuations (“，”, “。”, “、”, “？”, “?”).\n",
    "* Segment sentences into tokens with relative dependencies and POSs (Part of Speech).\n",
    "* Remove tokens with certain type of dependency and POS (i.e. PUNCT, x, PARENTHESISCATEGORY)\n",
    "* Remove unnecessary tokens\n",
    "  * punctuation\n",
    "  * word between two bracket\n",
    "* Extract possible tokens as candidates of entities and relations by its dependency and POS category,\n",
    "  * with regular expression (Entity, POS, dependencies): “^[V]”, “^[N]”,\n",
    "  * with certain dependencies and POS type,\n",
    "  * with given entity dictionaries (entity, POS, dependencies),\n",
    "  * with certain conjuction chars ([\"的\", \"、\", \"之\", \"及\", \"與\"]) ,\n",
    "* Clean up candidate list again by removing unnecessary tokens (conjunction characters). \n",
    "  * 晶圓的 -> 晶圓, 的製程 -> 製程, issues caused by adjectives.\n",
    "* Remove stopwords\n",
    "* Remove entity and relation with only one and more than 10 chars\n",
    "* Caculate TDiDF and sort\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Optional\n",
    "* Conclude frequency of occurrence of each node/edge candidate, keeping the first 20 ranks only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-monitor",
   "metadata": {},
   "source": [
    "# Configurations\n",
    "### Extract possible tokens as candidates of entities and relations by its dependency and POS category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "familiar-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "token_path = \"../results/210408_result/210408_dataset_monpaResult.csv\"\n",
    "entity_saving_path = \"../results/210408_result/210408_dataset_entity_result_MONPA.csv\"\n",
    "relation_saving_path = \"../results/210408_result/210408_dataset_relation_result_MONPA.csv\"\n",
    "main_result_saving_path = \"../results/210408_result/210408_dataset_main_result_MONPA.csv\"\n",
    "filtered_result_path = \"../results/210408_result/210408_dataset_filtered_result_MONPA.csv\"\n",
    "\n",
    "# token_path = \"../results/210330_result/210330_dataset_spaCyResult.csv\"\n",
    "# entity_saving_path = \"../results/210330_result/210330_dataset_entity_result_spaCy.csv\"\n",
    "# relation_saving_path = \"../results/210330_result/210330_dataset_relation_result_spaCy.csv\"\n",
    "# main_result_saving_path = \"../results/210330_result/210330_dataset_main_result_spaCy.csv\"\n",
    "# filtered_result_path = \"../results/210408_result/210408_dataset_filtered_result_spaCy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "economic-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONPA\n",
    "relation_dependencies_possible_list = []\n",
    "relation_pos_possible_list = [\"VH\", \"VC\", \"VJ\", \"VA\"]\n",
    "relation_pos_re = \"^[V]\"\n",
    "entity_dependencies_possible_list = []\n",
    "entity_pos_possible_list = [\"Na\", \"Nv\", \"Neu\", \"Nes\", \"Nf\", \"Ng\", \"Nh\", \"Neqa\", \"Nep\", \"Ncd\", \"FW\", \"DE\"]\n",
    "entity_pos_re = \"^[N]\"\n",
    "splitter_pos_list = [\"COMMACATEGORY\", \"PERIODCATEGORY\"]\n",
    "\n",
    "# spaCy\n",
    "# relation_dependencies_possible_list = [\"ROOT\", \"nmod:prep\", \"prep\", \"agent\", ]\n",
    "# relation_pos_possible_list = [\"VERB\"]\n",
    "# relation_pos_re = \"^[V]\"\n",
    "# entity_dependencies_possible_list = [\"compound:nn\", \"nsubj\", \"dep\", \"dobj\"]\n",
    "# entity_pos_possible_list = [\"NOUN\", \"PROPN\"]\n",
    "# entity_pos_re = \"^[N]\"\n",
    "# punct_pos_list = [\"PUNCT\"]\n",
    "\n",
    "# Common, usually entity\n",
    "sentences_splitter = [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\", \"；\", \";\", \".\"] + [\"\\r\\n\" * i for i in range(0, 100)]\n",
    "bracket_entity_list_first = [\"(\", \"（\", \"[\", \"［\", \"{\", \"｛\", \"<\", \"＜\", \"〔\", \"【\", \"〖\", \"《\", \"〈\"]\n",
    "bracket_entity_list_last = [\")\", \"）\", \"]\", \"］\", \"}\", \"｝\", \">\", \"＞\", \"〕\", \"】\", \"〗\", \"》\", \"〉\"]\n",
    "punct_entity_list = [\" \" * i for i in range(0, 100)]\n",
    "conjuction_entity_list = [\"的\", \"、\", \"之\", \"及\", \"與\"]\n",
    "not_entity_relation_list = [\"的\", \"、\", \"之\", \"及\", \"與\", \"\\r\\n \\r\\n \", \"\\r\\n \\r\\n  \"] +\\\n",
    "[\" \" * i for i in range(0, 100)] + [\"\\n\" * i for i in range(0, 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-internet",
   "metadata": {},
   "source": [
    "# Read in Tokens (Preparation beforehand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "diverse-compromise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143131\n",
      "  Segmented Element  POS  Dependecies\n",
      "0                12  Neu          NaN\n",
      "1                 吋   Nf          NaN\n",
      "2                晶圓   Na          NaN\n",
      "3                後段  Ncd          NaN\n",
      "4                製程   Na          NaN\n"
     ]
    }
   ],
   "source": [
    "# read in tokens\n",
    "\n",
    "dataToken = pd.read_csv(token_path, encoding=\"utf8\")\n",
    "\n",
    "# append a column for data if dependencies column does not exist (monpa case)\n",
    "if len(dataToken.columns) < 3:\n",
    "    dataToken.loc[:, \"Dependecies\"] = np.nan\n",
    "\n",
    "print(len(dataToken))\n",
    "print(dataToken.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-missouri",
   "metadata": {},
   "source": [
    "# Divide by Chinese Seperators\n",
    "# Segment sentences into tokens with relative dependencies and POSs (Part of Speech).\n",
    "# Remove tokens with certain type of dependency and POS (i.e. PUNCT, x, PARENTHESISCATEGORY)\n",
    "# Remove unnecessary tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "limited-courage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list length (Sentences):  9887 9887 9887\n",
      "['12', '吋', '晶圓', '後段', '製程', '之', '發展', '趨勢', '探討', '前瞻', '封裝', '系列', '2002年', '08月', '05日', '星期一', '瀏覽', '人次', '：', '台灣', '半導體', '產業', '結構', '完整'] ['Neu', 'Nf', 'Na', 'Ncd', 'Na', 'DE', 'Nv', 'Na', 'VE', 'FW', 'FW', 'FW', 'Nv', 'Na', 'Na', 'FW', 'FW', 'FW', 'FW', 'FW', 'Nd', 'Nd', 'Nd', 'FW', 'Nd', 'Na', 'FW', 'VC', 'Na', 'FW', 'FW', 'FW', 'FW', 'FW', 'FW', 'FW', 'FW', 'LOC', 'Na', 'Na', 'Na', 'VH'] [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "list length:  9887 9887 9887\n"
     ]
    }
   ],
   "source": [
    "# devide token by seperators\n",
    "total_entity_list = []\n",
    "sentence_entity_list = []\n",
    "total_label_list = []\n",
    "sentence_label_list = []\n",
    "total_dependencies_list = []\n",
    "sentence_dependencies_list = []\n",
    "\n",
    "\n",
    "# devide tokens by chinese punctuations\n",
    "for index, entityElement in enumerate(dataToken.iloc[:, 0]):\n",
    "    if entityElement not in sentences_splitter:\n",
    "        sentence_entity_list.append(entityElement)\n",
    "        sentence_label_list.append(dataToken.iloc[index, 1])\n",
    "        sentence_dependencies_list.append(dataToken.iloc[index, 2])\n",
    "    else:\n",
    "        total_entity_list.append(sentence_entity_list)\n",
    "        total_label_list.append(sentence_label_list)\n",
    "        total_dependencies_list.append(sentence_dependencies_list)\n",
    "        sentence_entity_list = []\n",
    "        sentence_label_list = []\n",
    "        sentence_dependencies_list = []\n",
    "    \n",
    "print(\"list length (Sentences): \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))\n",
    "\n",
    "\n",
    "# Clean up tokens and eliminate unneccesary tokens\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    for tokenIndex, tokens in enumerate(sentences):\n",
    "        # delete elements that's between two bracket\n",
    "        if sentences[tokenIndex] in bracket_entity_list_first:\n",
    "            left_bracket_index = bracket_entity_list_first.index(sentences[tokenIndex])\n",
    "            findingLimit =  (tokenIndex+11) if (tokenIndex+11) <= (len(sentences)) else len(sentences)\n",
    "            for findingLeftIndex in range(tokenIndex+1, findingLimit):\n",
    "                if sentences[findingLeftIndex] == bracket_entity_list_last[left_bracket_index]:\n",
    "                    for removalIndex in range(tokenIndex, findingLeftIndex+1):\n",
    "#                         print(total_entity_list[sentenceIndex][removalIndex])\n",
    "                        total_entity_list[sentenceIndex][removalIndex] = \"\"\n",
    "                        total_label_list[sentenceIndex][removalIndex]= \"\"\n",
    "                        total_dependencies_list[sentenceIndex][removalIndex] = \"\"\n",
    "                    break\n",
    "        elif sentences[tokenIndex] in punct_entity_list or sentences[tokenIndex] in sentences_splitter:\n",
    "            # set token that fit certain POS type to \"\"\n",
    "            total_entity_list[sentenceIndex][tokenIndex] = \"\"\n",
    "            \n",
    "    total_entity_list[sentenceIndex] = list(filter((\"\").__ne__, total_entity_list[sentenceIndex]))\n",
    "    total_label_list[sentenceIndex]  = list(filter((\"\").__ne__, total_label_list[sentenceIndex]))\n",
    "    total_dependencies_list[sentenceIndex] = list(filter((\"\").__ne__, total_dependencies_list[sentenceIndex]))\n",
    "                \n",
    "print(total_entity_list[0] , total_label_list[0], total_dependencies_list[0])\n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-israeli",
   "metadata": {},
   "source": [
    "# Extract possible tokens as candidates of entities and relations by its dependency and POS category.\n",
    "#  Clean up candidate list again by removing unnecessary tokens (conjunction characters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "expensive-longer",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-73b421e3ccef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpossibleElement\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mentity_index_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibleIndex\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                     \u001b[0misContinuous\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                     \u001b[0mcombine_entity_name\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpossible_entities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibleIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                     \u001b[0misContinuous\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "# get entity pairs and relations\n",
    "\n",
    "# get relations\n",
    "all_relations_list = []\n",
    "all_relations_index = []\n",
    "all_element_flatten = []\n",
    "all_relation_flatten = []\n",
    "\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    relation_list = []\n",
    "    relation_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1: \n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in relation_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in relation_pos_possible_list or\\\n",
    "            re.match(relation_pos_re, str(total_label_list[sentenceIndex][tokenIndex]), flags=re.IGNORECASE) != None:\n",
    "                relation_list.append(token)\n",
    "                relation_index_list.append(tokenIndex)\n",
    "    all_relations_list.append(relation_list)\n",
    "    all_relations_index.append(relation_index_list)\n",
    "\n",
    "\n",
    "# get entities\n",
    "all_reformatted_entities = []\n",
    "all_reformatted_index_list = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    entity_index_list = []\n",
    "    possible_entities = []\n",
    "    reformatted_entities = []\n",
    "    reformatted_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1:\n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in entity_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in entity_pos_possible_list or\\\n",
    "            re.match(entity_pos_re, str(total_label_list[sentenceIndex][tokenIndex]), flags=re.IGNORECASE) != None or\\\n",
    "            token in conjuction_entity_list:\n",
    "                entity_index_list.append(tokenIndex)\n",
    "                possible_entities.append(token)\n",
    "    # combine token if situate next to each other\n",
    "#     print(entity_index_list, possible_entities)\n",
    "    if len(possible_entities) > 0:\n",
    "        combine_entity_name = possible_entities[0]\n",
    "        for possibleIndex, possibleElement in enumerate(entity_index_list):\n",
    "#             print(possible_entities)\n",
    "            \n",
    "            isContinuous = False\n",
    "            if possibleIndex != 0:\n",
    "                if possibleElement == entity_index_list[possibleIndex - 1] + 1:\n",
    "                    isContinuous = True\n",
    "                    combine_entity_name += str(possible_entities[possibleIndex])\n",
    "                else:\n",
    "                    isContinuous = False\n",
    "            \n",
    "                if isContinuous == False:\n",
    "                    reformatted_entities.append(combine_entity_name)\n",
    "                    reformatted_index_list.append(entity_index_list[possibleIndex-1])\n",
    "                    combine_entity_name = possible_entities[possibleIndex]\n",
    "                    \n",
    "            if possibleIndex == (len(entity_index_list) - 1):\n",
    "                reformatted_entities.append(combine_entity_name)\n",
    "                reformatted_index_list.append(possibleElement)\n",
    "                \n",
    "#             print(combine_entity_name)     \n",
    "#     print(reformatted_entities)\n",
    "    all_reformatted_entities.append(reformatted_entities)\n",
    "    all_reformatted_index_list.append(reformatted_index_list)\n",
    "    \n",
    "# flatten section\n",
    "# remove unneccessary conjuction words\n",
    "for index, element in enumerate(all_reformatted_entities):\n",
    "    \n",
    "    for elementIndex, elementSingle in enumerate(element):\n",
    "        # remove single conjuction char in first or last position\n",
    "        if elementSingle[0] in conjuction_entity_list:\n",
    "            element[elementIndex] = elementSingle[1:]\n",
    "        elif elementSingle[-1] in conjuction_entity_list:\n",
    "            element[elementIndex] = elementSingle[:-1]\n",
    "            \n",
    "        all_element_flatten.append(element[elementIndex])\n",
    "        \n",
    "    for relationIndex, relationSingle in enumerate(all_relations_list[index]):\n",
    "        # remove single conjuction char in first or last position\n",
    "        if relationSingle[0] in conjuction_entity_list:\n",
    "            all_relations_list[index][relationIndex] = relationSingle[1:]\n",
    "        elif relationSingle[-1] in conjuction_entity_list:\n",
    "            all_relations_list[index][relationIndex] = relationSingle[:-1]\n",
    "            \n",
    "        all_relation_flatten.append(all_relations_list[index][relationIndex])\n",
    "                      \n",
    "        \n",
    "# print(all_relations_list, len(all_relations_list), \"\\n\")\n",
    "# print(all_reformatted_entities, len(all_reformatted_entities), )\n",
    "print(len(all_relations_list), len(all_relations_index), len(all_reformatted_entities), len(all_reformatted_index_list))\n",
    "\n",
    "# print out parsing result\n",
    "for index, element in enumerate(all_reformatted_entities[:15]):\n",
    "    print(element, all_reformatted_index_list[index], all_relations_list[index], all_relations_index[index])\n",
    "    \n",
    "data_result = pd.DataFrame({\n",
    "    \"Entity\":all_reformatted_entities,\n",
    "    \"EntityIndex\":all_reformatted_index_list,\n",
    "    \"Relation\":all_relations_list,\n",
    "    \"RelationIndex\":all_relations_index\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-parts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conculde dataframe with all tokens\n",
    "        \n",
    "for notIndex, notElement in enumerate(not_entity_relation_list):\n",
    "    all_element_flatten = list(filter((notElement).__ne__, all_element_flatten))\n",
    "    all_relation_flatten = list(filter((notElement).__ne__, all_relation_flatten))\n",
    "        \n",
    "data_entity = pd.DataFrame({\n",
    "    \"Element\":all_element_flatten,\n",
    "})\n",
    "\n",
    "data_relation = pd.DataFrame({\n",
    "    \"Element\":all_relation_flatten,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-rwanda",
   "metadata": {},
   "source": [
    "# Remove stopwords\n",
    "# Remove entity and relation with only one char and too many chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords list\n",
    "\n",
    "stopword_list = []\n",
    "\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"./stopwords/\")):\n",
    "    if fileElement[-2:] != \"md\":\n",
    "        data_temp = pd.read_csv(\"./stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n",
    "        stopword_list += data_temp.iloc[:, 0].tolist()\n",
    "        \n",
    "stopword_list = list(dict.fromkeys(stopword_list))\n",
    "        \n",
    "print(\"停用詞數目：\", len(stopword_list), \"\\n\\n\", stopword_list[-50:])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words and entity & relation that only occur once\n",
    "\n",
    "data_list = [data_entity, data_relation]\n",
    "\n",
    "for dataIndex, dataElement in enumerate(data_list):\n",
    "    drop_index = []\n",
    "    for rowIndex, rowElement in enumerate(dataElement.iloc[:, 0]):\n",
    "        if rowElement in stopword_list or len(str(rowElement)) <= 1 or len(str(rowElement)) >= 8:\n",
    "            drop_index.append(rowIndex)\n",
    "    # drop entity/relations that are inside stopword list\n",
    "    print(\"Original Length:\", len(dataElement))\n",
    "    dataElement.drop(index=drop_index, inplace=True)\n",
    "    print(\"After Removal:\", len(dataElement))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-zimbabwe",
   "metadata": {},
   "source": [
    "# Caculate TDiDF and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conculde a list with frequency\n",
    "\n",
    "def ratiolize(x):\n",
    "    x = str(np.around(x * 100, decimals=2)) + \"%\"\n",
    "    return x\n",
    "\n",
    "data_entity_value_count = data_entity.value_counts(ascending=False).to_frame()\n",
    "data_entity_value_count.reset_index(inplace=True)\n",
    "data_entity_value_count = data_entity_value_count.rename(columns = {'index':'Entity'})\n",
    "data_entity_value_count = data_entity_value_count.rename(columns = {0:'Count'})\n",
    "data_entity_value_count.loc[:, \"Ratio\"] = np.around(data_entity_value_count.iloc[:, 1] / len(data_entity_value_count),\n",
    "                                                    decimals=4)\n",
    "# data_entity_value_count.loc[:, \"Ratio\"] = data_entity_value_count.loc[:, \"Ratio\"].apply(ratiolize)\n",
    "\n",
    "data_relation_value_count = data_relation.value_counts(ascending=False).to_frame()\n",
    "data_relation_value_count.reset_index(inplace=True)\n",
    "data_relation_value_count = data_relation_value_count.rename(columns = {'index':'Relation'})\n",
    "data_relation_value_count = data_relation_value_count.rename(columns = {0:'Count'})\n",
    "data_relation_value_count.loc[:, \"Ratio\"] = np.around(data_relation_value_count.iloc[:, 1] / len(data_relation_value_count),\n",
    "                                                      decimals=4)\n",
    "# data_relation_value_count.loc[:, \"Ratio\"] = data_relation_value_count.loc[:, \"Ratio\"].apply(ratiolize)\n",
    "\n",
    "\n",
    "'''\n",
    "calculate tfidf\n",
    "'''\n",
    "documentNum = len(os.listdir(\"../data/\"))\n",
    "data_all = [data_entity_value_count, data_relation_value_count]\n",
    "\n",
    "# gather all document\n",
    "textDocuments = []\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"../data/\")):\n",
    "    textElement = \"\"\n",
    "    file = codecs.open(\"../data/\" + fileElement, 'r', encoding='utf8', errors='ignore')\n",
    "    for textIndex, textLines in enumerate(file):\n",
    "        textElement += textLines\n",
    "    textDocuments.append(textElement)\n",
    "\n",
    "\n",
    "for dataIndex, dataElement in enumerate(data_all):\n",
    "    for rowIndex, rowElement in enumerate(tqdm(dataElement.iloc[:, 0])):\n",
    "        document_count = 0\n",
    "        for textIndex, textElement in enumerate(textDocuments):\n",
    "            # check if entity/relation exists in the doucument\n",
    "            if rowElement in textElement:\n",
    "                document_count += 1\n",
    "        # assign document frequency\n",
    "        dataElement.loc[rowIndex, \"DocumentFrequency\"] = document_count\n",
    "        # assign tfidf\n",
    "        dataElement.loc[rowIndex, \"tfiDF\"] = np.around(dataElement.loc[rowIndex, \"Count\"] * \\\n",
    "            math.log(documentNum / (dataElement.loc[rowIndex, \"DocumentFrequency\"] + 1), 10), decimals=4)\n",
    "        \n",
    "# data_entity_value_count.sort_values(by=[\"tfiDF\"], ascending=False, inplace=True)\n",
    "# data_relation_value_count.sort_values(by=[\"tfiDF\"], ascending=False, inplace=True)\n",
    "\n",
    "print(len(data_entity), data_entity_value_count[:10], \"\\n\")\n",
    "print(len(data_relation), data_relation_value_count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving result without filtering\n",
    "data_entity_value_count.to_csv(entity_saving_path, encoding=\"utf8\", index=None, quoting=False)\n",
    "data_relation_value_count.to_csv(relation_saving_path, encoding=\"utf8\", index=None, quoting=False)\n",
    "data_result.to_csv(main_result_saving_path, encoding=\"utf8\", index=None, quoting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-council",
   "metadata": {},
   "source": [
    "# Filter out most frequent edge and relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_entity_value_count = pd.read_csv(entity_saving_path, encoding=\"utf8\")\n",
    "data_relation_value_count = pd.read_csv(relation_saving_path, encoding=\"utf8\")\n",
    "data_result = pd.read_csv(main_result_saving_path, encoding=\"utf8\")\n",
    "\n",
    "# take only first 20 places\n",
    "data_entity_filter = data_entity_value_count.loc[:, \"Element\"].tolist()[:20]\n",
    "data_relation_filter = data_relation_value_count.loc[:, \"Element\"].tolist()[:20]\n",
    "\n",
    "abandonIndexList = []\n",
    "\n",
    "for rowIndex, rowItem in enumerate(data_result.iloc[:, 0]):\n",
    "    need_to_abandon = True\n",
    "    entityList = rowItem.replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")[:-1]\n",
    "    relationList = data_result.iloc[rowIndex, 2].replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")[:-1]\n",
    "    \n",
    "    for entityIndex, entityElement in enumerate(entityList):\n",
    "        if entityElement in data_entity_filter:\n",
    "            need_to_abandon = False\n",
    "            break\n",
    "    \n",
    "    for relationIndex, relationElement in enumerate(relationList):\n",
    "        if need_to_abandon == False:\n",
    "            break\n",
    "        if relationElement in data_relation_filter:\n",
    "            need_to_abandon = False\n",
    "            break\n",
    "            \n",
    "    if need_to_abandon == True:\n",
    "        abandonIndexList.append(rowIndex)\n",
    "            \n",
    "data_result.drop(index=abandonIndexList, inplace=True)\n",
    "print(data_result)\n",
    "\n",
    "# export data\n",
    "data_result.to_csv(filtered_result_path, encoding=\"utf8\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monpa",
   "language": "python",
   "name": "monpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
