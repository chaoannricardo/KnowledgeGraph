{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "weird-forestry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Windows_Storage\\Storage\\Github\\KnowledgeGraph\\scripts\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-better",
   "metadata": {},
   "source": [
    "# Some Reference\n",
    "\n",
    "* Regular Expression Tester: https://regex101.com/\n",
    "* Some Re Tutorials: https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-wellington",
   "metadata": {},
   "source": [
    "# Processing Steps\n",
    "* Divide paragraphs into sentences by Chinese punctuations (“，”, “。”, “、”, “？”, “?”).\n",
    "* Segment sentences into tokens with relative dependencies and POSs (Part of Speech).\n",
    "* Remove tokens with certain type of dependency and POS (i.e. PUNCT, x, PARENTHESISCATEGORY)\n",
    "* Remove unnecessary tokens\n",
    "  * punctuation\n",
    "  * word between two bracket\n",
    "* Extract possible tokens as candidates of entities and relations by its dependency and POS category,\n",
    "  * with regular expression (Entity, POS, dependencies): “^[V]”, “^[N]”,\n",
    "  * with certain dependencies and POS type,\n",
    "  * with given entity dictionaries (entity, POS, dependencies),\n",
    "  * with certain conjuction chars ([\"的\", \"、\", \"之\", \"及\", \"與\"]) ,\n",
    "* Clean up candidate list again by removing unnecessary tokens (conjunction characters). \n",
    "  * 晶圓的 -> 晶圓, 的製程 -> 製程, issues caused by adjectives.\n",
    "* Remove stopwords\n",
    "* Remove entity and relation with only one and more than 10 chars\n",
    "* Caculate TDiDF and sort\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Optional\n",
    "* Conclude frequency of occurrence of each node/edge candidate, keeping the first 20 ranks only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-stocks",
   "metadata": {},
   "source": [
    "# Configurations\n",
    "### Extract possible tokens as candidates of entities and relations by its dependency and POS category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "funny-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "token_path = \"../results/210330_result/210330_dataset_monpaResult.csv\"\n",
    "entity_saving_path = \"../results/210408_result/210408_dataset_entity_result_MONPA.csv\"\n",
    "relation_saving_path = \"../results/210408_result/210408_dataset_relation_result_MONPA.csv\"\n",
    "main_result_saving_path = \"../results/210408_result/210408_dataset_main_result_MONPA.csv\"\n",
    "filtered_result_path = \"../results/210408_result/210408_dataset_filtered_result_MONPA.csv\"\n",
    "\n",
    "# token_path = \"../results/210330_result/210330_dataset_spaCyResult.csv\"\n",
    "# entity_saving_path = \"../results/210330_result/210330_dataset_entity_result_spaCy.csv\"\n",
    "# relation_saving_path = \"../results/210330_result/210330_dataset_relation_result_spaCy.csv\"\n",
    "# main_result_saving_path = \"../results/210330_result/210330_dataset_main_result_spaCy.csv\"\n",
    "# filtered_result_path = \"../results/210408_result/210408_dataset_filtered_result_spaCy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hawaiian-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONPA\n",
    "relation_dependencies_possible_list = []\n",
    "relation_pos_possible_list = [\"VH\", \"VC\", \"VJ\", \"VA\"]\n",
    "relation_pos_re = \"^[V]\"\n",
    "entity_dependencies_possible_list = []\n",
    "entity_pos_possible_list = [\"Na\", \"Nv\", \"Neu\", \"Nes\", \"Nf\", \"Ng\", \"Nh\", \"Neqa\", \"Nep\", \"Ncd\", \"FW\", \"DE\"]\n",
    "entity_pos_re = \"^[N]\"\n",
    "splitter_pos_list = [\"COMMACATEGORY\", \"PERIODCATEGORY\"]\n",
    "\n",
    "# spaCy\n",
    "# relation_dependencies_possible_list = [\"ROOT\", \"nmod:prep\", \"prep\", \"agent\", ]\n",
    "# relation_pos_possible_list = [\"VERB\"]\n",
    "# relation_pos_re = \"^[V]\"\n",
    "# entity_dependencies_possible_list = [\"compound:nn\", \"nsubj\", \"dep\", \"dobj\"]\n",
    "# entity_pos_possible_list = [\"NOUN\", \"PROPN\"]\n",
    "# entity_pos_re = \"^[N]\"\n",
    "# punct_pos_list = [\"PUNCT\"]\n",
    "\n",
    "# Common, usually entity\n",
    "sentences_splitter = [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\", \"；\", \";\", \".\"] + [\"\\r\\n\" * i for i in range(0, 100)]\n",
    "bracket_entity_list_first = [\"(\", \"（\", \"[\", \"［\", \"{\", \"｛\", \"<\", \"＜\", \"〔\", \"【\", \"〖\", \"《\", \"〈\"]\n",
    "bracket_entity_list_last = [\")\", \"）\", \"]\", \"］\", \"}\", \"｝\", \">\", \"＞\", \"〕\", \"】\", \"〗\", \"》\", \"〉\"]\n",
    "punct_entity_list = [\" \" * i for i in range(0, 100)]\n",
    "conjuction_entity_list = [\"的\", \"、\", \"之\", \"及\", \"與\"]\n",
    "not_entity_relation_list = [\"的\", \"、\", \"之\", \"及\", \"與\", \"\\r\\n \\r\\n \", \"\\r\\n \\r\\n  \"] +\\\n",
    "[\" \" * i for i in range(0, 100)] + [\"\\n\" * i for i in range(0, 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-white",
   "metadata": {},
   "source": [
    "# Read in Tokens (Preparation beforehand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "muslim-excerpt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114798\n",
      "  Segmented Element                  POS  Dependecies\n",
      "0            安森美半導體                  NaN          NaN\n",
      "1                 （  PARENTHESISCATEGORY          NaN\n",
      "2  ON Semiconductor                   FW          NaN\n",
      "3                 ）  PARENTHESISCATEGORY          NaN\n",
      "4                 ，        COMMACATEGORY          NaN\n"
     ]
    }
   ],
   "source": [
    "# read in tokens\n",
    "\n",
    "dataToken = pd.read_csv(token_path, encoding=\"utf8\")\n",
    "\n",
    "# append a column for data if dependencies column does not exist (monpa case)\n",
    "if len(dataToken.columns) < 3:\n",
    "    dataToken.loc[:, \"Dependecies\"] = np.nan\n",
    "\n",
    "print(len(dataToken))\n",
    "print(dataToken.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-picture",
   "metadata": {},
   "source": [
    "# Divide by Chinese Seperators\n",
    "# Segment sentences into tokens with relative dependencies and POSs (Part of Speech).\n",
    "# Remove tokens with certain type of dependency and POS (i.e. PUNCT, x, PARENTHESISCATEGORY)\n",
    "# Remove unnecessary tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brave-answer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list length (Sentences):  7865 7865 7865\n",
      "['安森美半導體'] [nan] [nan]\n",
      "list length:  7865 7865 7865\n"
     ]
    }
   ],
   "source": [
    "# devide token by seperators\n",
    "total_entity_list = []\n",
    "sentence_entity_list = []\n",
    "total_label_list = []\n",
    "sentence_label_list = []\n",
    "total_dependencies_list = []\n",
    "sentence_dependencies_list = []\n",
    "\n",
    "\n",
    "# devide tokens by chinese punctuations\n",
    "for index, entityElement in enumerate(dataToken.iloc[:, 0]):\n",
    "    if entityElement not in sentences_splitter:\n",
    "        sentence_entity_list.append(entityElement)\n",
    "        sentence_label_list.append(dataToken.iloc[index, 1])\n",
    "        sentence_dependencies_list.append(dataToken.iloc[index, 2])\n",
    "    else:\n",
    "        total_entity_list.append(sentence_entity_list)\n",
    "        total_label_list.append(sentence_label_list)\n",
    "        total_dependencies_list.append(sentence_dependencies_list)\n",
    "        sentence_entity_list = []\n",
    "        sentence_label_list = []\n",
    "        sentence_dependencies_list = []\n",
    "    \n",
    "print(\"list length (Sentences): \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))\n",
    "\n",
    "\n",
    "# Clean up tokens and eliminate unneccesary tokens\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    for tokenIndex, tokens in enumerate(sentences):\n",
    "        # delete elements that's between two bracket\n",
    "        if sentences[tokenIndex] in bracket_entity_list_first:\n",
    "            left_bracket_index = bracket_entity_list_first.index(sentences[tokenIndex])\n",
    "            findingLimit =  (tokenIndex+11) if (tokenIndex+11) <= (len(sentences)) else len(sentences)\n",
    "            for findingLeftIndex in range(tokenIndex+1, findingLimit):\n",
    "                if sentences[findingLeftIndex] == bracket_entity_list_last[left_bracket_index]:\n",
    "                    for removalIndex in range(tokenIndex, findingLeftIndex+1):\n",
    "#                         print(total_entity_list[sentenceIndex][removalIndex])\n",
    "                        total_entity_list[sentenceIndex][removalIndex] = \"\"\n",
    "                        total_label_list[sentenceIndex][removalIndex]= \"\"\n",
    "                        total_dependencies_list[sentenceIndex][removalIndex] = \"\"\n",
    "                    break\n",
    "        elif sentences[tokenIndex] in punct_entity_list or sentences[tokenIndex] in sentences_splitter:\n",
    "            # set token that fit certain POS type to \"\"\n",
    "            total_entity_list[sentenceIndex][tokenIndex] = \"\"\n",
    "            \n",
    "    total_entity_list[sentenceIndex] = list(filter((\"\").__ne__, total_entity_list[sentenceIndex]))\n",
    "    total_label_list[sentenceIndex]  = list(filter((\"\").__ne__, total_label_list[sentenceIndex]))\n",
    "    total_dependencies_list[sentenceIndex] = list(filter((\"\").__ne__, total_dependencies_list[sentenceIndex]))\n",
    "                \n",
    "print(total_entity_list[0] , total_label_list[0], total_dependencies_list[0])\n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-september",
   "metadata": {},
   "source": [
    "# Extract possible tokens as candidates of entities and relations by its dependency and POS category.\n",
    "#  Clean up candidate list again by removing unnecessary tokens (conjunction characters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "opposed-shipping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7865 7865 7865 7865\n",
      "[] [] [] []\n",
      "['今天', 'RDM系列矽光電倍增管陣列'] [0, 7] ['發表', '新'] [1, 2]\n",
      "['光學雷達感測器能力擴展到其', ''] [6, 8] ['廣泛', '智慧感測方案'] [7, 9]\n",
      "['ArrayRDM-0112A20-QFN', '市場上首款', '車規的SiPM產品'] [0, 5, 10] ['符合'] [6]\n",
      "['汽車產業及其他領域LiDAR應用中', '需求'] [9, 13] ['滿足', '增長'] [1, 11]\n",
      "['ArrayRDM-0112A20-QFN', '單片1×12SiPM像素陣列'] [0, 8] [] []\n",
      "['安森美半導體', '市場的RDM製程'] [1, 5] ['領先'] [2]\n",
      "['外 光的高'] [8] ['實現', '近', '紅', '靈敏度'] [1, 3, 4, 9]\n",
      "['905奈米', '業界的18.5％的光子偵測效率'] [3, 14] ['達到', '領先'] [5, 6]\n",
      "['SiPM', '內部增益', '其靈敏度', '光子水準'] [1, 4, 7, 12] ['高', '使', '達到'] [2, 5, 9]\n",
      "['功能', 'PDE結合使用'] [2, 6] ['高'] [3]\n",
      "['', '信號'] [4, 6] ['檢測', '微弱', '返回'] [1, 3, 5]\n",
      "[] [] [] []\n",
      "['反射目標'] [4] ['低'] [2]\n",
      "['距離'] [6] ['偵測到', '遠'] [2, 4]\n"
     ]
    }
   ],
   "source": [
    "# get entity pairs and relations\n",
    "\n",
    "# get relations\n",
    "all_relations_list = []\n",
    "all_relations_index = []\n",
    "all_element_flatten = []\n",
    "all_relation_flatten = []\n",
    "\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    relation_list = []\n",
    "    relation_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1: \n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in relation_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in relation_pos_possible_list or\\\n",
    "            re.match(relation_pos_re, str(total_label_list[sentenceIndex][tokenIndex]), flags=re.IGNORECASE) != None:\n",
    "                relation_list.append(token)\n",
    "                relation_index_list.append(tokenIndex)\n",
    "    all_relations_list.append(relation_list)\n",
    "    all_relations_index.append(relation_index_list)\n",
    "\n",
    "\n",
    "# get entities\n",
    "all_reformatted_entities = []\n",
    "all_reformatted_index_list = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    entity_index_list = []\n",
    "    possible_entities = []\n",
    "    reformatted_entities = []\n",
    "    reformatted_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1:\n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in entity_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in entity_pos_possible_list or\\\n",
    "            re.match(entity_pos_re, str(total_label_list[sentenceIndex][tokenIndex]), flags=re.IGNORECASE) != None or\\\n",
    "            token in conjuction_entity_list:\n",
    "                entity_index_list.append(tokenIndex)\n",
    "                possible_entities.append(token)\n",
    "    # combine token if situate next to each other\n",
    "#     print(entity_index_list, possible_entities)\n",
    "    if len(possible_entities) > 0:\n",
    "        combine_entity_name = possible_entities[0]\n",
    "        for possibleIndex, possibleElement in enumerate(entity_index_list):\n",
    "#             print(possible_entities)\n",
    "            \n",
    "            isContinuous = False\n",
    "            if possibleIndex != 0:\n",
    "                if possibleElement == entity_index_list[possibleIndex - 1] + 1:\n",
    "                    isContinuous = True\n",
    "                    combine_entity_name += possible_entities[possibleIndex]\n",
    "                else:\n",
    "                    isContinuous = False\n",
    "            \n",
    "                if isContinuous == False:\n",
    "                    reformatted_entities.append(combine_entity_name)\n",
    "                    reformatted_index_list.append(entity_index_list[possibleIndex-1])\n",
    "                    combine_entity_name = possible_entities[possibleIndex]\n",
    "                    \n",
    "            if possibleIndex == (len(entity_index_list) - 1):\n",
    "                reformatted_entities.append(combine_entity_name)\n",
    "                reformatted_index_list.append(possibleElement)\n",
    "                \n",
    "#             print(combine_entity_name)     \n",
    "#     print(reformatted_entities)\n",
    "    all_reformatted_entities.append(reformatted_entities)\n",
    "    all_reformatted_index_list.append(reformatted_index_list)\n",
    "    \n",
    "# flatten section\n",
    "# remove unneccessary conjuction words\n",
    "for index, element in enumerate(all_reformatted_entities):\n",
    "    \n",
    "    for elementIndex, elementSingle in enumerate(element):\n",
    "        # remove single conjuction char in first or last position\n",
    "        if elementSingle[0] in conjuction_entity_list:\n",
    "            element[elementIndex] = elementSingle[1:]\n",
    "        elif elementSingle[-1] in conjuction_entity_list:\n",
    "            element[elementIndex] = elementSingle[:-1]\n",
    "            \n",
    "        all_element_flatten.append(element[elementIndex])\n",
    "        \n",
    "    for relationIndex, relationSingle in enumerate(all_relations_list[index]):\n",
    "        # remove single conjuction char in first or last position\n",
    "        if relationSingle[0] in conjuction_entity_list:\n",
    "            all_relations_list[index][relationIndex] = relationSingle[1:]\n",
    "        elif relationSingle[-1] in conjuction_entity_list:\n",
    "            all_relations_list[index][relationIndex] = relationSingle[:-1]\n",
    "            \n",
    "        all_relation_flatten.append(all_relations_list[index][relationIndex])\n",
    "                      \n",
    "        \n",
    "# print(all_relations_list, len(all_relations_list), \"\\n\")\n",
    "# print(all_reformatted_entities, len(all_reformatted_entities), )\n",
    "print(len(all_relations_list), len(all_relations_index), len(all_reformatted_entities), len(all_reformatted_index_list))\n",
    "\n",
    "# print out parsing result\n",
    "for index, element in enumerate(all_reformatted_entities[:15]):\n",
    "    print(element, all_reformatted_index_list[index], all_relations_list[index], all_relations_index[index])\n",
    "    \n",
    "data_result = pd.DataFrame({\n",
    "    \"Entity\":all_reformatted_entities,\n",
    "    \"EntityIndex\":all_reformatted_index_list,\n",
    "    \"Relation\":all_relations_list,\n",
    "    \"RelationIndex\":all_relations_index\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "opened-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conculde dataframe with all tokens\n",
    "        \n",
    "for notIndex, notElement in enumerate(not_entity_relation_list):\n",
    "    all_element_flatten = list(filter((notElement).__ne__, all_element_flatten))\n",
    "    all_relation_flatten = list(filter((notElement).__ne__, all_relation_flatten))\n",
    "        \n",
    "data_entity = pd.DataFrame({\n",
    "    \"Element\":all_element_flatten,\n",
    "})\n",
    "\n",
    "data_relation = pd.DataFrame({\n",
    "    \"Element\":all_relation_flatten,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-shannon",
   "metadata": {},
   "source": [
    "# Remove stopwords\n",
    "# Remove entity and relation with only one char and too many chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "governmental-potter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "停用詞數目： 3249 \n",
      "\n",
      " ['where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', \"who's\", 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', \"won't\", 'would', \"wouldn't\", 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'zero', 'zt', 'ZT', 'zz', 'ZZ', '準備', '覆雜', '心裏', '註意', '裏面']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-8f51581c4209>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data_temp = pd.read_csv(\"./stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n"
     ]
    }
   ],
   "source": [
    "# import stopwords list\n",
    "\n",
    "stopword_list = []\n",
    "\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"./stopwords/\")):\n",
    "    if fileElement[-2:] != \"md\":\n",
    "        data_temp = pd.read_csv(\"./stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n",
    "        stopword_list += data_temp.iloc[:, 0].tolist()\n",
    "        \n",
    "stopword_list = list(dict.fromkeys(stopword_list))\n",
    "        \n",
    "print(\"停用詞數目：\", len(stopword_list), \"\\n\\n\", stopword_list[-50:])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "located-plane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Length: 18571\n",
      "After Removal: 12744\n",
      "Original Length: 14218\n",
      "After Removal: 8098\n"
     ]
    }
   ],
   "source": [
    "# remove stop words and entity & relation that only occur once\n",
    "\n",
    "data_list = [data_entity, data_relation]\n",
    "\n",
    "for dataIndex, dataElement in enumerate(data_list):\n",
    "    drop_index = []\n",
    "    for rowIndex, rowElement in enumerate(dataElement.iloc[:, 0]):\n",
    "        if rowElement in stopword_list or len(str(rowElement)) <= 1 or len(str(rowElement)) >= 8:\n",
    "            drop_index.append(rowIndex)\n",
    "    # drop entity/relations that are inside stopword list\n",
    "    print(\"Original Length:\", len(dataElement))\n",
    "    dataElement.drop(index=drop_index, inplace=True)\n",
    "    print(\"After Removal:\", len(dataElement))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-course",
   "metadata": {},
   "source": [
    "# Caculate TDiDF and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "indian-workplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8925/8925 [00:06<00:00, 1462.40it/s]\n",
      "100%|██████████| 2809/2809 [00:01<00:00, 1550.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12744   Element  Count   Ratio  DocumentFrequency    tfiDF\n",
      "0      晶圓     82  0.0092               14.0   0.0000\n",
      "1      技術     68  0.0076               14.0   0.0000\n",
      "2      製程     61  0.0068               15.0  -1.7098\n",
      "3      晶片     53  0.0059               13.0   1.5881\n",
      "4      產品     48  0.0054               13.0   1.4382\n",
      "5      設備     37  0.0041               11.0   3.5857\n",
      "6      表面     37  0.0041               12.0   2.2995\n",
      "7      資源     35  0.0039                4.0  16.6992\n",
      "8      成本     35  0.0039               10.0   4.7145\n",
      "9      測品     32  0.0036                2.0  22.3670 \n",
      "\n",
      "8098   Element  Count   Ratio  DocumentFrequency    tfiDF\n",
      "0      回收    131  0.0466                4.0  62.5029\n",
      "1      利用    108  0.0384                5.0  42.9775\n",
      "2      製造    106  0.0377               14.0   0.0000\n",
      "3      進行     90  0.0320                5.0  35.8146\n",
      "4      包括     86  0.0306               11.0   8.3343\n",
      "5      提供     68  0.0242               11.0   6.5899\n",
      "6      萃取     64  0.0228                1.0  56.0039\n",
      "7      不同     64  0.0228                5.0  25.4682\n",
      "8      清洗     61  0.0217                9.0  10.7416\n",
      "9      增加     60  0.0214               12.0   3.7289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# conculde a list with frequency\n",
    "\n",
    "def ratiolize(x):\n",
    "    x = str(np.around(x * 100, decimals=2)) + \"%\"\n",
    "    return x\n",
    "\n",
    "data_entity_value_count = data_entity.value_counts(ascending=False).to_frame()\n",
    "data_entity_value_count.reset_index(inplace=True)\n",
    "data_entity_value_count = data_entity_value_count.rename(columns = {'index':'Entity'})\n",
    "data_entity_value_count = data_entity_value_count.rename(columns = {0:'Count'})\n",
    "data_entity_value_count.loc[:, \"Ratio\"] = np.around(data_entity_value_count.iloc[:, 1] / len(data_entity_value_count),\n",
    "                                                    decimals=4)\n",
    "# data_entity_value_count.loc[:, \"Ratio\"] = data_entity_value_count.loc[:, \"Ratio\"].apply(ratiolize)\n",
    "\n",
    "data_relation_value_count = data_relation.value_counts(ascending=False).to_frame()\n",
    "data_relation_value_count.reset_index(inplace=True)\n",
    "data_relation_value_count = data_relation_value_count.rename(columns = {'index':'Relation'})\n",
    "data_relation_value_count = data_relation_value_count.rename(columns = {0:'Count'})\n",
    "data_relation_value_count.loc[:, \"Ratio\"] = np.around(data_relation_value_count.iloc[:, 1] / len(data_relation_value_count),\n",
    "                                                      decimals=4)\n",
    "# data_relation_value_count.loc[:, \"Ratio\"] = data_relation_value_count.loc[:, \"Ratio\"].apply(ratiolize)\n",
    "\n",
    "\n",
    "'''\n",
    "calculate tfidf\n",
    "'''\n",
    "documentNum = len(os.listdir(\"../data/\"))\n",
    "data_all = [data_entity_value_count, data_relation_value_count]\n",
    "\n",
    "# gather all document\n",
    "textDocuments = []\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"../data/\")):\n",
    "    textElement = \"\"\n",
    "    file = codecs.open(\"../data/\" + fileElement, 'r', encoding='utf8', errors='ignore')\n",
    "    for textIndex, textLines in enumerate(file):\n",
    "        textElement += textLines\n",
    "    textDocuments.append(textElement)\n",
    "\n",
    "\n",
    "for dataIndex, dataElement in enumerate(data_all):\n",
    "    for rowIndex, rowElement in enumerate(tqdm(dataElement.iloc[:, 0])):\n",
    "        document_count = 0\n",
    "        for textIndex, textElement in enumerate(textDocuments):\n",
    "            # check if entity/relation exists in the doucument\n",
    "            if rowElement in textElement:\n",
    "                document_count += 1\n",
    "        # assign document frequency\n",
    "        dataElement.loc[rowIndex, \"DocumentFrequency\"] = document_count\n",
    "        # assign tfidf\n",
    "        dataElement.loc[rowIndex, \"tfiDF\"] = np.around(dataElement.loc[rowIndex, \"Count\"] * \\\n",
    "            math.log(documentNum / (dataElement.loc[rowIndex, \"DocumentFrequency\"] + 1), 10), decimals=4)\n",
    "        \n",
    "# data_entity_value_count.sort_values(by=[\"tfiDF\"], ascending=False, inplace=True)\n",
    "# data_relation_value_count.sort_values(by=[\"tfiDF\"], ascending=False, inplace=True)\n",
    "\n",
    "print(len(data_entity), data_entity_value_count[:10], \"\\n\")\n",
    "print(len(data_relation), data_relation_value_count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "noted-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving result without filtering\n",
    "data_entity_value_count.to_csv(entity_saving_path, encoding=\"utf8\", index=None, quoting=False)\n",
    "data_relation_value_count.to_csv(relation_saving_path, encoding=\"utf8\", index=None, quoting=False)\n",
    "data_result.to_csv(main_result_saving_path, encoding=\"utf8\", index=None, quoting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-youth",
   "metadata": {},
   "source": [
    "# Filter out most frequent edge and relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "personal-monitoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Entity  \\\n",
      "20             ['SiPM', '陽光條件下', '距離', '時', '訊噪', '性能']   \n",
      "21                          ['其他優勢', '電源偏置', '溫度變化敏感性']   \n",
      "29                                         ['其他感知模式互補']   \n",
      "37    ['安森美半導體汽車感測部門', '總監', '', '解析度深度數據', '挑戰的微光條件...   \n",
      "39                                  ['距離、高性價比的LiDAR方案']   \n",
      "...                                                 ...   \n",
      "7849                                  [' 壞去', '達', '%']   \n",
      "7851           ['凝器置於', '控制設備(', '設備', '碳吸 附床及吸收', '後']   \n",
      "7852                         ['冷凝處理', '廢氣成分中', '溫度之不同']   \n",
      "7853                 ['冷凝作用', '兩種方式在定溫', '增加系統之壓力2在定壓']   \n",
      "7856                            ['冷凝器型式', '表面式及接觸式冷凝器']   \n",
      "\n",
      "                 EntityIndex                             Relation  \\\n",
      "20    [0, 7, 10, 13, 18, 20]    ['明亮', '進行', '長', '測', '提供', '佳']   \n",
      "21                [1, 7, 14]                     ['包括', '低', '低']   \n",
      "29                       [5]                   ['通過', '提供', '冗餘']   \n",
      "37    [3, 5, 11, 15, 23, 28]  ['資深', '表示', '提供', '高', '充滿', '準確']   \n",
      "39                       [9]                          ['提供', '遠']   \n",
      "...                      ...                                  ...   \n",
      "7849               [4, 7, 9]               ['增加', '破', '率', '98']   \n",
      "7851      [3, 9, 14, 20, 23]           [' 冷', '較', '', '處理', ')']   \n",
      "7852              [1, 6, 10]                    ['利用', '凝結', '而']   \n",
      "7853              [1, 8, 16]                           ['包括', '']   \n",
      "7856                 [4, 11]                    ['常用', '包括', '有']   \n",
      "\n",
      "               RelationIndex  \n",
      "20     [3, 8, 9, 11, 14, 16]  \n",
      "21                [2, 4, 10]  \n",
      "29                 [0, 7, 8]  \n",
      "37    [4, 7, 10, 12, 18, 25]  \n",
      "39                    [1, 2]  \n",
      "...                      ...  \n",
      "7849            [1, 2, 6, 8]  \n",
      "7851       [1, 4, 6, 12, 22]  \n",
      "7852              [3, 7, 11]  \n",
      "7853                 [3, 12]  \n",
      "7856               [1, 5, 6]  \n",
      "\n",
      "[1086 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data_entity_value_count = pd.read_csv(entity_saving_path, encoding=\"utf8\")\n",
    "data_relation_value_count = pd.read_csv(relation_saving_path, encoding=\"utf8\")\n",
    "data_result = pd.read_csv(main_result_saving_path, encoding=\"utf8\")\n",
    "\n",
    "# take only first 20 places\n",
    "data_entity_filter = data_entity_value_count.loc[:, \"Element\"].tolist()[:20]\n",
    "data_relation_filter = data_relation_value_count.loc[:, \"Element\"].tolist()[:20]\n",
    "\n",
    "abandonIndexList = []\n",
    "\n",
    "for rowIndex, rowItem in enumerate(data_result.iloc[:, 0]):\n",
    "    need_to_abandon = True\n",
    "    entityList = rowItem.replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")[:-1]\n",
    "    relationList = data_result.iloc[rowIndex, 2].replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")[:-1]\n",
    "    \n",
    "    for entityIndex, entityElement in enumerate(entityList):\n",
    "        if entityElement in data_entity_filter:\n",
    "            need_to_abandon = False\n",
    "            break\n",
    "    \n",
    "    for relationIndex, relationElement in enumerate(relationList):\n",
    "        if need_to_abandon == False:\n",
    "            break\n",
    "        if relationElement in data_relation_filter:\n",
    "            need_to_abandon = False\n",
    "            break\n",
    "            \n",
    "    if need_to_abandon == True:\n",
    "        abandonIndexList.append(rowIndex)\n",
    "            \n",
    "data_result.drop(index=abandonIndexList, inplace=True)\n",
    "print(data_result)\n",
    "\n",
    "# export data\n",
    "data_result.to_csv(filtered_result_path, encoding=\"utf8\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monpa",
   "language": "python",
   "name": "monpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
