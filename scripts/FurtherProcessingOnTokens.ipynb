{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suited-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-vessel",
   "metadata": {},
   "source": [
    "# Some Reference\n",
    "\n",
    "* Regular Expression Tester: https://regex101.com/\n",
    "* Some Re Tutorials: https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-orbit",
   "metadata": {},
   "source": [
    "# Processing Steps\n",
    "* Divide paragraphs into sentences by Chinese punctuations (“，”, “。”, “、”, “？”, “?”).\n",
    "* Segment sentences into tokens with relative dependencies and POSs (Part of Speech).\n",
    "* Remove tokens with certain type of dependency and POS (i.e. PUNCT, x, PARENTHESISCATEGORY)\n",
    "* Remove unnecessary tokens\n",
    "  * punctuation\n",
    "  * word between two bracket\n",
    "* Extract possible tokens as candidates of entities and relations by its dependency and POS category,\n",
    "  * with regular expression (Entity, POS, dependencies): “^[V]”, “^[N]”,\n",
    "  * with certain dependencies and POS type,\n",
    "  * with given entity dictionaries (entity, POS, dependencies),\n",
    "  * with certain conjuction chars ([\"的\", \"、\", \"之\", \"及\", \"與\"]) ,\n",
    "* Clean up candidate list again by removing unnecessary tokens (conjunction characters). \n",
    "  * 晶圓的 -> 晶圓, 的製程 -> 製程, issues caused by adjectives.\n",
    "* Remove stopwords\n",
    "* Remove entity and relation with only one and more than 10 chars\n",
    "* Caculate TDiDF and sort\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Optional\n",
    "* Conclude frequency of occurrence of each node/edge candidate, keeping the first 20 ranks only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "joint-harassment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Windows_Storage\\Storage\\Github\\KnowledgeGraph\\scripts\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-passport",
   "metadata": {},
   "source": [
    "# Read in Tokens (Preparation beforehand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "treated-palmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114798\n",
      "  Segmented Element                  POS  Dependecies\n",
      "0            安森美半導體                  NaN          NaN\n",
      "1                 （  PARENTHESISCATEGORY          NaN\n",
      "2  ON Semiconductor                   FW          NaN\n",
      "3                 ）  PARENTHESISCATEGORY          NaN\n",
      "4                 ，        COMMACATEGORY          NaN\n"
     ]
    }
   ],
   "source": [
    "# read in tokens\n",
    "\n",
    "# dataToken = pd.read_csv(\"../results/210330_result/210330_dataset_spaCyResult.csv\", encoding=\"utf8\")\n",
    "dataToken = pd.read_csv(\"../results/210330_result/210330_dataset_monpaResult.csv\", encoding=\"utf8\")\n",
    "\n",
    "# append a column for data if dependencies column does not exist (monpa case)\n",
    "if len(dataToken.columns) < 3:\n",
    "    dataToken.loc[:, \"Dependecies\"] = np.nan\n",
    "\n",
    "print(len(dataToken))\n",
    "print(dataToken.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-wayne",
   "metadata": {},
   "source": [
    "# Configuration:\n",
    "### Extract possible tokens as candidates of entities and relations by its dependency and POS category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "forbidden-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONPA\n",
    "relation_dependencies_possible_list = []\n",
    "relation_pos_possible_list = [\"VH\", \"VC\", \"VJ\", \"VA\"]\n",
    "relation_pos_re = \"^[V]\"\n",
    "entity_dependencies_possible_list = []\n",
    "entity_pos_possible_list = [\"Na\", \"Nv\", \"Neu\", \"Nes\", \"Nf\", \"Ng\", \"Nh\", \"Neqa\", \"Nep\", \"Ncd\", \"FW\", \"DE\"]\n",
    "entity_pos_re = \"^[N]\"\n",
    "bracket_pos_list = [\"PARENTHESISCATEGORY\"]\n",
    "punct_pos_list = [\"COMMACATEGORY\", \"PERIODCATEGORY\"]\n",
    "\n",
    "# spaCy\n",
    "# relation_dependencies_possible_list = [\"ROOT\", \"nmod:prep\", \"prep\", \"agent\", ]\n",
    "# relation_pos_possible_list = [\"VERB\"]\n",
    "# relation_pos_re = \"^[V]\"\n",
    "# entity_dependencies_possible_list = [\"compound:nn\", \"nsubj\", \"dep\", \"dobj\"]\n",
    "# entity_pos_possible_list = [\"NOUN\", \"PROPN\"]\n",
    "# entity_pos_re = \"^[N]\"\n",
    "# bracket_pos_list = []\n",
    "# punct_pos_list = [\"PUNCT\"]\n",
    "\n",
    "# Common, usually entity\n",
    "sentences_splitter = [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\"]\n",
    "bracket_entity_list = [\"(\", \")\", \"（\", \"）\"]\n",
    "punct_entity_list = [\" \" * i for i in range(0, 100)]\n",
    "conjuction_entity_list = [\"的\", \"、\", \"之\", \"及\", \"與\"]\n",
    "not_entity_relation_list = [\"的\", \"、\", \"之\", \"及\", \"與\", \"\\r\\n \\r\\n \", \"\\r\\n \\r\\n  \"] +\\\n",
    "[\" \" * i for i in range(0, 100)] + [\"\\n\" * i for i in range(0, 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-circle",
   "metadata": {},
   "source": [
    "# Divide by Chinese Seperators\n",
    "# Segment sentences into tokens with relative dependencies and POSs (Part of Speech).\n",
    "# Remove tokens with certain type of dependency and POS (i.e. PUNCT, x, PARENTHESISCATEGORY)\n",
    "# Remove unnecessary tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accepting-johns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list length (Sentences):  7453 7453 7453\n",
      "['安森美半導體'] [nan] [nan]\n",
      "list length:  7453 7453 7453\n"
     ]
    }
   ],
   "source": [
    "# devide token by seperators\n",
    "total_entity_list = []\n",
    "sentence_entity_list = []\n",
    "total_label_list = []\n",
    "sentence_label_list = []\n",
    "total_dependencies_list = []\n",
    "sentence_dependencies_list = []\n",
    "\n",
    "\n",
    "# devide tokens by chinese punctuations\n",
    "for index, entityElement in enumerate(dataToken.iloc[:, 0]):\n",
    "    if entityElement not in sentences_splitter:\n",
    "        sentence_entity_list.append(entityElement)\n",
    "        sentence_label_list.append(dataToken.iloc[index, 1])\n",
    "        sentence_dependencies_list.append(dataToken.iloc[index, 2])\n",
    "    else:\n",
    "        total_entity_list.append(sentence_entity_list)\n",
    "        total_label_list.append(sentence_label_list)\n",
    "        total_dependencies_list.append(sentence_dependencies_list)\n",
    "        sentence_entity_list = []\n",
    "        sentence_label_list = []\n",
    "        sentence_dependencies_list = []\n",
    "    \n",
    "print(\"list length (Sentences): \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))\n",
    "\n",
    "\n",
    "# Clean up tokens and eliminate unneccesary tokens\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    indexPunct_0 = -9999\n",
    "    indexPunct_1 = -9999\n",
    "    for tokenIndex, tokens in enumerate(sentences):\n",
    "        # set up element indexes that are between two punctuation\n",
    "        if total_label_list[sentenceIndex][tokenIndex] in bracket_pos_list or sentences[tokenIndex] in bracket_entity_list:\n",
    "            if indexPunct_0 == -9999:\n",
    "                indexPunct_0 = tokenIndex\n",
    "            else:\n",
    "                indexPunct_1 = tokenIndex\n",
    "        elif total_label_list[sentenceIndex][tokenIndex] in punct_pos_list or sentences[tokenIndex] in punct_entity_list:\n",
    "            # set token that fit certain POS type to \"\"\n",
    "            total_entity_list[sentenceIndex][tokenIndex] = \"\"\n",
    "            \n",
    "    # remove tokens that are between two punctuations, and certain POS type\n",
    "    if indexPunct_1 == -9999 and indexPunct_0 != -9999:\n",
    "        total_entity_list[sentenceIndex][indexPunct_0] = \"\"\n",
    "        total_label_list[sentenceIndex][indexPunct_0] = \"\"\n",
    "        total_dependencies_list[sentenceIndex][indexPunct_0] = \"\"\n",
    "    elif indexPunct_1 != -9999 and indexPunct_0 != -9999:\n",
    "        for removeIndex in range(indexPunct_0, indexPunct_1 + 1):\n",
    "            total_entity_list[sentenceIndex][removeIndex] = \"\"\n",
    "            total_label_list[sentenceIndex][removeIndex] = \"\"\n",
    "            total_dependencies_list[sentenceIndex][removeIndex] = \"\"\n",
    "            \n",
    "    total_entity_list[sentenceIndex] = list(filter((\"\").__ne__, total_entity_list[sentenceIndex]))\n",
    "    total_label_list[sentenceIndex]  = list(filter((\"\").__ne__, total_label_list[sentenceIndex]))\n",
    "    total_dependencies_list[sentenceIndex] = list(filter((\"\").__ne__, total_dependencies_list[sentenceIndex]))\n",
    "                \n",
    "print(total_entity_list[0] , total_label_list[0], total_dependencies_list[0])\n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-laugh",
   "metadata": {},
   "source": [
    "# Extract possible tokens as candidates of entities and relations by its dependency and POS category.\n",
    "#  Clean up candidate list again by removing unnecessary tokens (conjunction characters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "devoted-compiler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7453 7453 7453 7453\n",
      "[] [] [] []\n",
      "['今天', 'RDM系列矽光電倍增管陣列'] [0, 7] ['發表', '新'] [1, 2]\n",
      "['光學雷達感測器能力擴展到其', ''] [6, 8] ['廣泛', '智慧感測方案'] [7, 9]\n",
      "['ArrayRDM-0112A20-QFN', '市場上首款', '車規的SiPM產品'] [0, 5, 10] ['符合'] [6]\n",
      "['汽車產業及其他領域LiDAR應用中', '需求'] [9, 13] ['滿足', '增長'] [1, 11]\n",
      "['ArrayRDM-0112A20-QFN', '單片1×12SiPM像素陣列'] [0, 8] [] []\n",
      "['安森美半導體', '市場的RDM製程'] [1, 5] ['領先'] [2]\n",
      "['外 光的高'] [8] ['實現', '近', '紅', '靈敏度'] [1, 3, 4, 9]\n",
      "['905奈米'] [3] [] []\n",
      "['SiPM', '內部增益', '其靈敏度', '光子水準'] [1, 4, 7, 12] ['高', '使', '達到'] [2, 5, 9]\n",
      "['功能', 'PDE結合使用'] [2, 6] ['高'] [3]\n",
      "['', '信號'] [4, 6] ['檢測', '微弱', '返回'] [1, 3, 5]\n",
      "[] [] [] []\n",
      "['反射目標'] [4] ['低'] [2]\n",
      "['距離'] [6] ['偵測到', '遠'] [2, 4]\n"
     ]
    }
   ],
   "source": [
    "# get entity pairs and relations\n",
    "\n",
    "# get relations\n",
    "all_relations_list = []\n",
    "all_relations_index = []\n",
    "all_element_flatten = []\n",
    "all_relation_flatten = []\n",
    "\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    relation_list = []\n",
    "    relation_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1: \n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in relation_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in relation_pos_possible_list or\\\n",
    "            re.match(relation_pos_re, str(total_label_list[sentenceIndex][tokenIndex]), flags=re.IGNORECASE) != None:\n",
    "                relation_list.append(token)\n",
    "                relation_index_list.append(tokenIndex)\n",
    "    all_relations_list.append(relation_list)\n",
    "    all_relations_index.append(relation_index_list)\n",
    "\n",
    "\n",
    "# get entities\n",
    "all_reformatted_entities = []\n",
    "all_reformatted_index_list = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    entity_index_list = []\n",
    "    possible_entities = []\n",
    "    reformatted_entities = []\n",
    "    reformatted_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1:\n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in entity_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in entity_pos_possible_list or\\\n",
    "            re.match(entity_pos_re, str(total_label_list[sentenceIndex][tokenIndex]), flags=re.IGNORECASE) != None or\\\n",
    "            token in conjuction_entity_list:\n",
    "                entity_index_list.append(tokenIndex)\n",
    "                possible_entities.append(token)\n",
    "    # combine token if situate next to each other\n",
    "#     print(entity_index_list, possible_entities)\n",
    "    if len(possible_entities) > 0:\n",
    "        combine_entity_name = possible_entities[0]\n",
    "        for possibleIndex, possibleElement in enumerate(entity_index_list):\n",
    "#             print(possible_entities)\n",
    "            \n",
    "            isContinuous = False\n",
    "            if possibleIndex != 0:\n",
    "                if possibleElement == entity_index_list[possibleIndex - 1] + 1:\n",
    "                    isContinuous = True\n",
    "                    combine_entity_name += possible_entities[possibleIndex]\n",
    "                else:\n",
    "                    isContinuous = False\n",
    "            \n",
    "                if isContinuous == False:\n",
    "                    reformatted_entities.append(combine_entity_name)\n",
    "                    reformatted_index_list.append(entity_index_list[possibleIndex-1])\n",
    "                    combine_entity_name = possible_entities[possibleIndex]\n",
    "                    \n",
    "            if possibleIndex == (len(entity_index_list) - 1):\n",
    "                reformatted_entities.append(combine_entity_name)\n",
    "                reformatted_index_list.append(possibleElement)\n",
    "                \n",
    "#             print(combine_entity_name)     \n",
    "#     print(reformatted_entities)\n",
    "    all_reformatted_entities.append(reformatted_entities)\n",
    "    all_reformatted_index_list.append(reformatted_index_list)\n",
    "    \n",
    "# flatten section\n",
    "# remove unneccessary conjuction words\n",
    "for index, element in enumerate(all_reformatted_entities):\n",
    "    \n",
    "    for elementIndex, elementSingle in enumerate(element):\n",
    "        # remove single conjuction char in first or last position\n",
    "        if elementSingle[0] in conjuction_entity_list:\n",
    "            element[elementIndex] = elementSingle[1:]\n",
    "        elif elementSingle[-1] in conjuction_entity_list:\n",
    "            element[elementIndex] = elementSingle[:-1]\n",
    "            \n",
    "        all_element_flatten.append(element[elementIndex])\n",
    "        \n",
    "    for relationIndex, relationSingle in enumerate(all_relations_list[index]):\n",
    "        # remove single conjuction char in first or last position\n",
    "        if relationSingle[0] in conjuction_entity_list:\n",
    "            all_relations_list[index][relationIndex] = relationSingle[1:]\n",
    "        elif relationSingle[-1] in conjuction_entity_list:\n",
    "            all_relations_list[index][relationIndex] = relationSingle[:-1]\n",
    "            \n",
    "        all_relation_flatten.append(all_relations_list[index][relationIndex])\n",
    "                      \n",
    "        \n",
    "# print(all_relations_list, len(all_relations_list), \"\\n\")\n",
    "# print(all_reformatted_entities, len(all_reformatted_entities), )\n",
    "print(len(all_relations_list), len(all_relations_index), len(all_reformatted_entities), len(all_reformatted_index_list))\n",
    "\n",
    "# print out parsing result\n",
    "for index, element in enumerate(all_reformatted_entities[:15]):\n",
    "    print(element, all_reformatted_index_list[index], all_relations_list[index], all_relations_index[index])\n",
    "    \n",
    "data_result = pd.DataFrame({\n",
    "    \"Entity\":all_reformatted_entities,\n",
    "    \"EntityIndex\":all_reformatted_index_list,\n",
    "    \"Relation\":all_relations_list,\n",
    "    \"RelationIndex\":all_relations_index\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "still-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conculde dataframe with all tokens\n",
    "        \n",
    "for notIndex, notElement in enumerate(not_entity_relation_list):\n",
    "    all_element_flatten = list(filter((notElement).__ne__, all_element_flatten))\n",
    "    all_relation_flatten = list(filter((notElement).__ne__, all_relation_flatten))\n",
    "        \n",
    "data_entity = pd.DataFrame({\n",
    "    \"Element\":all_element_flatten,\n",
    "})\n",
    "\n",
    "data_relation = pd.DataFrame({\n",
    "    \"Element\":all_relation_flatten,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-mauritius",
   "metadata": {},
   "source": [
    "# Remove stopwords\n",
    "# Remove entity and relation with only one char and too many chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "august-norfolk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "停用詞數目： 3249 \n",
      "\n",
      " ['where', 'whereafter', 'whereas', 'whereby', 'wherein', \"where's\", 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', \"who's\", 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', \"won't\", 'would', \"wouldn't\", 'yes', 'yet', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\", 'zero', 'zt', 'ZT', 'zz', 'ZZ', '準備', '覆雜', '心裏', '註意', '裏面']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-8f51581c4209>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data_temp = pd.read_csv(\"./stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n"
     ]
    }
   ],
   "source": [
    "# import stopwords list\n",
    "\n",
    "stopword_list = []\n",
    "\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"./stopwords/\")):\n",
    "    if fileElement[-2:] != \"md\":\n",
    "        data_temp = pd.read_csv(\"./stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n",
    "        stopword_list += data_temp.iloc[:, 0].tolist()\n",
    "        \n",
    "stopword_list = list(dict.fromkeys(stopword_list))\n",
    "        \n",
    "print(\"停用詞數目：\", len(stopword_list), \"\\n\\n\", stopword_list[-50:])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "manual-female",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Length: 17460\n",
      "After Removal: 12108\n",
      "Original Length: 13495\n",
      "After Removal: 7669\n"
     ]
    }
   ],
   "source": [
    "# remove stop words and entity & relation that only occur once\n",
    "\n",
    "data_list = [data_entity, data_relation]\n",
    "\n",
    "for dataIndex, dataElement in enumerate(data_list):\n",
    "    drop_index = []\n",
    "    for rowIndex, rowElement in enumerate(dataElement.iloc[:, 0]):\n",
    "        if rowElement in stopword_list or len(str(rowElement)) <= 1 or len(str(rowElement)) >= 8:\n",
    "            drop_index.append(rowIndex)\n",
    "    # drop entity/relations that are inside stopword list\n",
    "    print(\"Original Length:\", len(dataElement))\n",
    "    dataElement.drop(index=drop_index, inplace=True)\n",
    "    print(\"After Removal:\", len(dataElement))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-algebra",
   "metadata": {},
   "source": [
    "# Caculate TDiDF and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "united-currency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.678695704841557"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "57 * math.log(15/(15+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spare-ballot",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8582/8582 [00:05<00:00, 1495.70it/s]\n",
      "100%|██████████| 2674/2674 [00:01<00:00, 1564.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12108   Element  Count   Ratio  DocumentFrequency    tfiDF\n",
      "0      晶圓     74  0.0086               14.0   0.0000\n",
      "1      技術     69  0.0080               14.0   0.0000\n",
      "2      製程     57  0.0066               15.0  -1.5976\n",
      "3      晶片     51  0.0059               13.0   1.5281\n",
      "4      產品     43  0.0050               13.0   1.2884\n",
      "5      表面     34  0.0040               12.0   2.1130\n",
      "6      成本     34  0.0040               10.0   4.5798\n",
      "7      設備     32  0.0037               11.0   3.1011\n",
      "8      市場     32  0.0037                8.0   7.0992\n",
      "9      測品     31  0.0036                2.0  21.6681 \n",
      "\n",
      "7669   Element  Count   Ratio  DocumentFrequency    tfiDF\n",
      "0      回收    118  0.0441                4.0  56.3003\n",
      "1      製造    106  0.0396               14.0   0.0000\n",
      "2      利用     97  0.0363                5.0  38.6002\n",
      "3      進行     89  0.0333                5.0  35.4167\n",
      "4      包括     83  0.0310               11.0   8.0435\n",
      "5      不同     64  0.0239                5.0  25.4682\n",
      "6      提供     62  0.0232               11.0   6.0084\n",
      "7      萃取     60  0.0224                1.0  52.5037\n",
      "8      增加     59  0.0221               12.0   3.6667\n",
      "9      生產     58  0.0217               11.0   5.6208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# conculde a list with frequency\n",
    "\n",
    "def ratiolize(x):\n",
    "    x = str(np.around(x * 100, decimals=2)) + \"%\"\n",
    "    return x\n",
    "\n",
    "data_entity_value_count = data_entity.value_counts(ascending=False).to_frame()\n",
    "data_entity_value_count.reset_index(inplace=True)\n",
    "data_entity_value_count = data_entity_value_count.rename(columns = {'index':'Entity'})\n",
    "data_entity_value_count = data_entity_value_count.rename(columns = {0:'Count'})\n",
    "data_entity_value_count.loc[:, \"Ratio\"] = np.around(data_entity_value_count.iloc[:, 1] / len(data_entity_value_count),\n",
    "                                                    decimals=4)\n",
    "# data_entity_value_count.loc[:, \"Ratio\"] = data_entity_value_count.loc[:, \"Ratio\"].apply(ratiolize)\n",
    "\n",
    "data_relation_value_count = data_relation.value_counts(ascending=False).to_frame()\n",
    "data_relation_value_count.reset_index(inplace=True)\n",
    "data_relation_value_count = data_relation_value_count.rename(columns = {'index':'Relation'})\n",
    "data_relation_value_count = data_relation_value_count.rename(columns = {0:'Count'})\n",
    "data_relation_value_count.loc[:, \"Ratio\"] = np.around(data_relation_value_count.iloc[:, 1] / len(data_relation_value_count),\n",
    "                                                      decimals=4)\n",
    "# data_relation_value_count.loc[:, \"Ratio\"] = data_relation_value_count.loc[:, \"Ratio\"].apply(ratiolize)\n",
    "\n",
    "\n",
    "'''\n",
    "calculate tfidf\n",
    "'''\n",
    "documentNum = len(os.listdir(\"../data/\"))\n",
    "data_all = [data_entity_value_count, data_relation_value_count]\n",
    "\n",
    "# gather all document\n",
    "textDocuments = []\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"../data/\")):\n",
    "    textElement = \"\"\n",
    "    file = codecs.open(\"../data/\" + fileElement, 'r', encoding='utf8', errors='ignore')\n",
    "    for textIndex, textLines in enumerate(file):\n",
    "        textElement += textLines\n",
    "    textDocuments.append(textElement)\n",
    "\n",
    "\n",
    "for dataIndex, dataElement in enumerate(data_all):\n",
    "    for rowIndex, rowElement in enumerate(tqdm(dataElement.iloc[:, 0])):\n",
    "        document_count = 0\n",
    "        for textIndex, textElement in enumerate(textDocuments):\n",
    "            # check if entity/relation exists in the doucument\n",
    "            if rowElement in textElement:\n",
    "                document_count += 1\n",
    "        # assign document frequency\n",
    "        dataElement.loc[rowIndex, \"DocumentFrequency\"] = document_count\n",
    "        # assign tfidf\n",
    "        dataElement.loc[rowIndex, \"tfiDF\"] = np.around(dataElement.loc[rowIndex, \"Count\"] * \\\n",
    "            math.log(documentNum / (dataElement.loc[rowIndex, \"DocumentFrequency\"] + 1), 10), decimals=4)\n",
    "        \n",
    "# data_entity_value_count.sort_values(by=[\"tfiDF\"], ascending=False, inplace=True)\n",
    "# data_relation_value_count.sort_values(by=[\"tfiDF\"], ascending=False, inplace=True)\n",
    "\n",
    "print(len(data_entity), data_entity_value_count[:10], \"\\n\")\n",
    "print(len(data_relation), data_relation_value_count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "changing-baltimore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving result without filtering\n",
    "\n",
    "# data_entity_value_count.to_csv(\"../results/210330_result/210330_dataset_entity_result_spaCy.csv\", encoding=\"utf8\", index=None)\n",
    "# data_relation_value_count.to_csv(\"../results/210330_result/210330_dataset_relation_result_spaCy.csv\", encoding=\"utf8\", index=None)\n",
    "# data_result.to_csv(\"../results/210330_result/210330_dataset_main_result_spaCy.csv\", encoding=\"utf8\", index=None)\n",
    "\n",
    "data_entity_value_count.to_csv(\"../results/210408_result/210408_dataset_entity_result_MONPA.csv\", encoding=\"utf8\", index=None)\n",
    "data_relation_value_count.to_csv(\"../results/210408_result/210408_dataset_relation_result_MONPA.csv\", encoding=\"utf8\", index=None)\n",
    "data_result.to_csv(\"../results/210408_result/210408_dataset_main_result_MONPA.csv\", encoding=\"utf8\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-authority",
   "metadata": {},
   "source": [
    "# Filter out most frequent edge and relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "maritime-hostel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Entity             EntityIndex  \\\n",
      "20    ['SiPM', '陽光條件下', '距離', '時', '訊噪', '性能']  [0, 7, 10, 13, 18, 20]   \n",
      "21                 ['其他優勢', '電源偏置', '溫度變化敏感性']              [1, 7, 14]   \n",
      "29                                ['其他感知模式互補']                     [5]   \n",
      "39                         ['距離、高性價比的LiDAR方案']                     [9]   \n",
      "42                             ['多樣化', '感知模式']                  [1, 6]   \n",
      "...                                        ...                     ...   \n",
      "7437                         [' 壞去', '達', '%']               [4, 7, 9]   \n",
      "7440                ['冷凝處理', '廢氣成分中', '溫度之不同']              [1, 6, 10]   \n",
      "7441        ['冷凝作用', '兩種方式在定溫', '增加系統之壓力2在定壓']              [1, 8, 16]   \n",
      "7443                ['廢氣', '冷凝器', '冷凝劑', '達成']          [2, 7, 11, 13]   \n",
      "7444                   ['冷凝器型式', '表面式及接觸式冷凝器']                 [4, 11]   \n",
      "\n",
      "                                Relation          RelationIndex  \n",
      "20     ['明亮', '進行', '長', '測', '提供', '佳']  [3, 8, 9, 11, 14, 16]  \n",
      "21                      ['包括', '低', '低']             [2, 4, 10]  \n",
      "29                    ['通過', '提供', '冗餘']              [0, 7, 8]  \n",
      "39                           ['提供', '遠']                 [1, 2]  \n",
      "42                          ['提供', '互補']                 [0, 3]  \n",
      "...                                  ...                    ...  \n",
      "7437              ['增加', '破', '率', '98']           [1, 2, 6, 8]  \n",
      "7440                   ['利用', '凝結', '而']             [3, 7, 11]  \n",
      "7441                          ['包括', '']                [3, 12]  \n",
      "7443  ['做為', '控制', '排', '放用', '使用', '來']   [0, 1, 3, 4, 10, 12]  \n",
      "7444                   ['常用', '包括', '有']              [1, 5, 6]  \n",
      "\n",
      "[1025 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# data_entity_value_count = pd.read_csv(\"../results/210330_result/210330_dataset_entity_result_spaCy.csv\", encoding=\"utf8\")\n",
    "# data_relation_value_count = pd.read_csv(\"../results/210330_result/210330_dataset_relation_result_spaCy.csv\", encoding=\"utf8\")\n",
    "# data_result = pd.read_csv(\"../results/210330_result/210330_dataset_main_result_spaCy.csv\", encoding=\"utf8\")\n",
    "\n",
    "data_entity_value_count = pd.read_csv(\"../results/210408_result/210408_dataset_entity_result_MONPA.csv\", encoding=\"utf8\")\n",
    "data_relation_value_count = pd.read_csv(\"../results/210408_result/210408_dataset_relation_result_MONPA.csv\", encoding=\"utf8\")\n",
    "data_result = pd.read_csv(\"../results/210408_result/210408_dataset_main_result_MONPA.csv\", encoding=\"utf8\")\n",
    "\n",
    "# take only first 20 places\n",
    "data_entity_filter = data_entity_value_count.loc[:, \"Element\"].tolist()[:20]\n",
    "data_relation_filter = data_relation_value_count.loc[:, \"Element\"].tolist()[:20]\n",
    "\n",
    "abandonIndexList = []\n",
    "\n",
    "for rowIndex, rowItem in enumerate(data_result.iloc[:, 0]):\n",
    "    need_to_abandon = True\n",
    "    entityList = rowItem.replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")[:-1]\n",
    "    relationList = data_result.iloc[rowIndex, 2].replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")[:-1]\n",
    "    \n",
    "    for entityIndex, entityElement in enumerate(entityList):\n",
    "        if entityElement in data_entity_filter:\n",
    "            need_to_abandon = False\n",
    "            break\n",
    "    \n",
    "    for relationIndex, relationElement in enumerate(relationList):\n",
    "        if need_to_abandon == False:\n",
    "            break\n",
    "        if relationElement in data_relation_filter:\n",
    "            need_to_abandon = False\n",
    "            break\n",
    "            \n",
    "    if need_to_abandon == True:\n",
    "        abandonIndexList.append(rowIndex)\n",
    "            \n",
    "data_result.drop(index=abandonIndexList, inplace=True)\n",
    "print(data_result)\n",
    "\n",
    "# export data\n",
    "data_result.to_csv(\"../results/210408_result/210408_dataset_filtered_result_spaCy.csv\", encoding=\"utf8\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monpa",
   "language": "python",
   "name": "monpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
