{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "functioning-advocate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Desktop\\Ricardo\\KnowledgeGraph\\scripts\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-appointment",
   "metadata": {},
   "source": [
    "# Some Reference\n",
    "\n",
    "* Regular Expression Tester: https://regex101.com/\n",
    "* Some Re Tutorials: https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-emerald",
   "metadata": {},
   "source": [
    "# Processing Steps\n",
    "* Divide paragraphs into sentences by Chinese punctuations (“，”, “。”, “、”, “？”, “?”).\n",
    "* Segment sentences into tokens with relative dependencies and POSs (Part of Speech).\n",
    "* Remove tokens with certain type of dependency and POS (i.e. PUNCT, x, PARENTHESISCATEGORY)\n",
    "* Remove unnecessary tokens\n",
    "  * punctuation\n",
    "  * word between two bracket\n",
    "* Extract possible tokens as candidates of entities and relations by its dependency and POS category,\n",
    "  * with regular expression (Entity, POS, dependencies): “^[V]”, “^[N]”,\n",
    "  * with certain dependencies and POS type,\n",
    "  * with given entity dictionaries (entity, POS, dependencies),\n",
    "  * with certain conjuction chars ([\"的\", \"、\", \"之\", \"及\", \"與\"]) ,\n",
    "* Clean up candidate list again by removing unnecessary tokens (conjunction characters). \n",
    "  * 晶圓的 -> 晶圓, 的製程 -> 製程, issues caused by adjectives.\n",
    "* Remove stopwords\n",
    "* Remove entity and relation with only one and more than 8 chars\n",
    "* Caculate TFiDF/Frequency and sort\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Optional\n",
    "* Conclude frequency of occurrence of each node/edge candidate, keeping the first 20 ranks only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-malaysia",
   "metadata": {},
   "source": [
    "# Configurations\n",
    "### Extract possible tokens as candidates of entities and relations by its dependency and POS category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suitable-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "token_path = \"../results/210408_result/210408_dataset_monpaResult.csv\"\n",
    "entity_saving_path = \"../results/210408_result/210408_dataset_entity_result_MONPA.csv\"\n",
    "relation_saving_path = \"../results/210408_result/210408_dataset_relation_result_MONPA.csv\"\n",
    "main_result_saving_path = \"../results/210408_result/210408_dataset_main_result_MONPA.csv\"\n",
    "filtered_result_path = \"../results/210408_result/210408_dataset_filtered_result_MONPA.csv\"\n",
    "\n",
    "# token_path = \"../results/210330_result/210330_dataset_spaCyResult.csv\"\n",
    "# entity_saving_path = \"../results/210330_result/210330_dataset_entity_result_spaCy.csv\"\n",
    "# relation_saving_path = \"../results/210330_result/210330_dataset_relation_result_spaCy.csv\"\n",
    "# main_result_saving_path = \"../results/210330_result/210330_dataset_main_result_spaCy.csv\"\n",
    "# filtered_result_path = \"../results/210408_result/210408_dataset_filtered_result_spaCy.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "corrected-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONPA\n",
    "relation_dependencies_possible_list = []\n",
    "relation_pos_possible_list = [\"VH\", \"VC\", \"VJ\", \"VA\"]\n",
    "relation_pos_re = \"^[V]\"\n",
    "entity_dependencies_possible_list = []\n",
    "entity_pos_possible_list = [\"Na\", \"Nv\", \"Neu\", \"Nes\", \"Nf\", \"Ng\", \"Nh\", \"Neqa\", \"Nep\", \"Ncd\", \"FW\", \"DE\"]\n",
    "entity_pos_re = \"^[N]\"\n",
    "splitter_pos_list = [\"COMMACATEGORY\", \"PERIODCATEGORY\"]\n",
    "\n",
    "# spaCy\n",
    "# relation_dependencies_possible_list = [\"ROOT\", \"nmod:prep\", \"prep\", \"agent\", ]\n",
    "# relation_pos_possible_list = [\"VERB\"]\n",
    "# relation_pos_re = \"^[V]\"\n",
    "# entity_dependencies_possible_list = [\"compound:nn\", \"nsubj\", \"dep\", \"dobj\"]\n",
    "# entity_pos_possible_list = [\"NOUN\", \"PROPN\"]\n",
    "# entity_pos_re = \"^[N]\"\n",
    "# punct_pos_list = [\"PUNCT\"]\n",
    "\n",
    "# Common, usually entity\n",
    "sentences_splitter = [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\", \"；\", \";\", \".\"] + [\"\\r\\n\" * i for i in range(0, 100)]\n",
    "bracket_entity_list_first = [\"(\", \"（\", \"[\", \"［\", \"{\", \"｛\", \"<\", \"＜\", \"〔\", \"【\", \"〖\", \"《\", \"〈\"]\n",
    "bracket_entity_list_last = [\")\", \"）\", \"]\", \"］\", \"}\", \"｝\", \">\", \"＞\", \"〕\", \"】\", \"〗\", \"》\", \"〉\"]\n",
    "punct_entity_list = [\" \" * i for i in range(0, 100)]\n",
    "conjuction_entity_list = [\"的\", \"、\", \"之\", \"及\", \"與\"]\n",
    "not_entity_relation_list = [\"的\", \"、\", \"之\", \"及\", \"與\", \"\\r\\n \\r\\n \", \"\\r\\n \\r\\n  \"] +\\\n",
    "[\" \" * i for i in range(0, 100)] + [\"\\n\" * i for i in range(0, 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-brand",
   "metadata": {},
   "source": [
    "# Read in Tokens (Preparation beforehand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "forty-colombia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143131\n",
      "  Segmented Element  POS  Dependecies\n",
      "0                12  Neu          NaN\n",
      "1                 吋   Nf          NaN\n",
      "2                晶圓   Na          NaN\n",
      "3                後段  Ncd          NaN\n",
      "4                製程   Na          NaN\n"
     ]
    }
   ],
   "source": [
    "# read in tokens\n",
    "\n",
    "dataToken = pd.read_csv(token_path, encoding=\"utf8\")\n",
    "\n",
    "# append a column for data if dependencies column does not exist (monpa case)\n",
    "if len(dataToken.columns) < 3:\n",
    "    dataToken.loc[:, \"Dependecies\"] = np.nan\n",
    "\n",
    "print(len(dataToken))\n",
    "print(dataToken.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-determination",
   "metadata": {},
   "source": [
    "# Divide by Chinese Seperators\n",
    "# Segment sentences into tokens with relative dependencies and POSs (Part of Speech).\n",
    "# Remove tokens with certain type of dependency and POS (i.e. PUNCT, x, PARENTHESISCATEGORY)\n",
    "# Remove unnecessary tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "straight-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list length (Sentences):  9887 9887 9887\n",
      "['12', '吋', '晶圓', '後段', '製程', '之', '發展', '趨勢', '探討', '前瞻', '封裝', '系列', '2002年', '08月', '05日', '星期一', '瀏覽', '人次', '：', '台灣', '半導體', '產業', '結構', '完整'] ['Neu', 'Nf', 'Na', 'Ncd', 'Na', 'DE', 'Nv', 'Na', 'VE', 'FW', 'FW', 'FW', 'Nv', 'Na', 'Na', 'FW', 'FW', 'FW', 'FW', 'FW', 'Nd', 'Nd', 'Nd', 'FW', 'Nd', 'Na', 'FW', 'VC', 'Na', 'FW', 'FW', 'FW', 'FW', 'FW', 'FW', 'FW', 'FW', 'LOC', 'Na', 'Na', 'Na', 'VH'] [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
      "list length:  9887 9887 9887\n"
     ]
    }
   ],
   "source": [
    "# devide token by seperators\n",
    "total_entity_list = []\n",
    "sentence_entity_list = []\n",
    "total_label_list = []\n",
    "sentence_label_list = []\n",
    "total_dependencies_list = []\n",
    "sentence_dependencies_list = []\n",
    "\n",
    "\n",
    "# devide tokens by chinese punctuations\n",
    "for index, entityElement in enumerate(dataToken.iloc[:, 0]):\n",
    "    if entityElement not in sentences_splitter:\n",
    "        sentence_entity_list.append(entityElement)\n",
    "        sentence_label_list.append(dataToken.iloc[index, 1])\n",
    "        sentence_dependencies_list.append(dataToken.iloc[index, 2])\n",
    "    else:\n",
    "        total_entity_list.append(sentence_entity_list)\n",
    "        total_label_list.append(sentence_label_list)\n",
    "        total_dependencies_list.append(sentence_dependencies_list)\n",
    "        sentence_entity_list = []\n",
    "        sentence_label_list = []\n",
    "        sentence_dependencies_list = []\n",
    "    \n",
    "print(\"list length (Sentences): \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))\n",
    "\n",
    "\n",
    "# Clean up tokens and eliminate unneccesary tokens\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    for tokenIndex, tokens in enumerate(sentences):\n",
    "        # delete elements that's between two bracket\n",
    "        if sentences[tokenIndex] in bracket_entity_list_first:\n",
    "            left_bracket_index = bracket_entity_list_first.index(sentences[tokenIndex])\n",
    "            findingLimit =  (tokenIndex+11) if (tokenIndex+11) <= (len(sentences)) else len(sentences)\n",
    "            for findingLeftIndex in range(tokenIndex+1, findingLimit):\n",
    "                if sentences[findingLeftIndex] == bracket_entity_list_last[left_bracket_index]:\n",
    "                    for removalIndex in range(tokenIndex, findingLeftIndex+1):\n",
    "#                         print(total_entity_list[sentenceIndex][removalIndex])\n",
    "                        total_entity_list[sentenceIndex][removalIndex] = \"\"\n",
    "                        total_label_list[sentenceIndex][removalIndex]= \"\"\n",
    "                        total_dependencies_list[sentenceIndex][removalIndex] = \"\"\n",
    "                    break\n",
    "        elif sentences[tokenIndex] in punct_entity_list or sentences[tokenIndex] in sentences_splitter:\n",
    "            # set token that fit certain POS type to \"\"\n",
    "            total_entity_list[sentenceIndex][tokenIndex] = \"\"\n",
    "            \n",
    "    total_entity_list[sentenceIndex] = list(filter((\"\").__ne__, total_entity_list[sentenceIndex]))\n",
    "    total_label_list[sentenceIndex]  = list(filter((\"\").__ne__, total_label_list[sentenceIndex]))\n",
    "    total_dependencies_list[sentenceIndex] = list(filter((\"\").__ne__, total_dependencies_list[sentenceIndex]))\n",
    "                \n",
    "print(total_entity_list[0] , total_label_list[0], total_dependencies_list[0])\n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-wrist",
   "metadata": {},
   "source": [
    "# Extract possible tokens as candidates of entities and relations by its dependency and POS category.\n",
    "#  Clean up candidate list again by removing unnecessary tokens (conjunction characters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "built-arkansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9887 9887 9887 9887\n",
      "['12吋晶圓後段製程之發展趨勢', '前瞻封裝系列2002年08月05日星期一瀏覽人次：台灣半導體產業結構完整'] [7, 23] ['探討'] [8]\n",
      "['其', '晶片製造合作模式'] [1, 8] ['緊密', '延伸'] [2, 3]\n",
      "['數年間', '半導體製程供應鏈'] [3, 11] ['過去', '建立', '完整'] [1, 5, 7]\n",
      "['附加價值', '製程發展'] [6, 9] ['持續', '朝向', '高'] [1, 2, 3]\n",
      "['成本效益與縮短產品', '時程的考量'] [6, 10] ['上市'] [7]\n",
      "['12吋晶圓廠', '晶圓代工下一波的競爭主力'] [2, 12] ['成為'] [4]\n",
      "['國際整合元件大廠', '產能', '趨勢下'] [4, 7, 12] ['委外', '代工'] [8, 9]\n",
      "['晶圓', '', '表現'] [3, 5, 7] ['造就', '代工', '亮麗'] [0, 4, 6]\n",
      "['半導體產業的競爭優勢'] [7] ['建立'] [1]\n",
      "['國內晶圓', '兩', '龍頭', '積體電路', '科學園區', '科學園區', '12吋晶圓代工製造廠'] [1, 3, 5, 9, 15, 18, 27] ['代工', '大', '開始', '投資', '興建'] [2, 4, 20, 21, 22]\n",
      "['小部份產能'] [3] ['進行', '試產'] [1, 4]\n",
      "['國際級的整合元件大廠', '12吋晶圓的發展趨勢'] [5, 14] ['前進'] [15]\n",
      "['後段的封裝製造廠', '整合性', '服務'] [4, 7, 10] ['使得', '提供', '化'] [0, 6, 9]\n",
      "['製程技術'] [4] ['強化'] [2]\n",
      "['晶圓尺寸增加後'] [5] ['滿足'] [1]\n"
     ]
    }
   ],
   "source": [
    "# get entity pairs and relations\n",
    "\n",
    "# get relations\n",
    "all_relations_list = []\n",
    "all_relations_index = []\n",
    "all_element_flatten = []\n",
    "all_relation_flatten = []\n",
    "\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    relation_list = []\n",
    "    relation_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1: \n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in relation_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in relation_pos_possible_list or\\\n",
    "            re.match(relation_pos_re, str(total_label_list[sentenceIndex][tokenIndex]), flags=re.IGNORECASE) != None:\n",
    "                relation_list.append(token)\n",
    "                relation_index_list.append(tokenIndex)\n",
    "    all_relations_list.append(relation_list)\n",
    "    all_relations_index.append(relation_index_list)\n",
    "\n",
    "\n",
    "# get entities\n",
    "all_reformatted_entities = []\n",
    "all_reformatted_index_list = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    entity_index_list = []\n",
    "    possible_entities = []\n",
    "    reformatted_entities = []\n",
    "    reformatted_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1:\n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in entity_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in entity_pos_possible_list or\\\n",
    "            re.match(entity_pos_re, str(total_label_list[sentenceIndex][tokenIndex]), flags=re.IGNORECASE) != None or\\\n",
    "            token in conjuction_entity_list:\n",
    "                entity_index_list.append(tokenIndex)\n",
    "                possible_entities.append(token)\n",
    "    # combine token if situate next to each other\n",
    "#     print(entity_index_list, possible_entities)\n",
    "    if len(possible_entities) > 0:\n",
    "        combine_entity_name = possible_entities[0]\n",
    "        for possibleIndex, possibleElement in enumerate(entity_index_list):\n",
    "#             print(possible_entities)\n",
    "            \n",
    "            isContinuous = False\n",
    "            if possibleIndex != 0:\n",
    "                if possibleElement == entity_index_list[possibleIndex - 1] + 1:\n",
    "                    isContinuous = True\n",
    "                    combine_entity_name += str(possible_entities[possibleIndex])\n",
    "                else:\n",
    "                    isContinuous = False\n",
    "            \n",
    "                if isContinuous == False:\n",
    "                    reformatted_entities.append(combine_entity_name)\n",
    "                    reformatted_index_list.append(entity_index_list[possibleIndex-1])\n",
    "                    combine_entity_name = possible_entities[possibleIndex]\n",
    "                    \n",
    "            if possibleIndex == (len(entity_index_list) - 1):\n",
    "                reformatted_entities.append(combine_entity_name)\n",
    "                reformatted_index_list.append(possibleElement)\n",
    "                \n",
    "#             print(combine_entity_name)     \n",
    "#     print(reformatted_entities)\n",
    "    all_reformatted_entities.append(reformatted_entities)\n",
    "    all_reformatted_index_list.append(reformatted_index_list)\n",
    "    \n",
    "# flatten section\n",
    "# remove unneccessary conjuction words\n",
    "for index, element in enumerate(all_reformatted_entities):\n",
    "    \n",
    "    for elementIndex, elementSingle in enumerate(element):\n",
    "        # remove single conjuction char in first or last position\n",
    "        if elementSingle[0] in conjuction_entity_list:\n",
    "            element[elementIndex] = elementSingle[1:]\n",
    "        elif elementSingle[-1] in conjuction_entity_list:\n",
    "            element[elementIndex] = elementSingle[:-1]\n",
    "            \n",
    "        all_element_flatten.append(element[elementIndex])\n",
    "        \n",
    "    for relationIndex, relationSingle in enumerate(all_relations_list[index]):\n",
    "        # remove single conjuction char in first or last position\n",
    "        if relationSingle[0] in conjuction_entity_list:\n",
    "            all_relations_list[index][relationIndex] = relationSingle[1:]\n",
    "        elif relationSingle[-1] in conjuction_entity_list:\n",
    "            all_relations_list[index][relationIndex] = relationSingle[:-1]\n",
    "            \n",
    "        all_relation_flatten.append(all_relations_list[index][relationIndex])\n",
    "                      \n",
    "        \n",
    "# print(all_relations_list, len(all_relations_list), \"\\n\")\n",
    "# print(all_reformatted_entities, len(all_reformatted_entities), )\n",
    "print(len(all_relations_list), len(all_relations_index), len(all_reformatted_entities), len(all_reformatted_index_list))\n",
    "\n",
    "# print out parsing result\n",
    "for index, element in enumerate(all_reformatted_entities[:15]):\n",
    "    print(element, all_reformatted_index_list[index], all_relations_list[index], all_relations_index[index])\n",
    "    \n",
    "data_result = pd.DataFrame({\n",
    "    \"Entity\":all_reformatted_entities,\n",
    "    \"EntityIndex\":all_reformatted_index_list,\n",
    "    \"Relation\":all_relations_list,\n",
    "    \"RelationIndex\":all_relations_index\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "actual-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conculde dataframe with all tokens\n",
    "        \n",
    "for notIndex, notElement in enumerate(not_entity_relation_list):\n",
    "    all_element_flatten = list(filter((notElement).__ne__, all_element_flatten))\n",
    "    all_relation_flatten = list(filter((notElement).__ne__, all_relation_flatten))\n",
    "        \n",
    "data_entity = pd.DataFrame({\n",
    "    \"Element\":all_element_flatten,\n",
    "})\n",
    "\n",
    "data_relation = pd.DataFrame({\n",
    "    \"Element\":all_relation_flatten,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-savannah",
   "metadata": {},
   "source": [
    "# Remove stopwords\n",
    "# Remove entity and relation with only one char and too many chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "partial-detector",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\monpa\\lib\\site-packages\\ipykernel_launcher.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "停用詞數目： 2003299 \n",
      "\n",
      " [\"t's\", 'twice', 'unfortunately', 'unless', 'unlikely', 'unto', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'value', 'various', 'viz', 'vs', 'want', 'wants', \"wasn't\", 'way', \"we'd\", 'welcome', \"we'll\", 'went', \"we're\", \"weren't\", \"we've\", \"what's\", \"where's\", \"who's\", 'willing', 'wish', 'wonder', \"won't\", \"wouldn't\", 'yes', \"you'd\", \"you'll\", \"you're\", \"you've\", 'zero', 'zt', 'ZT', 'zz', 'ZZ', '準備', '覆雜', '心裏', '註意', '裏面']\n"
     ]
    }
   ],
   "source": [
    "# import stopwords list\n",
    "\n",
    "stopword_list = []\n",
    "\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"./stopwords/\")):\n",
    "    if fileElement[-2:] != \"md\":\n",
    "        data_temp = pd.read_csv(\"./stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n",
    "        stopword_list += data_temp.iloc[:, 0].tolist()\n",
    "        \n",
    "stopword_list = list(dict.fromkeys(stopword_list))\n",
    "        \n",
    "print(\"停用詞數目：\", len(stopword_list), \"\\n\\n\", stopword_list[-50:])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "opening-surrey",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23302/23302 [07:54<00:00, 49.08it/s]\n",
      "  0%|          | 6/17909 [00:00<05:57, 50.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Length: 23302\n",
      "After Removal: 16093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17909/17909 [06:04<00:00, 49.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Length: 17909\n",
      "After Removal: 10229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# remove stop words and entity & relation that only occur once\n",
    "\n",
    "data_list = [data_entity, data_relation]\n",
    "\n",
    "for dataIndex, dataElement in enumerate(data_list):\n",
    "    drop_index = []\n",
    "    for rowIndex, rowElement in enumerate(tqdm(dataElement.iloc[:, 0])):\n",
    "        if rowElement in stopword_list or len(str(rowElement)) <= 1 or len(str(rowElement)) >= 8:\n",
    "            drop_index.append(rowIndex)\n",
    "    # drop entity/relations that are inside stopword list\n",
    "    print(\"Original Length:\", len(dataElement))\n",
    "    dataElement.drop(index=drop_index, inplace=True)\n",
    "    print(\"After Removal:\", len(dataElement))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-uganda",
   "metadata": {},
   "source": [
    "# Caculate TFiDF/Frequency and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cathedral-teach",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10608/10608 [00:05<00:00, 1966.39it/s]\n",
      "100%|██████████| 3220/3220 [00:01<00:00, 1982.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16093   Element  Count   Ratio  DocumentFrequency    tfiDF\n",
      "0      晶圓    135  0.0127               26.0   0.0000\n",
      "1      技術     90  0.0085               26.0   0.0000\n",
      "2      製程     83  0.0078               27.0  -1.3109\n",
      "3      晶片     68  0.0064               22.0   4.7352\n",
      "4      產品     66  0.0062               22.0   4.5960\n",
      "5      成本     48  0.0045               17.0   8.4524\n",
      "6      表面     45  0.0042               20.0   4.9115\n",
      "7      測品     41  0.0039                3.0  34.0015\n",
      "8      過程     40  0.0038               20.0   4.3658\n",
      "9      設備     40  0.0038               22.0   2.7854 \n",
      "\n",
      "10229   Element  Count   Ratio  DocumentFrequency    tfiDF\n",
      "0      回收    132  0.0410                5.0  86.2241\n",
      "1      製造    127  0.0394               25.0   2.0816\n",
      "2      利用    113  0.0351                6.0  66.2480\n",
      "3      進行    107  0.0332                7.0  56.5253\n",
      "4      包括    102  0.0317               17.0  17.9613\n",
      "5      成長     92  0.0286               16.0  18.4842\n",
      "6      提供     88  0.0273               18.0  13.4297\n",
      "7      增加     85  0.0264               21.0   7.5600\n",
      "8      不同     84  0.0261                6.0  49.2463\n",
      "9      生產     72  0.0224               21.0   6.4038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# conculde a list with frequency\n",
    "\n",
    "def ratiolize(x):\n",
    "    x = str(np.around(x * 100, decimals=2)) + \"%\"\n",
    "    return x\n",
    "\n",
    "data_entity_value_count = data_entity.value_counts(ascending=False).to_frame()\n",
    "data_entity_value_count.reset_index(inplace=True)\n",
    "data_entity_value_count = data_entity_value_count.rename(columns = {'index':'Entity'})\n",
    "data_entity_value_count = data_entity_value_count.rename(columns = {0:'Count'})\n",
    "data_entity_value_count.loc[:, \"Ratio\"] = np.around(data_entity_value_count.iloc[:, 1] / len(data_entity_value_count),\n",
    "                                                    decimals=4)\n",
    "# data_entity_value_count.loc[:, \"Ratio\"] = data_entity_value_count.loc[:, \"Ratio\"].apply(ratiolize)\n",
    "\n",
    "data_relation_value_count = data_relation.value_counts(ascending=False).to_frame()\n",
    "data_relation_value_count.reset_index(inplace=True)\n",
    "data_relation_value_count = data_relation_value_count.rename(columns = {'index':'Relation'})\n",
    "data_relation_value_count = data_relation_value_count.rename(columns = {0:'Count'})\n",
    "data_relation_value_count.loc[:, \"Ratio\"] = np.around(data_relation_value_count.iloc[:, 1] / len(data_relation_value_count),\n",
    "                                                      decimals=4)\n",
    "# data_relation_value_count.loc[:, \"Ratio\"] = data_relation_value_count.loc[:, \"Ratio\"].apply(ratiolize)\n",
    "\n",
    "\n",
    "'''\n",
    "calculate tfidf\n",
    "'''\n",
    "documentNum = len(os.listdir(\"../data/\"))\n",
    "data_all = [data_entity_value_count, data_relation_value_count]\n",
    "\n",
    "# gather all document\n",
    "textDocuments = []\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"../data/\")):\n",
    "    textElement = \"\"\n",
    "    file = codecs.open(\"../data/\" + fileElement, 'r', encoding='utf8', errors='ignore')\n",
    "    for textIndex, textLines in enumerate(file):\n",
    "        textElement += textLines\n",
    "    textDocuments.append(textElement)\n",
    "\n",
    "\n",
    "for dataIndex, dataElement in enumerate(data_all):\n",
    "    for rowIndex, rowElement in enumerate(tqdm(dataElement.iloc[:, 0])):\n",
    "        document_count = 0\n",
    "        for textIndex, textElement in enumerate(textDocuments):\n",
    "            # check if entity/relation exists in the doucument\n",
    "            if rowElement in textElement:\n",
    "                document_count += 1\n",
    "        # assign document frequency\n",
    "        dataElement.loc[rowIndex, \"DocumentFrequency\"] = document_count\n",
    "        # assign tfidf\n",
    "        dataElement.loc[rowIndex, \"tfiDF\"] = np.around(dataElement.loc[rowIndex, \"Count\"] * \\\n",
    "            math.log(documentNum / (dataElement.loc[rowIndex, \"DocumentFrequency\"] + 1), 10), decimals=4)\n",
    "        \n",
    "# data_entity_value_count.sort_values(by=[\"tfiDF\"], ascending=False, inplace=True)\n",
    "# data_relation_value_count.sort_values(by=[\"tfiDF\"], ascending=False, inplace=True)\n",
    "\n",
    "print(len(data_entity), data_entity_value_count[:10], \"\\n\")\n",
    "print(len(data_relation), data_relation_value_count[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "demographic-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving result without filtering\n",
    "data_entity_value_count.to_csv(entity_saving_path, encoding=\"utf8\", index=None, quoting=False)\n",
    "data_relation_value_count.to_csv(relation_saving_path, encoding=\"utf8\", index=None, quoting=False)\n",
    "data_result.to_csv(main_result_saving_path, encoding=\"utf8\", index=None, quoting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-dairy",
   "metadata": {},
   "source": [
    "# Filter out most frequent edge and relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "equivalent-cholesterol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Entity         EntityIndex  \\\n",
      "7                             ['晶圓', '', '表現']           [3, 5, 7]   \n",
      "12                   ['後段的封裝製造廠', '整合性', '服務']          [4, 7, 10]   \n",
      "40                              ['產能', '產品良率']              [3, 8]   \n",
      "41                ['晶圓', '應用趨勢就晶圓尺寸的', '趨勢而言']          [0, 8, 11]   \n",
      "51    ['12吋晶圓的後段製程', '吋晶圓的競爭優勢', '應用朝向', '趨勢']     [5, 14, 18, 22]   \n",
      "...                                        ...                 ...   \n",
      "9871                         [' 壞去', '達', '%']           [4, 7, 9]   \n",
      "9873  ['凝器置於', '控制設備(', '設備', '碳吸 附床及吸收', '後']  [3, 9, 14, 20, 23]   \n",
      "9874                ['冷凝處理', '廢氣成分中', '溫度之不同']          [1, 6, 10]   \n",
      "9875        ['冷凝作用', '兩種方式在定溫', '增加系統之壓力2在定壓']          [1, 8, 16]   \n",
      "9878                   ['冷凝器型式', '表面式及接觸式冷凝器']             [4, 11]   \n",
      "\n",
      "                        Relation      RelationIndex  \n",
      "7             ['造就', '代工', '亮麗']          [0, 4, 6]  \n",
      "12             ['使得', '提供', '化']          [0, 6, 9]  \n",
      "40                  ['增加', '提高']             [2, 6]  \n",
      "41                            []                 []  \n",
      "51                  ['發展', '高階']            [6, 19]  \n",
      "...                          ...                ...  \n",
      "9871      ['增加', '破', '率', '98']       [1, 2, 6, 8]  \n",
      "9873  [' 冷', '較', '', '處理', ')']  [1, 4, 6, 12, 22]  \n",
      "9874           ['利用', '凝結', '而']         [3, 7, 11]  \n",
      "9875                  ['包括', '']            [3, 12]  \n",
      "9878           ['常用', '包括', '有']          [1, 5, 6]  \n",
      "\n",
      "[1334 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data_entity_value_count = pd.read_csv(entity_saving_path, encoding=\"utf8\")\n",
    "data_relation_value_count = pd.read_csv(relation_saving_path, encoding=\"utf8\")\n",
    "data_result = pd.read_csv(main_result_saving_path, encoding=\"utf8\")\n",
    "\n",
    "# take only first 20 places\n",
    "data_entity_filter = data_entity_value_count.loc[:, \"Element\"].tolist()[:20]\n",
    "data_relation_filter = data_relation_value_count.loc[:, \"Element\"].tolist()[:20]\n",
    "\n",
    "abandonIndexList = []\n",
    "\n",
    "for rowIndex, rowItem in enumerate(data_result.iloc[:, 0]):\n",
    "    need_to_abandon = True\n",
    "    entityList = rowItem.replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")[:-1]\n",
    "    relationList = data_result.iloc[rowIndex, 2].replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")[:-1]\n",
    "    \n",
    "    for entityIndex, entityElement in enumerate(entityList):\n",
    "        if entityElement in data_entity_filter:\n",
    "            need_to_abandon = False\n",
    "            break\n",
    "    \n",
    "    for relationIndex, relationElement in enumerate(relationList):\n",
    "        if need_to_abandon == False:\n",
    "            break\n",
    "        if relationElement in data_relation_filter:\n",
    "            need_to_abandon = False\n",
    "            break\n",
    "            \n",
    "    if need_to_abandon == True:\n",
    "        abandonIndexList.append(rowIndex)\n",
    "            \n",
    "data_result.drop(index=abandonIndexList, inplace=True)\n",
    "print(data_result)\n",
    "\n",
    "# export data\n",
    "data_result.to_csv(filtered_result_path, encoding=\"utf8\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
