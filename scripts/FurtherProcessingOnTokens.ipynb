{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "frozen-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-western",
   "metadata": {},
   "source": [
    "# Some Reference\n",
    "\n",
    "* Regular Expression Tester: https://regex101.com/\n",
    "* Some Re Tutorials: https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tutorial-actor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Windows_Storage\\Storage\\Github\\KnowledgeGraph\\scripts\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "similar-graduation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114798\n",
      "  Segmented Element                  POS  Dependecies\n",
      "0            安森美半導體                  NaN          NaN\n",
      "1                 （  PARENTHESISCATEGORY          NaN\n",
      "2  ON Semiconductor                   FW          NaN\n",
      "3                 ）  PARENTHESISCATEGORY          NaN\n",
      "4                 ，        COMMACATEGORY          NaN\n"
     ]
    }
   ],
   "source": [
    "# read in tokens\n",
    "\n",
    "# dataToken = pd.read_csv(\"../results/210330_result/210330_dataset_spaCyResult.csv\", encoding=\"utf8\")\n",
    "dataToken = pd.read_csv(\"../results/210330_result/210330_dataset_monpaResult.csv\", encoding=\"utf8\")\n",
    "\n",
    "# append a column for data if dependencies column does not exist (monpa case)\n",
    "if len(dataToken.columns) < 3:\n",
    "    dataToken.loc[:, \"Dependecies\"] = np.nan\n",
    "\n",
    "print(len(dataToken))\n",
    "print(dataToken.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "loaded-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONPA\n",
    "# relation_dependencies_possible_list = []\n",
    "# relation_pos_possible_list = [\"VH\", \"VC\", \"VJ\", \"VA\"]\n",
    "# relation_pos_re = \"^[V]\"\n",
    "# entity_dependencies_possible_list = []\n",
    "# entity_pos_possible_list = [\"Na\", \"Nv\", \"Neu\", \"Nes\", \"Nf\", \"Ng\", \"Nh\", \"Neqa\", \"Nep\", \"Ncd\", \"FW\", \"DE\"]\n",
    "# entity_pos_re = \"^[N]\"\n",
    "# bracket_pos_list = [\"PARENTHESISCATEGORY\"]\n",
    "# punct_pos_list = [\"COMMACATEGORY\", \"PERIODCATEGORY\"]\n",
    "\n",
    "# spaCy\n",
    "relation_dependencies_possible_list = [\"ROOT\", \"nmod:prep\", \"prep\", \"agent\", ]\n",
    "relation_pos_possible_list = [\"VERB\"]\n",
    "relation_pos_re = \"^[V]\"\n",
    "entity_dependencies_possible_list = [\"compound:nn\", \"nsubj\", \"dep\", \"dobj\"]\n",
    "entity_pos_possible_list = [\"NOUN\", \"PROPN\"]\n",
    "entity_pos_re = \"^[N]\"\n",
    "bracket_pos_list = []\n",
    "punct_pos_list = [\"PUNCT\"]\n",
    "\n",
    "# Common, usually entity\n",
    "sentences_splitter = [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\"]\n",
    "bracket_entity_list = [\"(\", \")\", \"（\", \"）\"]\n",
    "punct_entity_list = [\" \" * i for i in range(0, 100)]\n",
    "conjuction_entity_list = [\"的\", \"、\", \"之\", \"及\", \"與\"]\n",
    "not_entity_relation_list = [\"的\", \"、\", \"之\", \"及\", \"與\", \"\\r\\n \\r\\n \", \"\\r\\n \\r\\n  \"] +\\\n",
    "[\" \" * i for i in range(0, 100)] + [\"\\n\" * i for i in range(0, 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "defined-aggregate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list length (Sentences):  7453 7453 7453\n",
      "['安森美半導體', '（', 'ON Semiconductor', '）'] [nan, 'PARENTHESISCATEGORY', 'FW', 'PARENTHESISCATEGORY'] [nan, nan, nan, nan]\n",
      "list length:  7453 7453 7453\n"
     ]
    }
   ],
   "source": [
    "# devide token by seperators\n",
    "total_entity_list = []\n",
    "sentence_entity_list = []\n",
    "total_label_list = []\n",
    "sentence_label_list = []\n",
    "total_dependencies_list = []\n",
    "sentence_dependencies_list = []\n",
    "\n",
    "\n",
    "# devide tokens by chinese punctuations\n",
    "for index, entityElement in enumerate(dataToken.iloc[:, 0]):\n",
    "    if entityElement not in sentences_splitter:\n",
    "        sentence_entity_list.append(entityElement)\n",
    "        sentence_label_list.append(dataToken.iloc[index, 1])\n",
    "        sentence_dependencies_list.append(dataToken.iloc[index, 2])\n",
    "    else:\n",
    "        total_entity_list.append(sentence_entity_list)\n",
    "        total_label_list.append(sentence_label_list)\n",
    "        total_dependencies_list.append(sentence_dependencies_list)\n",
    "        sentence_entity_list = []\n",
    "        sentence_label_list = []\n",
    "        sentence_dependencies_list = []\n",
    "    \n",
    "print(\"list length (Sentences): \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))\n",
    "\n",
    "\n",
    "# Clean up tokens and eliminate unneccesary tokens\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    indexPunct_0 = -9999\n",
    "    indexPunct_1 = -9999\n",
    "    for tokenIndex, tokens in enumerate(sentences):\n",
    "        # set up element indexes that are between two punctuation\n",
    "        if total_label_list[sentenceIndex][tokenIndex] in bracket_pos_list or sentences[tokenIndex] in bracket_entity_list:\n",
    "            if indexPunct_0 == 0:\n",
    "                indexPunct_0 = tokenIndex\n",
    "            else:\n",
    "                indexPunct_1 = tokenIndex\n",
    "        elif total_label_list[sentenceIndex][tokenIndex] in punct_pos_list or sentences[tokenIndex] in punct_entity_list:\n",
    "            # set token that fit certain POS type to \"\"\n",
    "            total_entity_list[sentenceIndex][tokenIndex] = \"\"\n",
    "            \n",
    "    # remove tokens that are between two punctuations, and certain POS type\n",
    "    if indexPunct_1 == -9999 and indexPunct_0 != -9999:\n",
    "        total_entity_list[sentenceIndex][indexPunct_0] = \"\"\n",
    "        total_label_list[sentenceIndex][indexPunct_0] = \"\"\n",
    "        total_dependencies_list[sentenceIndex][indexPunct_0] = \"\"\n",
    "    elif indexPunct_1 != -9999 and indexPunct_0 != -9999:\n",
    "        for removeIndex in range(indexPunct_0, indexPunct_1 + 1):\n",
    "            total_entity_list[sentenceIndex][removeIndex] = \"\"\n",
    "            total_label_list[sentenceIndex][removeIndex] = \"\"\n",
    "            total_dependencies_list[sentenceIndex][removeIndex] = \"\"\n",
    "            \n",
    "    total_entity_list[sentenceIndex] = list(filter((\"\").__ne__, total_entity_list[sentenceIndex]))\n",
    "    total_label_list[sentenceIndex]  = list(filter((\"\").__ne__, total_label_list[sentenceIndex]))\n",
    "    total_dependencies_list[sentenceIndex] = list(filter((\"\").__ne__, total_dependencies_list[sentenceIndex]))\n",
    "                \n",
    "print(total_entity_list[0] , total_label_list[0], total_dependencies_list[0])\n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "elder-mitchell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7453 7453 7453 7453\n",
      "['安森美半導體'] [0] [] []\n",
      "['今天', '', '系列矽光電倍增管'] [0, 3, 6] ['發表', '新'] [1, 2]\n",
      "['光學雷達', '擴展到其', ''] [2, 9, 11] ['廣泛', '智慧感測方案'] [10, 12]\n",
      "['ArrayRDM-0112A20-QFN', '市場上首款', '車規', '產品'] [0, 5, 8, 10] ['符合'] [6]\n",
      "['汽車產業及其他領域', '應用中', '需求'] [6, 9, 13] ['滿足', '增長'] [1, 11]\n",
      "['ArrayRDM-0112A20-QFN', '單片1', '12', '陣列'] [0, 3, 5, 8] [] []\n",
      "['安森美半導體', '市場的RDM製程'] [1, 5] ['領先'] [2]\n",
      "['外 ', ''] [5, 10] ['實現', '近', '紅', '靈敏度'] [1, 3, 4, 12]\n",
      "['905奈米', '業界的18.5％的光子偵測效率'] [3, 17] ['達到', '領先'] [8, 9]\n",
      "['', '內部增益', '其靈敏度', '光子水準'] [1, 4, 7, 12] ['高', '使', '達到'] [2, 5, 9]\n",
      "['功能', '結合使用'] [2, 6] ['高'] [3]\n",
      "['', '信號'] [4, 6] ['檢測', '微弱', '返回'] [1, 3, 5]\n",
      "[] [] [] []\n",
      "['反射目標'] [4] ['低'] [2]\n",
      "['距離'] [6] ['偵測到', '遠'] [2, 4]\n"
     ]
    }
   ],
   "source": [
    "# get entity pairs and relations\n",
    "\n",
    "# get relations\n",
    "all_relations_list = []\n",
    "all_relations_index = []\n",
    "all_element_flatten = []\n",
    "all_relation_flatten = []\n",
    "\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    relation_list = []\n",
    "    relation_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1: \n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in relation_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in relation_pos_possible_list or\\\n",
    "            re.match(relation_pos_re, str(total_label_list[sentenceIndex][tokenIndex]), flags=re.IGNORECASE) != None:\n",
    "                relation_list.append(token)\n",
    "                relation_index_list.append(tokenIndex)\n",
    "    all_relations_list.append(relation_list)\n",
    "    all_relations_index.append(relation_index_list)\n",
    "\n",
    "\n",
    "# get entities\n",
    "all_reformatted_entities = []\n",
    "all_reformatted_index_list = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    entity_index_list = []\n",
    "    possible_entities = []\n",
    "    reformatted_entities = []\n",
    "    reformatted_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1:\n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in entity_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in entity_pos_possible_list or\\\n",
    "            re.match(entity_pos_re, str(total_label_list[sentenceIndex][tokenIndex]), flags=re.IGNORECASE) != None or\\\n",
    "            token in conjuction_entity_list:\n",
    "                entity_index_list.append(tokenIndex)\n",
    "                possible_entities.append(token)\n",
    "    # combine token if situate next to each other\n",
    "#     print(entity_index_list, possible_entities)\n",
    "    if len(possible_entities) > 0:\n",
    "        combine_entity_name = possible_entities[0]\n",
    "        for possibleIndex, possibleElement in enumerate(entity_index_list):\n",
    "#             print(possible_entities)\n",
    "            \n",
    "            isContinuous = False\n",
    "            if possibleIndex != 0:\n",
    "                if possibleElement == entity_index_list[possibleIndex - 1] + 1:\n",
    "                    isContinuous = True\n",
    "                    combine_entity_name += possible_entities[possibleIndex]\n",
    "                else:\n",
    "                    isContinuous = False\n",
    "            \n",
    "                if isContinuous == False:\n",
    "                    reformatted_entities.append(combine_entity_name)\n",
    "                    reformatted_index_list.append(entity_index_list[possibleIndex-1])\n",
    "                    combine_entity_name = possible_entities[possibleIndex]\n",
    "                    \n",
    "            if possibleIndex == (len(entity_index_list) - 1):\n",
    "                reformatted_entities.append(combine_entity_name)\n",
    "                reformatted_index_list.append(possibleElement)\n",
    "                \n",
    "#             print(combine_entity_name)     \n",
    "#     print(reformatted_entities)\n",
    "    all_reformatted_entities.append(reformatted_entities)\n",
    "    all_reformatted_index_list.append(reformatted_index_list)\n",
    "    \n",
    "# flatten section\n",
    "# remove unneccessary conjuction words\n",
    "for index, element in enumerate(all_reformatted_entities):\n",
    "    \n",
    "    for elementIndex, elementSingle in enumerate(element):\n",
    "        # remove single conjuction char in first or last position\n",
    "        if elementSingle[0] in conjuction_entity_list:\n",
    "            element[elementIndex] = elementSingle[1:]\n",
    "        elif elementSingle[-1] in conjuction_entity_list:\n",
    "            element[elementIndex] = elementSingle[:-1]\n",
    "            \n",
    "        all_element_flatten.append(element[elementIndex])\n",
    "        \n",
    "    for relationIndex, relationSingle in enumerate(all_relations_list[index]):\n",
    "        # remove single conjuction char in first or last position\n",
    "        if relationSingle[0] in conjuction_entity_list:\n",
    "            all_relations_list[index][relationIndex] = relationSingle[1:]\n",
    "        elif relationSingle[-1] in conjuction_entity_list:\n",
    "            all_relations_list[index][relationIndex] = relationSingle[:-1]\n",
    "            \n",
    "        all_relation_flatten.append(all_relations_list[index][relationIndex])\n",
    "                      \n",
    "        \n",
    "# print(all_relations_list, len(all_relations_list), \"\\n\")\n",
    "# print(all_reformatted_entities, len(all_reformatted_entities), )\n",
    "print(len(all_relations_list), len(all_relations_index), len(all_reformatted_entities), len(all_reformatted_index_list))\n",
    "\n",
    "# print out parsing result\n",
    "for index, element in enumerate(all_reformatted_entities[:15]):\n",
    "    print(element, all_reformatted_index_list[index], all_relations_list[index], all_relations_index[index])\n",
    "    \n",
    "data_result = pd.DataFrame({\n",
    "    \"Entity\":all_reformatted_entities,\n",
    "    \"EntityIndex\":all_reformatted_index_list,\n",
    "    \"Relation\":all_relations_list,\n",
    "    \"RelationIndex\":all_relations_index\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recovered-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conculde dataframe with all tokens\n",
    "        \n",
    "for notIndex, notElement in enumerate(not_entity_relation_list):\n",
    "    all_element_flatten = list(filter((notElement).__ne__, all_element_flatten))\n",
    "    all_relation_flatten = list(filter((notElement).__ne__, all_relation_flatten))\n",
    "        \n",
    "data_entity = pd.DataFrame({\n",
    "    \"Entity\":all_element_flatten,\n",
    "})\n",
    "\n",
    "data_relation = pd.DataFrame({\n",
    "    \"Relation\":all_relation_flatten,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "continued-average",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2310 \n",
      "\n",
      " ['行动', '表明', '表示', '要求', '规定', '觉得', '认为', '认真', '认识', '说明', '转动', '转变', '转贴', '达到', '迅速', '过去', '过来', '运用', '这点', '这种', '这麽', '进入', '进步', '进行', '适应', '适当', '适用', '逐渐', '通常', '造成', '遇到', '遭到', '避免', '那麽', '部分', '采取', '里面', '重大', '重新', '重要', '问题', '防止', '附近', '限制', '随著', '集中', '需要', '高兴', '是不是', '说说']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-c879b4674082>:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data_temp = pd.read_csv(\"../docs/stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n"
     ]
    }
   ],
   "source": [
    "# import stopwords list\n",
    "\n",
    "stopword_list = []\n",
    "\n",
    "for fileIndex, fileElement in enumerate(os.listdir(\"./stopwords/\")):\n",
    "    if fileElement[-2:] != \"md\":\n",
    "        data_temp = pd.read_csv(\"./stopwords/\" + fileElement, encoding=\"utf8\", sep=\"@#$%&*\")\n",
    "        stopword_list += data_temp.iloc[:, 0].tolist()\n",
    "        \n",
    "stopword_list = list(dict.fromkeys(stopword_list))\n",
    "        \n",
    "print(len(stopword_list), \"\\n\\n\", stopword_list[-50:])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "amazing-estimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21084\n",
      "18727\n",
      "14301\n",
      "11894\n"
     ]
    }
   ],
   "source": [
    "# remove stop words\n",
    "\n",
    "data_list = [data_entity, data_relation]\n",
    "\n",
    "for dataIndex, dataElement in enumerate(data_list):\n",
    "    drop_index = []\n",
    "    for rowIndex, rowElement in enumerate(dataElement.iloc[:, 0]):\n",
    "        if rowElement in stopword_list:\n",
    "            drop_index.append(rowIndex)\n",
    "    # drop entity/relations that are inside stopword list\n",
    "    print(len(dataElement))\n",
    "    dataElement.drop(index=drop_index, inplace=True)\n",
    "    print(len(dataElement))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dietary-immigration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18727    Entity  Count  Ratio\n",
      "0      晶圓    110  0.95%\n",
      "1      技術    108  0.93%\n",
      "2       圖     99  0.85%\n",
      "3      製程     98  0.85%\n",
      "4       後     87  0.75%\n",
      "5       年     72  0.62%\n",
      "6      晶片     63  0.54%\n",
      "7      產品     61  0.53%\n",
      "8       %     58   0.5%\n",
      "9      我們     55  0.47%\n",
      "10     設備     53  0.46%\n",
      "11      中     48  0.41%\n",
      "12     市場     48  0.41%\n",
      "13      時     46   0.4%\n",
      "14      為     45  0.39%\n",
      "15    半導體     44  0.38%\n",
      "16     資源     42  0.36%\n",
      "17     蝕刻     41  0.35%\n",
      "18      ~     39  0.34%\n",
      "19     表面     39  0.34% \n",
      "\n",
      "11894    Relation  Count  Ratio\n",
      "0         為    238  6.83%\n",
      "1         高    153  4.39%\n",
      "2        回收    124  3.56%\n",
      "3        製造    110  3.16%\n",
      "4         化    109  3.13%\n",
      "5        利用    101   2.9%\n",
      "6        包括     91  2.61%\n",
      "7        產生     89  2.55%\n",
      "8        進行     88  2.53%\n",
      "9         來     72  2.07%\n",
      "10       進行     70  2.01%\n",
      "11       不同     65  1.87%\n",
      "12       提供     64  1.84%\n",
      "13        新     63  1.81%\n",
      "14       萃取     61  1.75%\n",
      "15       清洗     61  1.75%\n",
      "16       增加     60  1.72%\n",
      "17       相關     58  1.66%\n",
      "18       成長     57  1.64%\n",
      "19       生產     57  1.64%\n"
     ]
    }
   ],
   "source": [
    "# conculde a list with frequency\n",
    "\n",
    "def ratiolize(x):\n",
    "    x = str(np.around(x * 100, decimals=2)) + \"%\"\n",
    "    return x\n",
    "\n",
    "data_entity_value_count = data_entity.value_counts(ascending=False).to_frame()\n",
    "data_entity_value_count.reset_index(inplace=True)\n",
    "data_entity_value_count = data_entity_value_count.rename(columns = {'index':'Entity'})\n",
    "data_entity_value_count = data_entity_value_count.rename(columns = {0:'Count'})\n",
    "data_entity_value_count.loc[:, \"Ratio\"] = data_entity_value_count.iloc[:, 1] / len(data_entity_value_count)\n",
    "data_entity_value_count.loc[:, \"Ratio\"] = data_entity_value_count.loc[:, \"Ratio\"].apply(ratiolize)\n",
    "\n",
    "data_relation_value_count = data_relation.value_counts(ascending=False).to_frame()\n",
    "data_relation_value_count.reset_index(inplace=True)\n",
    "data_relation_value_count = data_relation_value_count.rename(columns = {'index':'Relation'})\n",
    "data_relation_value_count = data_relation_value_count.rename(columns = {0:'Count'})\n",
    "data_relation_value_count.loc[:, \"Ratio\"] = data_relation_value_count.iloc[:, 1] / len(data_relation_value_count)\n",
    "data_relation_value_count.loc[:, \"Ratio\"] = data_relation_value_count.loc[:, \"Ratio\"].apply(ratiolize)\n",
    "\n",
    "print(len(data_entity), data_entity_value_count[:20], \"\\n\")\n",
    "print(len(data_relation), data_relation_value_count[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "duplicate-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Savinf result without filtering\n",
    "\n",
    "# data_entity_value_count.to_csv(\"../results/210330_result/210330_dataset_entity_result_spaCy.csv\", encoding=\"utf8\", index=None)\n",
    "# data_relation_value_count.to_csv(\"../results/210330_result/210330_dataset_relation_result_spaCy.csv\", encoding=\"utf8\", index=None)\n",
    "# data_result.to_csv(\"../results/210330_result/210330_dataset_main_result_spaCy.csv\", encoding=\"utf8\", index=None)\n",
    "\n",
    "data_entity_value_count.to_csv(\"../results/210330_result/210330_dataset_entity_result_MONPA.csv\", encoding=\"utf8\", index=None)\n",
    "data_relation_value_count.to_csv(\"../results/210330_result/210330_dataset_relation_result_MONPA.csv\", encoding=\"utf8\", index=None)\n",
    "data_result.to_csv(\"../results/210330_result/210330_dataset_main_result_MONPA.csv\", encoding=\"utf8\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-board",
   "metadata": {},
   "source": [
    "# Filter out most frequent edge and relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tired-shape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Entity          EntityIndex  \\\n",
      "9               ['', '內部增益', '其靈敏度', '光子水準']        [1, 4, 7, 12]   \n",
      "20          ['陽光條件下', '距離', '時', '訊噪', '性能']  [7, 10, 13, 18, 20]   \n",
      "21               ['其他優勢', '電源偏置', '溫度變化敏感性']           [1, 7, 14]   \n",
      "25                        ['應用', '市場', '方案']            [2, 6, 8]   \n",
      "29                              ['其他感知模式互補']                  [5]   \n",
      "...                                      ...                  ...   \n",
      "7440                 ['冷凝處理', '廢氣成分中', '溫度']            [1, 6, 9]   \n",
      "7441  ['冷凝作用', '兩種', '定溫', '增加系統之壓力', '在定壓']    [1, 5, 8, 13, 16]   \n",
      "7444              ['冷凝器型式', '表面式及接觸式', '凝器']           [4, 9, 11]   \n",
      "7447                     ['下', '蒸 氣附著', '上']            [1, 6, 9]   \n",
      "7450                             ['後', '處置']               [2, 6]   \n",
      "\n",
      "                               Relation          RelationIndex  \n",
      "9                      ['高', '使', '達到']              [2, 5, 9]  \n",
      "20    ['明亮', '進行', '長', '測', '提供', '佳']  [3, 8, 9, 11, 14, 16]  \n",
      "21                     ['包括', '低', '低']             [2, 4, 10]  \n",
      "25                         ['實現', '廣大']                 [1, 4]  \n",
      "29                   ['通過', '提供', '冗餘']              [0, 7, 8]  \n",
      "...                                 ...                    ...  \n",
      "7440                  ['利用', '凝結', '而']             [3, 7, 11]  \n",
      "7441                         ['包括', '']                [3, 12]  \n",
      "7444                  ['常用', '包括', '有']              [1, 5, 6]  \n",
      "7447             ['凝結', '來', '管', '外殼']           [0, 2, 7, 8]  \n",
      "7450                  ['排出', '做', '適當']              [1, 3, 4]  \n",
      "\n",
      "[1502 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# data_entity_value_count = pd.read_csv(\"../results/210330_result/210330_dataset_entity_result_spaCy.csv\", encoding=\"utf8\")\n",
    "# data_relation_value_count = pd.read_csv(\"../results/210330_result/210330_dataset_relation_result_spaCy.csv\", encoding=\"utf8\")\n",
    "# data_result = pd.read_csv(\"../results/210330_result/210330_dataset_main_result_spaCy.csv\", encoding=\"utf8\")\n",
    "\n",
    "data_entity_value_count = pd.read_csv(\"../results/210330_result/210330_dataset_entity_result_MONPA.csv\", encoding=\"utf8\")\n",
    "data_relation_value_count = pd.read_csv(\"../results/210330_result/210330_dataset_relation_result_MONPA.csv\", encoding=\"utf8\")\n",
    "data_result = pd.read_csv(\"../results/210330_result/210330_dataset_main_result_MONPA.csv\", encoding=\"utf8\")\n",
    "\n",
    "# take only first 20 places\n",
    "data_entity_filter = data_entity_value_count.loc[:, \"Entity\"].tolist()[:20]\n",
    "data_relation_filter = data_relation_value_count.loc[:, \"Relation\"].tolist()[:20]\n",
    "\n",
    "abandonIndexList = []\n",
    "\n",
    "for rowIndex, rowItem in enumerate(data_result.iloc[:, 0]):\n",
    "    need_to_abandon = True\n",
    "    entityList = rowItem.replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")[:-1]\n",
    "    relationList = data_result.iloc[rowIndex, 2].replace(\"'\", \"\").replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split(\",\")[:-1]\n",
    "    \n",
    "    for entityIndex, entityElement in enumerate(entityList):\n",
    "        if entityElement in data_entity_filter:\n",
    "            need_to_abandon = False\n",
    "            break\n",
    "    \n",
    "    for relationIndex, relationElement in enumerate(relationList):\n",
    "        if need_to_abandon == False:\n",
    "            break\n",
    "        if relationElement in data_relation_filter:\n",
    "            need_to_abandon = False\n",
    "            break\n",
    "            \n",
    "    if need_to_abandon == True:\n",
    "        abandonIndexList.append(rowIndex)\n",
    "            \n",
    "data_result.drop(index=abandonIndexList, inplace=True)\n",
    "print(data_result)\n",
    "\n",
    "# export data\n",
    "data_result.to_csv(\"../results/210330_result/210330_dataset_filtered_result_spaCy.csv\", encoding=\"utf8\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monpa",
   "language": "python",
   "name": "monpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
