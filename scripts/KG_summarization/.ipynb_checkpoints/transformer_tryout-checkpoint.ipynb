{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3409fd25-5092-4172-b8e7-e14aae41330d",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853b0745-ef0a-4f4c-9f7e-cbf5b6c1722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "from torchtext.legacy import data\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "TRAIN_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\train.src\"\n",
    "TRAIN_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\train.tgt\"\n",
    "VALID_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\valid.src\"\n",
    "VALID_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\valid.tgt\"\n",
    "TEST_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\test.src\"\n",
    "TEST_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\test.tgt\"\n",
    "\n",
    "SEED = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4334a21f-0e89-40f2-a4ee-b8dc196d8d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148318 148318 20394 20394 16689 16688\n",
      "非常时期，行非常之策。抗疫期间，中央银行频繁释放流动性，有效对冲了疫情带来的负面影响，但也产生了新的潜在风险。3月末，广义货币（M2）同比增长10.1%，增速分别上年同期高1.5个百分点；本外币贷款余额同比增长12.3%，一季度人民币贷款增加7.1万亿元，同比多增1.29万亿元。3月末，社会融资规模存量同比增长11.5%，增速比上年末高0.8个百分点。一季度，社融规模增加11.08万亿元，同比多2.47万亿元。这些资金引发了人们对房地产强劲反弹的担忧。一季度，为防止企业资金链断裂，金融机构提供的短期贷款新增了2.3万亿元，同比多增1.25万亿元；支持企业复工复产，中长期贷款新增了3.04万亿元，多增4766亿元。住户贷款总量同比少增了约6000亿元，这是受疫情影响，居民消费和购房大幅减少所致。但是，3月份住户贷款明显好转，新增贷款9865亿元，同比多增961亿元，比2月大幅多增。一季度，全国房地产开发投资同比下降7.7%，降幅比1-2月份收窄8.6个百分点。其中，住宅投资下降7.2%，降幅收窄8.8个百分点。商品房销售面积下降26.3%，销售额下降24.7%，降幅分别比1-2月收窄13.6和11.2个百分点。这些都说明，随着经济恢复正常化，房地产也在加快恢复。当前流动性比较充裕，迫切需要引导资金合理流动。由于存在“复工不达产”的现象，疫情导致供应链、产业链受到一定冲击，企业生产订单减少，企业增加资金投入的风险加大；加之疫情全球蔓延、外部形势仍不明朗，未来外贸形势严峻，使得资金流向实体经济受阻。受预期影响，金融市场动荡加剧，不会成为吸纳资金的很好去处。疫情期间，各地暂停房屋建设和销售，这些“延后性需求”可能会在疫情后爆发出来，加之前期疫情导致“供给不足”，房屋供求矛盾将变得更加突出。常言道：“按下葫芦浮起瓢”，前期投放的资金是否会趁机流入房地产市场兴风作浪？确实令人担忧。面对当下异常复杂的国内外经济形势，中央再次强调“要坚持房子是用来住的、不是用来炒的定位，促进房地产市场平稳健康发展”，表明中央始终保持着清醒头脑和战略定力，及时提示要防范房地产炒作风险。（文丨徐洪才中国政策科学研究会经济政策委员会副主任欧美同学会留美分会副会长） （原题为《热评丨重申“房住不炒”具有重要现实意义》）(本文来自澎湃新闻，更多原创资讯请下载“澎湃新闻”APP) \n",
      "\n",
      " 中央再次强调“要坚持房子是用来住的、不是用来炒的定位，促进房地产市场平稳健康发展”，表明中央始终保持着清醒头脑和战略定力，及时提示要防范房地产炒作风险。\n"
     ]
    }
   ],
   "source": [
    "train_x_list = []\n",
    "train_y_list = []\n",
    "valid_x_list = []\n",
    "valid_y_list = []\n",
    "test_x_list = []\n",
    "test_y_list = []\n",
    "\n",
    "file_train_x = codecs.open(TRAIN_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_train_y = codecs.open(TRAIN_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_valid_x = codecs.open(VALID_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_valid_y = codecs.open(VALID_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_test_x = codecs.open(TEST_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_test_y = codecs.open(TEST_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "\n",
    "# create list for training, validation, testing set\n",
    "while True:\n",
    "    line = file_train_x.readline()\n",
    "    train_x_list.append(line.replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "    \n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "while True:\n",
    "    line = file_train_y.readline()\n",
    "    train_y_list.append(line.replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "    \n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "while True:\n",
    "    line = file_valid_x.readline()\n",
    "    valid_x_list.append(line.replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "    \n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "while True:\n",
    "    line = file_valid_y.readline()\n",
    "    valid_y_list.append(line.replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "    \n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "while True:\n",
    "    line = file_test_x.readline()\n",
    "    test_x_list.append(line.replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "    \n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "while True:\n",
    "    line = file_test_y.readline()\n",
    "    test_y_list.append(line.replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "    \n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "print(len(train_x_list), len(train_y_list), len(valid_x_list), len(valid_y_list), len(test_x_list), len(test_y_list))\n",
    "print(train_x_list[10], \"\\n\\n\", train_y_list[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b36d6e9-13a8-4173-acab-339de7a19cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenization function\n",
    "def tokenize(text):\n",
    "    return [char for char in text]\n",
    "\n",
    "SRC = Field(tokenize = tokenize, \n",
    "            init_token = '<bos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True,                 \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize, \n",
    "            init_token = '<bos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True,                 \n",
    "            batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12622806-ae73-4fe5-b770-a3f7155d208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148318it [01:02, 2355.69it/s]\n",
      "20394it [00:07, 2623.28it/s]\n",
      "100%|██████████| 16689/16689 [00:08<00:00, 1944.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# building customize dataset\n",
    "# reference: https://www.programmersought.com/article/7283735573/\n",
    "\n",
    "# get_dataset constructs and returns the examples and fields required by the Dataset\n",
    "def get_dataset(input_data, output_data, input_data_field, output_data_field, test=False):\n",
    "\t# idData pair training is useless during training, use None to specify its corresponding field\n",
    "    fields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"src\", input_data_field), (\"trg\", output_data_field)]       \n",
    "    examples = []\n",
    "\n",
    "    if test:\n",
    "        # If it is a test set, the label is not loaded\n",
    "        for text in tqdm(input_data):\n",
    "            examples.append(data.Example.fromlist([None, text, None], fields))\n",
    "    else:\n",
    "        for text, label in tqdm(zip(input_data, output_data)):\n",
    "            examples.append(data.Example.fromlist([None, text, label], fields))\n",
    "    return examples, fields\n",
    "\n",
    "# Get the examples and fields needed to build the Dataset\n",
    "train_examples, train_fields = get_dataset(train_x_list, train_y_list, SRC, TRG)\n",
    "valid_examples, valid_fields = get_dataset(valid_x_list, valid_y_list, SRC, TRG)\n",
    "test_examples, test_fields = get_dataset(test_x_list, test_y_list, SRC, None, test=True)\n",
    "\n",
    "#Build Dataset dataset\n",
    "train_data = data.Dataset(train_examples, train_fields)\n",
    "valid_data = data.Dataset(valid_examples, valid_fields)\n",
    "test_data = data.Dataset(test_examples, test_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34881da-1823-4fb8-b703-630aeed639fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 10008\n",
      "Unique tokens in target (en) vocabulary: 5504\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f8a66d3-6ae2-45d0-aa4a-34180f98b45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2f6235f-4f7c-42f0-89e8-45ca2401c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "# Use BucketIterator instead of the usual one to automatically deal with padding problem\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n",
    "                                                                      batch_size = BATCH_SIZE, \n",
    "                                                                      device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef39553-362d-4ff8-a3c8-d0e66a5d54ff",
   "metadata": {},
   "source": [
    "# Modeling Part\n",
    "<figure>\n",
    "   <center>\n",
    "<img src='./figures/TransoformerModelStructure.PNG' width=\"400\"/>\n",
    "<figcaption>Self-attention</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567d7267-e15a-4a6f-bf47-92cb3c9ba00d",
   "metadata": {},
   "source": [
    "## a. Encoder\n",
    "\n",
    "這裡先讓大家有一個概觀，了解 `Encoder` 的輸入與輸出分別是什麼:\n",
    "\n",
    "目前進行的機器翻譯任務是德文翻譯成英文：\n",
    "\n",
    "* `Encoder`: \n",
    "  * **輸入**: 德文句子 (`source sentence`) 進行 `word embedding` 之後形成詞向量矩陣 $X\\in R^{N\\times d_{model}}$ ($d_{model}$ 表示詞向量維度)，最主要的目的是將 `source sentence` 作為 `Query,Key,Value(Q,K,V)` 進行 `self-attention`。\n",
    "  * **輸出**: `hidden representation` (矩陣)，維度與轉為 word embedding 的輸入一樣 ($\\in R^{N\\times d_{model}}$)。\n",
    "\n",
    "* `Source mask`\n",
    "  \n",
    "  在前處理中，需要把輸入的每個 batch 的句子做 padding 變成統一長度，主要是希望計算 loss 時不被算進去，padding 的地方也不希望模型注意到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6754f6f4-b43b-43fa-9f25-8b7102df568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_layers, n_heads, pf_dim, dropout, learnable_pos= True, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        # input_dim = dictionary size of the src language\n",
    "        # nn.Embedding: 吃進一個 token (整數), 吐出一個 d_model 維度的向量\n",
    "        self.tok_embedding = nn.Embedding(input_dim, d_model)\n",
    "        if learnable_pos:\n",
    "            self.pos_embedding = nn.Embedding(max_length, d_model)\n",
    "        else:\n",
    "            self.pos_embedding = PositionalEncoding(max_length, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_seq_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_seq_len)\n",
    "        Output:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "        # pos.shape = (batch_size, src_len)\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        # src.shape = (batch_size, src_len, d_model)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "                \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf1424aa-9b24-44db-928e-b54d4fa0c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer used inside Encoder module\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.ff_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.self_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(d_model, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_seq_len)\n",
    "        Output:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"        \n",
    "        # self-attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "               \n",
    "        # positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        # dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22e7c1-964b-4a3e-b42a-3ece23ad04bf",
   "metadata": {},
   "source": [
    "## b. Positional Encoding\n",
    "\n",
    "Word Embedding 所表達的是所有詞向量之間的相似關係，而 Transformer 的做法是透過內積解決RNN的長距離依賴問題 (long-range dependenices) ，但是 Transformer 這樣做卻沒有考慮到句子中的詞先後順序關係，透過 Positional Encoding ，讓詞向量之間不只因為 word embedding 語義關係而靠近，也可以因為詞之間的位置相互靠近而靠近。Positional Encoding 的公式如下：\n",
    "\n",
    "$$\n",
    "PE_{(pos,2i)} = \\sin(pos/10000^{\\frac{2i}{d_{model}}}) \\\\\n",
    "PE_{(pos,2i+1)} = \\cos(pos/10000^{\\frac{2i}{d_{model}}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff6fe5e-87bc-47ab-8c5d-97a6c7f7c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_length, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        # position.shape = (max_length, 1)\n",
    "        angle_rates = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # angle_rates.shape = (d_model/2,)\n",
    "        angle_rads = position * angle_rates\n",
    "        # angle_rads.shape = (max_length, d_model/2)\n",
    "        pe[:, 0::2] = torch.sin(angle_rads)   # 取偶數\n",
    "        pe[:, 1::2] = torch.cos(angle_rads)   # 取奇數\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # pe.shape = (1, max_length, d_model)\n",
    "        self.register_buffer('pe', pe) # register a constant tensor (not updated by optim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x.shape = (batch_size, src_len)\n",
    "        Output:\n",
    "            x.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"\n",
    "        x = torch.zeros(x.size(0), x.size(1), self.d_model) + self.pe[:, :x.size(1)]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897ca75b-88f0-46f0-9a5e-384e35e9cf24",
   "metadata": {},
   "source": [
    "## c. Multi-head self-attention\n",
    "\n",
    "* Self-attention\n",
    "<figure>\n",
    "<center>\n",
    "<img src='./figures/Multi-headSelf-attention.PNG' width=\"600\"/>\n",
    "<figcaption>Multi-Head Attention structure</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "架構如上圖，`Self-attention` 又稱自注意機制，在論文中被稱為 `Scaled Dot-Product Attention`\b，這個架構是 `Transformer` 的核心架構，在這模型會學到句子中詞與詞之間的關係。考慮一個句子，要關注的詞稱為 `Query`，被關注的詞則稱為 `Key`。通常都是一個 `Query` 去關注多個 `Key`。Self-attention 基本上有以下幾個步驟：\n",
    "\n",
    "1. `Word embedding`: 假設句子斷詞後(n 個詞)，每個詞已轉為詞向量且詞向量維度為 $d_k$，則句子就能表示為 $X\\in R^{n\\times d_k}$。\n",
    "2. `Q,K,V`: 將 $X$ 分別通過三個不同的全連接層 $W_Q,W_K,W_V$ 得到 $Q,K,V\\in R^{n\\times d_k}$。\n",
    "3. `Self-attention`: 將 $Q$ 和 $K$ 做矩陣相乘 ($QK^\\top$) 得到注意力矩陣，如同下圖中的方陣一樣，每個詞之間都會有一個注意力的值 $M_{i,j}$ (在此教學最後我們會視覺化這個注意力矩陣)，最後再將注意力矩陣與 $V$ 做矩陣相乘，得到輸出 $\\in R^{n\\times d_k}$。\n",
    "\n",
    "$$\n",
    "\\mathrm{Attention}(Q,K,V) = \\mathrm{Softmax}(\\frac{QK^\\top}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "`Transformer` 的精髓就在此，使用矩陣相乘讓詞與詞之間計算注意力，而矩陣相乘本質上就是多個內積，所以 `self-attention` 就是使用內積來實現注意力機制。\n",
    "\n",
    "* Multi-head\n",
    "\n",
    "  流程: 將 `Q,K,V` 分成 num_heads 份，各自做 self-attention，然後再 Concat ，接著通過 dense 輸出。\n",
    "\n",
    "  分成 num_heads 的優點最主要是希望讓每個 head 各自注意到序列中不同的地方，而且切分成較小的矩陣還能加速訓練過程。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9b9136b-ab04-4745-901f-ef5e33493989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 確保d_model可以被num_heads整除\n",
    "        assert d_model % self.n_heads == 0\n",
    "        self.head_dim = d_model // n_heads  # 將 d_model dimension 分成 n_heads 份\n",
    "        \n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            query.shape = (batch_size, query_len, d_model)\n",
    "            key.shape = (batch_size, key_len, d_model)\n",
    "            value.shape = (batch_size, value_len, d_model)\n",
    "            mask.shape = (batch_size, 1, query_len, key_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, query_len, d_model)\n",
    "            attention_weights.shape = (batch_size, n_heads, query_len, key_len)\n",
    "        \"\"\"   \n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # 通過全連結層形成 Q,K,V        \n",
    "        Q = self.wq(query)\n",
    "        K = self.wk(key)\n",
    "        V = self.wv(value)\n",
    "        # Q.shape = (batch_size, query_len, d_model)\n",
    "        # K.shape = (batch_size, key_len, d_model)\n",
    "        # V.shape = (batch_size, value_len, d_model)\n",
    "\n",
    "        # 將 q,k,v 等份切成 num_heads 份        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # Q.shape = (batch_size, n_heads, query_len, head_dim)\n",
    "        # K.shape = (batch_size, n_heads, key_len, head_dim)\n",
    "        # V.shape = (batch_size, n_heads, value_len, head_dim)\n",
    "\n",
    "        # 每個 heads 分別做 Q,K 內積        \n",
    "        scaled_attention_logits = torch.einsum(\"ijkl,ijml->ijkm\", [Q, K]) / self.scale\n",
    "        # scaled_attention_logits.shape = (batch_size, n_heads, query_len, key_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_attention_logits = scaled_attention_logits.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # 得到每個 heads 的 self-attention matrix\n",
    "        attention_weights = torch.softmax(scaled_attention_logits, dim=-1)       \n",
    "        # attention_weights.shape = (batch_size, n_heads, query_len, key_len)\n",
    "                \n",
    "        output = torch.matmul(self.dropout(attention_weights), V)\n",
    "        # output.shape = (batch_size, n_heads, query_len, head_dim)\n",
    "        \n",
    "        output = output.permute(0, 2, 1, 3).contiguous()\n",
    "        # output.shape = (batch_size, query_len, n_heads, head_dim)\n",
    "        \n",
    "        # concat 所有 heads\n",
    "        output = output.view(batch_size, -1, self.d_model)\n",
    "        # output.shape = (batch_size, query_len, d_model)\n",
    "        \n",
    "        output = self.fc_out(output)\n",
    "        # output.shape = (batch_size, query_len, d_model)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28303082-308c-45ae-b861-e482983cf0a0",
   "metadata": {},
   "source": [
    "## d. Point-wise feed forward network\n",
    "\n",
    "通過兩個全連接層：\n",
    "\n",
    "* 第一層 $W_1\\in R^{pf_{dim}\\times d_{model}}$ : 將 $d_{model}$ 變為 $pf_{dim}$。\n",
    "\n",
    "* 第二層 $W_2\\in R^{d_{model}\\times pf_{dim}}$ : 將 $pf_{dim}$ 變回 $d_{model}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d9b2aaf-8456-4dbe-a6e0-9cf7a53d7128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, d_model, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(d_model, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x.shape = (batch_size, seq_len, d_model)\n",
    "        Output:\n",
    "            x.shape = (batch_size, seq_len, d_model)\n",
    "        \"\"\"   \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca43d7-4153-4177-81ab-06cd9b6042d2",
   "metadata": {},
   "source": [
    "## e. Decoder\n",
    "\n",
    "* `Decoder` 的輸入與輸出分別是什麼:\n",
    "\n",
    "  * **輸入**: Encoder output 與英文句子 (`target sentence`)，一開始在預測時是不會有真實答案的，所以會在中文句子開頭加上 `<BOS>` token 形成 `target sentence`(`shifted right`)，後來使用 `Auto regressive` 的方式進行預測，每當預測完一個詞，就將該預測詞與原始輸入詞接在一起再輸入給模型直到預測出 `<EOS>` 為止。\n",
    "  * **輸出**: 英文句子 (`target sentence`)，使用 `Auto regressive` 方式來預測，例如: 輸入 `<BOS>`，預測 $p_1$；輸入 $p_1$，預測 $p_2$，直到預測出 `<EOS>` 為止。\n",
    "\n",
    "* `Masked multi-head attention`\n",
    "  * Masked self-attention，後面需要觀察的 attention weight matrix\n",
    "  * 使用 trg_mask，讓 decoder 輸入只能往前看\n",
    "\n",
    "    source mask 前面談過，這裡來談談 target mask：\n",
    "    \n",
    "    這個在 (masked) self-attention 會使用到，簡單來說就是不讓當前的字去注意到之後字，如下圖。每個詞只能夠往前注意，會這麼做的原因是因為 `Transformer` 在訓練時是一次將正確答案 (整個句子) 輸入給 Decoder，因為預測時是一個詞一個詞依序往後預測，所以不能讓模型先看到答案後面的詞。\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='./figures/MaskedAttentionMatrix.PNG' width=\"300\"/>\n",
    "<figcaption>Masked attention matrix</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b93437d-8f99-49e8-8187-22dda49b1cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, d_model, n_layers, n_heads, pf_dim, dropout, max_length=100):\n",
    "        super().__init__()\n",
    "        # output_dim = dictionary size of the trg language\n",
    "        self.tok_embedding = nn.Embedding(output_dim, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_length, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            trg.shape = (batch_size, trg_len)\n",
    "            enc_src.shape = (batch_size, src_len, d_model)\n",
    "            trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, trg_len, output_dim)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"   \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(device)                    \n",
    "        # pos.shape = (batch_size, trg_len)\n",
    "            \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))        \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention_weights = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            # trg.shape = (batch_size, trg_len, d_model)\n",
    "            # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        # output.shape = (batch_size, trg_len, output_dim)    \n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4c02123-ad26-4bee-ba13-4afbc1c28fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder layer for decoder usage\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.ff_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.self_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(d_model, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            trg.shape = (batch_size, trg_len, d_model)\n",
    "            enc_src.shape = (batch_size, src_len, d_model)\n",
    "            trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        Output:\n",
    "            trg.shape = (batch_size, trg_len, d_model)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"  \n",
    "        # self-attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))    \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "            \n",
    "        # encoder attention\n",
    "        _trg, attention_weights = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))            \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        return trg, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e752040-812e-421e-86d7-b09402791863",
   "metadata": {},
   "source": [
    "## g. Transformer\n",
    "將編碼器 (Encoder) 與解碼器 (Decoder) 結合起來成為 Transformer：\n",
    "\n",
    "  * **輸入**: 德文句子 (`target sentence`) 與英文句子 (`target sentence`)。\n",
    "  * **輸出**: word embedded 的輸出句子以及每個 head 英文對德文句子的注意力矩陣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15019d4b-9d20-4b2b-88ed-370d0d4fdbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        # src.shape = (batch_size, src_len)\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        # trg.shape = (batch_size, trg_len)\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # trg_pad_mask.shape = (batch_size, 1, 1, trg_len)\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        # # 製造上三角矩陣\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()\n",
    "        # trg_sub_mask.shape = (trg_len, trg_len)\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask # AND operation: T & T = T, T & F = F, F & F = F\n",
    "        # trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_len)\n",
    "            trg.shape = (batch_size, trg_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, trg_len, output_dim)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"     \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        # src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        # trg_mask.shape = (batch size, 1, trg_len, trg_len)\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        # enc_src.shape = (batch_size, src_len, d_model)\n",
    "                \n",
    "        output, attention_weights = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        # output.shape = (batch_size, trg_len, output_dim)\n",
    "        # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea872eb9-d3ec-4508-93e9-7f46e5cb1067",
   "metadata": {},
   "source": [
    "# Training Model\n",
    "## Selecting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6de281d-6e68-48bd-87ff-d23db366d033",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "D_MODEL = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "N_EPOCHS = 15\n",
    "CLIP = 1\n",
    "\n",
    "# 建立 encoder 和 decoder class\n",
    "enc = Encoder(INPUT_DIM, D_MODEL, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, D_MODEL, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT)\n",
    "\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token] # PAD_IDX=1\n",
    "\n",
    "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaed6381-31e6-432f-aa92-ebc709688714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (tok_embedding): Embedding(10008, 256)\n",
      "    (pos_embedding): Embedding(100, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (tok_embedding): Embedding(5504, 256)\n",
      "    (pos_embedding): Embedding(100, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=256, out_features=5504, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 9,390,464 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "        \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "print(model.apply(init_weights))\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c9004-3ce4-4a95-83bf-6de17d13ffc9",
   "metadata": {},
   "source": [
    "## b. 選擇優化器與損失函數\n",
    "這邊使用分類任務的損失函數`CrossEntropyLoss`。但部分句子為因為 `padding` 而有許多的 `1`，但是那並不是真實的標籤，所以必須忽略 `padding` 位置的損失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e2ed6aa-b3b1-4871-b181-77cf76c832b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    '''\n",
    "    A wrapper class for optimizer \n",
    "    From https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Optim.py\n",
    "    '''\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4273f308-4b97-4675-8d66-0c6d9c9caea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "optimizer = ScheduledOptim(torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-09),\n",
    "                           d_model=D_MODEL, n_warmup_steps=4000)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eefb2bd-0c4e-4ad8-a0fe-af7ab6168d5f",
   "metadata": {},
   "source": [
    "## c. 定義訓練與驗證迴圈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f68b1fb0-a982-45e0-831d-0a3410bafaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip):  \n",
    "    # train mode\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        # src.shape = (batch_size, src_len)\n",
    "        \n",
    "        trg = batch.trg\n",
    "        # trg.shape = (batch_size, trg_len)\n",
    "        \n",
    "        # 梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(src, trg[:,:-1]) # full teacher forcing\n",
    "        # output.shape = (batch_size, trg_len-1, output_dim)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        # output.shape = ((trg_len-1) * batch_ize, output_dim)\n",
    "        # trg.shape = ((trg_len-1) * batch_size)\n",
    "        \n",
    "        # 計算 loss\n",
    "        loss = criterion(output, trg) # outputs by default are from logits; trg no need to do one-hot encoding\n",
    "        # 反向傳播，計算梯度\n",
    "        loss.backward()\n",
    "        # 做 regularization，使得整體梯度 norm 不超過 1，以防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # 更新優化器\n",
    "        optimizer.step_and_update_lr()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c55ea5d1-0d89-407f-9846-a341591ffeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, iterator, criterion):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            # src.shape = (src_len, batch_size)\n",
    "            \n",
    "            trg = batch.trg\n",
    "            # trg.shape = (trg_len, batch_size)\n",
    "            \n",
    "            output, _ = model(src, trg[:,:-1]) # turn off teacher forcing\n",
    "            # output.shape = (trg_len, batch_size, output_dim)\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            # output.shape = ((trg_len-1) * batch_ize, output_dim)\n",
    "            # trg.shape = ((trg_len-1) * batch_size)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "788efdfa-f938-495b-a0ff-64bba8ed9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算跑一個 Epoch 的時間\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    \n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "# 將 loss vs. Epoch 畫出來\n",
    "def showPlot(tr_points, va_points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(tr_points, label='train loss')\n",
    "    plt.plot(va_points, label='validation loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1c01cd5-569c-4a45-af24-290cd42228b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iterator, valid_iterator, optimizer, criterion):\n",
    "    best_valid_loss = float('inf')\n",
    "    plot_tr_loss = []\n",
    "    plot_va_loss = []\n",
    "    for epoch in range(N_EPOCHS):\n",
    "    \n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate_epoch(model, valid_iterator, criterion)\n",
    "        plot_tr_loss.append(train_loss)\n",
    "        plot_va_loss.append(valid_loss)\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            # 儲存模型 (只存權重)\n",
    "            torch.save(model.state_dict(), 'SavedModel/tr-model_0720.pt')\n",
    "    \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        # PPL 是 perplexity 的縮寫，基本上就是 cross-entropy 指數化；其值越小越好 (minimize probability likelyhood)\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    showPlot(plot_tr_loss, plot_va_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8944f33d-3c8c-4ad1-956d-d5436a416766",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d0f14dae6647>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-4fa4ee5fe4fc>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_iterator, valid_iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mplot_tr_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-d825988303c3>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# 梯度歸零\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# full teacher forcing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;31m# output.shape = (batch_size, trg_len-1, output_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-9ef833cccd39>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, trg)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# trg_mask.shape = (batch size, 1, trg_len, trg_len)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0menc_src\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;31m# enc_src.shape = (batch_size, src_len, d_model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-db07d4ad227d>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, src_mask)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# pos.shape = (batch_size, src_len)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtok_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[1;31m# src.shape = (batch_size, src_len, d_model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    145\u001b[0m         return F.embedding(\n\u001b[0;32m    146\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1911\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1912\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1913\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "train(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
