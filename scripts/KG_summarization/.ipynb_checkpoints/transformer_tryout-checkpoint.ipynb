{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGypE_zDn2xe"
   },
   "source": [
    "# Transformer\n",
    "\n",
    "在 2017 年，Google Brain 團隊發布一篇論文 - [`Attention is all you need`](https://arxiv.org/abs/1706.03762)，此論文提出一個嶄新的架構 - `Transformer`，一舉轟動的自然語言處理領域，`Transformer` 是目前自然語言處理中所有 `SOTA Model (State-of-the-Arts)` 的基底，這篇論文一開始是將 `Transformer` 應用在機器翻譯任務上，其表現直接碾壓了 `Seq2seq` 系列模型，其貢獻可以總結為兩點：\n",
    "\n",
    "* 平行計算: `Seq2seq` 的編碼器以及解碼器都是 `RNN` ，序列輸入給 `RNN` 時必須先等 $x_t$ 運算完才能運算 $x_{t+1}$，這樣導致 `Seq2seq` 無法平行計算。而 `Transformer` 是一次輸入所有序列 $X=\\{x_1,\\cdots,x_n\\}$，不需要去等待前一個時間點。\n",
    "\n",
    "* `Bidirectional Representation`: `Seq2seq` 輸入序列的方式是單向的 ($x_1\\rightarrow x_2\\rightarrow \\cdots \\rightarrow x_n$)，而有時候要理解句子不能只由前往後，也要由後往前 (也就是上下文的資訊)，雖然後來有許多論文提出雙向 Seq2seq (Bidirectional LSTM) 來提升模型表現，但本質上兩個方向在計算過程中還是互相獨立的，效果有限。而 `Transformer` 是透過 `Self-attention` 來一次獲得上下文的資訊，計算過程中不使用 `RNN`。\n",
    "\n",
    "今日深度學習所用的主要套件版本為 (Google Colab 上都已安裝):\n",
    "* python 3.7\n",
    "* PyTorch 1.8\n",
    "* Torchtext 0.9\n",
    "\n",
    "Refs: \n",
    "1. https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
    "2. https://pytorch.org/tutorials/beginner/transformer_tutorial\n",
    "3. https://andrewpeng.dev/transformer-pytorch/\n",
    "4. https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FzBmx31B-zY"
   },
   "source": [
    "# 下載語言工具\n",
    "\n",
    "本次實作資料集工具來源為 [spaCy](https://spacy.io/usage/models) 中的英德文語言部份，首先需要下載語言工具，**下載完畢後請重新啟動執行階段 (執行階段 -> 重新啟動執行階段)** **Runtime -> Restart runtime**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21169,
     "status": "ok",
     "timestamp": 1626971828789,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "GHfJ0iOxCDcQ",
    "outputId": "e442d150-29ae-40a5-e956-b9e59917c850"
   },
   "source": [
    "!pip install -U spacy\n",
    "!pip install einops\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOzbvYHGoHLF"
   },
   "source": [
    "# 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7l1zAjLun06v"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import math, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import einops\n",
    "import gdown\n",
    "import spacy\n",
    "import random, math, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator, Dataset, Example\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i8UALqBXoO7M"
   },
   "outputs": [],
   "source": [
    "# 為了重複實驗方便，我們固定隨機種子\n",
    "SEED = 87\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YijVwAkoZw_"
   },
   "source": [
    "# 資料前處理\n",
    "\n",
    "今天我們將採用 torchtext 中的資料集 Multi30k 來做示範。這是擁有約 30,000 條相對應英文、德文與法文語句的資料集；每個句子約 12 個單字左右。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifd_EXaKodyd"
   },
   "source": [
    "## a. 德文與英文斷詞\n",
    "首先，我們需要將句子斷成合適的最小單位。所以，我們將利用 [spaCy](https://spacy.io/usage/models) 套件來做為斷詞工具 (tokenizer)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Xw9s3bDuoPF1"
   },
   "outputs": [],
   "source": [
    "# # 載入斷詞模型\n",
    "# spacy_de = spacy.load('de_core_news_sm') # for German\n",
    "# spacy_en = spacy.load('en_core_web_sm')  # for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iZIsxVJEn1d6"
   },
   "outputs": [],
   "source": [
    "# 定義斷詞函數\n",
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    將給定的德文語句斷詞\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    將給定的英文語句斷詞\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize(text):\n",
    "    return [char for char in text.split(\" \")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wO895IT_oukU"
   },
   "source": [
    "## b. 添加`<BOS>`(begin of sequence),`<EOS>`(end of sequence) 在句子頭尾\n",
    "我們利用 torchtext 中的 [Field](https://torchtext.readthedocs.io/en/latest/data.html#fields) 物件來統一處理如何將斷詞後的文本轉為 torch tensors。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 101 51 51 21 21\n",
      "['山 西 省 晋 中 市 祁 县 乔 家 大 院 景 区 内 。   IC   资 料 图 山 西 省 晋 中 市 乔 家 大 院 景 区 在 被 文 化 和 旅 游 部 取 消 旅 游 景 区 质 量 等 级 次 日 ， 晋 中 市 召 开 专 题 会 议 研 究 部 署 整 改 工 作 等 。 据 《 晋 中 日 报 》 报 道 ， 8 月 1 日 下 午 ， 晋 中 市 委 书 记 赵 建 平 主 持 召 开 专 题 会 议 ， 研 究 部 署 乔 家 大 院 景 区 再 创 5A 品 牌 整 改 提 升 工 作 。 他 强 调 ， 要 深 刻 汲 取 乔 家 大 院 景 区 摘 牌 教 训 ， 全 面 认 领 问 题 清 单 ， 以 铁 的 手 腕 、 铁 的 措 施 ， 从 严 从 实 彻 查 整 改 ， 以 脱 胎 换 骨 、 凤 凰 涅 槃 的 景 区 形 象 及 时 回 应 社 会 关 切 。 市 委 常 委 、 秘 书 长 、 统 战 部 长 鹿 建 平 ， 副 市 长 辛 琰 、 郝 向 明 参 加 。 在 认 真 听 取 相 关 整 改 提 升 工 作 情 况 汇 ', '3 月 26 日 下 午 ， 水 利 部 水 旱 灾 害 防 御 司 司 长 田 以 堂 在 国 务 院 联 防 联 控 机 制 新 闻 发 布 会 上 表 示 ， 今 年 以 来 ， 三 峡 水 库 向 长 江 中 下 游 累 计 补 水 90 多 亿 立 方 米 ， 为 疫 情 防 控 、 复 工 复 产 提 供 了 用 水 保 障 。 下 一 步 ， 相 关 部 门 将 继 续 调 度 好 三 峡 水 库 ， 同 时 也 保 证 发 电 、 航 运 等 各 方 面 的 用 水 需 求 。 田 以 堂 说 ， 三 峡 工 程 有 防 洪 、 抗 旱 功 能 和 生 态 综 合 效 应 。 三 峡 工 程 建 成 之 前 ， 一 月 至 三 月 宜 昌 站 的 平 均 流 量 在 4000 立 方 米 每 秒 左 右 。 三 峡 工 程 建 成 后 ， 按 照 有 关 规 定 下 泄 流 量 不 少 于 6000 立 方 米 每 秒 。 今 年 ， 由 于 疫 情 防 控 和 复 工 复 产 的 需 求 ， 3 月 前 ， 三 峡 水 库 按 照 不 少 于 7000 立 方 米 每'] \n",
      "\n",
      " ['赵 建 平 指 出 ， 要 主 动 开 展 自 我 剖 析 ， 详 细 制 订 整 改 方 案 ， 以 更 严 的 标 准 、 更 优 的 服 务 ， 认 真 落 实 整 改 措 施 ， 确 保 在 最 短 时 间 内 取 得 显 著 成 效 ， 重 新 达 到 5A 评 估 标 准 。', '田 以 堂 在 国 务 院 联 防 联 控 机 制 新 闻 发 布 会 上 表 示 ， 今 年 以 来 ， 三 峡 水 库 向 长 江 中 下 游 累 计 补 水 90 多 亿 立 方 米 ， 为 疫 情 防 控 、 复 工 复 产 提 供 了 用 水 保 障 。']\n"
     ]
    }
   ],
   "source": [
    "''' Data Source Configuration '''\n",
    "TRAIN_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\train.src\"\n",
    "TRAIN_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\train.tgt\"\n",
    "VALID_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\valid.src\"\n",
    "VALID_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\valid.tgt\"\n",
    "TEST_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\test.src\"\n",
    "TEST_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\test.tgt\"\n",
    "\n",
    "''' Data Configuration '''\n",
    "TRAIN_SAMPLE_NUM = 100\n",
    "VALID_SAMPLE_NUM = 50\n",
    "TEST_SAMPLE_NUM = 20\n",
    "WORD_MIN_FREQUENCY = 0\n",
    "# MAX_LENGTH_INPUT = int(np.max([len(data)+2 for data in train_x_list]))\n",
    "# MAX_LENGTH_OUTPUT = int(np.max([len(data)+2 for data in train_y_list]))\n",
    "MAX_LENGTH = 500\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "\n",
    "''' Process Begins '''\n",
    "train_x_list = []\n",
    "train_y_list = []\n",
    "valid_x_list = []\n",
    "valid_y_list = []\n",
    "test_x_list = []\n",
    "test_y_list = []\n",
    "\n",
    "file_train_x = codecs.open(TRAIN_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_train_y = codecs.open(TRAIN_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_valid_x = codecs.open(VALID_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_valid_y = codecs.open(VALID_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_test_x = codecs.open(TEST_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_test_y = codecs.open(TEST_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "\n",
    "# create list for training, validation, testing set\n",
    "temp_index = 0\n",
    "while True or temp_index == TRAIN_SAMPLE_NUM:\n",
    "    line = file_train_x.readline()\n",
    "    train_x_list.append(line.replace(\"\\n\", \"\"))\n",
    "\n",
    "    if not line or temp_index == TRAIN_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "\n",
    "while True or temp_index == TRAIN_SAMPLE_NUM:\n",
    "    line = file_train_y.readline()\n",
    "    train_y_list.append(line.replace(\"\\n\", \"\"))\n",
    "\n",
    "    if not line or temp_index == TRAIN_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "\n",
    "while True or temp_index == VALID_SAMPLE_NUM:\n",
    "    line = file_valid_x.readline()\n",
    "    valid_x_list.append(line.replace(\"\\n\", \"\"))\n",
    "\n",
    "    if not line or temp_index == VALID_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "\n",
    "while True or temp_index == VALID_SAMPLE_NUM:\n",
    "    line = file_valid_y.readline()\n",
    "    valid_y_list.append(line.replace(\"\\n\", \"\"))\n",
    "\n",
    "    if not line or temp_index == VALID_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "\n",
    "while True or temp_index == TEST_SAMPLE_NUM:\n",
    "    line = file_test_x.readline()\n",
    "    test_x_list.append(line.replace(\"\\n\", \"\"))\n",
    "\n",
    "    if not line or temp_index == TEST_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "\n",
    "while True or temp_index == TEST_SAMPLE_NUM:\n",
    "    line = file_test_y.readline()\n",
    "    test_y_list.append(line.replace(\"\\n\", \"\")[:10])\n",
    "\n",
    "    if not line or temp_index == TEST_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "        \n",
    "# convert samples to desired length\n",
    "for listItem in [train_x_list, train_y_list, valid_x_list, valid_y_list, test_x_list, test_y_list]:\n",
    "    for textIndex, textElement in enumerate(listItem):\n",
    "        listItem[textIndex] = textElement[:MAX_LENGTH]\n",
    "        \n",
    "\n",
    "print(len(train_x_list), len(train_y_list), len(valid_x_list), len(valid_y_list), len(test_x_list),\n",
    "          len(test_y_list))\n",
    "print(train_x_list[:2], \"\\n\\n\", train_y_list[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 德文為 source\n",
    "# SRC = Field(tokenize = tokenize, \n",
    "#             init_token = '<bos>', \n",
    "#             eos_token = '<eos>', \n",
    "#             lower = True,                 # 全部轉為小寫\n",
    "#             batch_first = True)           # batch size (dimension) 放最前面\n",
    "\n",
    "# # 英文為 target\n",
    "# TRG = Field(tokenize = tokenize, \n",
    "#             init_token = '<bos>', \n",
    "#             eos_token = '<eos>', \n",
    "#             lower = True,                 # 全部轉為小寫\n",
    "#             batch_first = True)           # batch size (dimension) 放最前面\n",
    "\n",
    "\n",
    "SRC = Field(sequential=True,\n",
    "                  tokenize=tokenize,\n",
    "                  init_token = '<bos>', \n",
    "                  eos_token = '<eos>',\n",
    "                  lower=True,\n",
    "                  batch_first = True) \n",
    "\n",
    "TRG = Field(sequential=True,\n",
    "                  tokenize=tokenize,\n",
    "                  init_token = '<bos>', \n",
    "                  eos_token = '<eos>',\n",
    "                  lower=True,\n",
    "                  batch_first = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:00, ?it/s]\n",
      "51it [00:00, 12384.76it/s]\n",
      "21it [00:00, 10535.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.legacy.data.dataset.Dataset object at 0x000001C2A6F8C848>\n",
      "Number of training examples: 101\n",
      "Number of validation examples: 51\n",
      "Number of testing examples: 21\n",
      "Unique tokens in source (de) vocabulary: 1470\n",
      "Unique tokens in target (en) vocabulary: 863\n",
      "['3', '月', '26', '日', '下', '午', '，', '水', '利', '部', '水', '旱', '灾', '害', '防', '御', '司', '司', '长', '田', '以', '堂', '在', '国', '务', '院', '联', '防', '联', '控', '机', '制', '新', '闻', '发', '布', '会', '上', '表', '示', '，', '今', '年', '以', '来', '，', '三', '峡', '水', '库', '向', '长', '江', '中', '下', '游', '累', '计', '补', '水', '90', '多', '亿', '立', '方', '米', '，', '为', '疫', '情', '防', '控', '、', '复', '工', '复', '产', '提', '供', '了', '用', '水', '保', '障', '。', '下', '一', '步', '，', '相', '关', '部', '门', '将', '继', '续', '调', '度', '好', '三', '峡', '水', '库', '，', '同', '时', '也', '保', '证', '发', '电', '、', '航', '运', '等', '各', '方', '面', '的', '用', '水', '需', '求', '。', '田', '以', '堂', '说', '，', '三', '峡', '工', '程', '有', '防', '洪', '、', '抗', '旱', '功', '能', '和', '生', '态', '综', '合', '效', '应', '。', '三', '峡', '工', '程', '建', '成', '之', '前', '，', '一', '月', '至', '三', '月', '宜', '昌', '站', '的', '平', '均', '流', '量', '在', '4000', '立', '方', '米', '每', '秒', '左', '右', '。', '三', '峡', '工', '程', '建', '成', '后', '，', '按', '照', '有', '关', '规', '定', '下', '泄', '流', '量', '不', '少', '于', '6000', '立', '方', '米', '每', '秒', '。', '今', '年', '，', '由', '于', '疫', '情', '防', '控', '和', '复', '工', '复', '产', '的', '需', '求', '，', '3', '月', '前', '，', '三', '峡', '水', '库', '按', '照', '不', '少', '于', '7000', '立', '方', '米', '每']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(input_list, output_list, text_field, label_field, test=False):\n",
    "\t# idData pair training is useless during training, use None to specify its corresponding field\n",
    "    fields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"src\", text_field), (\"trg\", label_field)]       \n",
    "    examples = []\n",
    "\n",
    "#     if test:\n",
    "#         # If it is a test set, the label is not loaded\n",
    "#         for text in tqdm(input_list):\n",
    "#             examples.append(Example.fromlist([None, text, None], fields))\n",
    "#     else:\n",
    "#         for text, label in tqdm(zip(input_list, output_list)):\n",
    "#             examples.append(Example.fromlist([None, text, label], fields))\n",
    "            \n",
    "            \n",
    "    for text, label in tqdm(zip(input_list, output_list)):\n",
    "            examples.append(Example.fromlist([None, text, label], fields))\n",
    "    \n",
    "    return examples, fields\n",
    "\n",
    "# Get the examples and fields needed to build the Dataset\n",
    "train_examples, train_fields = get_dataset(train_x_list, train_y_list, SRC, TRG)\n",
    "valid_examples, valid_fields = get_dataset(valid_x_list, valid_y_list, SRC, TRG)\n",
    "test_examples, test_fields = get_dataset(test_x_list, test_y_list, SRC, TRG, test=True)\n",
    "\n",
    "#Build Dataset dataset\n",
    "train_data = Dataset(train_examples, train_fields)\n",
    "valid_data = Dataset(valid_examples, valid_fields)\n",
    "test_data = Dataset(test_examples, test_fields)\n",
    "\n",
    "\n",
    "print(train_data)\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "\n",
    "\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")\n",
    "\n",
    "print(train_data[1].src)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Use BucketIterator instead of the usual one to automatically deal with padding problem\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n",
    "                                                                      batch_size = BATCH_SIZE, \n",
    "                                                                      device = device, # If you use gpu, here -1 is replaced with the GPU number.\n",
    "        sort_key=lambda x: len(x.src), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "        sort_within_batch=False,\n",
    "        repeat=False # we pass repeat=False because we want to wrap this Iterator layer.)\n",
    "                                                                     )\n",
    "\n",
    "# # Constructing iterators for both training and validation sets\n",
    "# train_iterator, valid_iterator = BucketIterator.splits(\n",
    "#         (train_data, valid_data), #Build the data set required by the dataset\n",
    "#         batch_sizes=BATCH_SIZE,\n",
    "#         device=device, # If you use gpu, here -1 is replaced with the GPU number.\n",
    "#         sort_key=lambda x: len(x.src), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "#         sort_within_batch=False,\n",
    "#         repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    "# )\n",
    "\n",
    "# test_iterator = Iterator(test_data, batch_size=BATCH_SIZE, device=device, sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtRkqvM2o4e3"
   },
   "source": [
    "## c. 切分資料集 Multi30k\n",
    "使用功能函數 splits 切分資料為訓練、驗證與測試集；參數 exts 是要告知切分時哪個語言是 source (前)，哪個是 target (後)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4888,
     "status": "ok",
     "timestamp": 1626971857433,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "J8iG1lLAowJ2",
    "outputId": "8a9dc91e-0036-4c1c-e38e-82afdbb9f757"
   },
   "outputs": [],
   "source": [
    "# train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (SRC, TRG))\n",
    "\n",
    "# print(train_data)\n",
    "# print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "# print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "# print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "\n",
    "# SRC.build_vocab(train_data, min_freq = 2)\n",
    "# TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "# print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "# print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")\n",
    "\n",
    "# print(train_data[1].src)\n",
    "\n",
    "# BATCH_SIZE = 128\n",
    "\n",
    "# # Use BucketIterator instead of the usual one to automatically deal with padding problem\n",
    "# train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n",
    "#                                                                       batch_size = BATCH_SIZE, \n",
    "#                                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyHz7LnYpPNw"
   },
   "source": [
    "# 建立 transformer 模型\n",
    "\n",
    "`Transformer` 分為編碼器 (Encoder) 與解碼器 (Decoder)，中間會有一條線 (矩陣) 連接，在 `Transformer` 中，最核心的架構為 `Multi-head Attention` 這個架構，後續有許多論文都針對這個架構提出新的改進方式。\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1BmuP7m3J0-jEdLtSGV6oBsSYGcne9llD' width=\"400\"/>\n",
    "<figcaption>Self-attention</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2eDzsF7pcpl"
   },
   "source": [
    "## a. Encoder\n",
    "\n",
    "這裡先讓大家有一個概觀，了解 `Encoder` 的輸入與輸出分別是什麼:\n",
    "\n",
    "目前進行的機器翻譯任務是德文翻譯成英文：\n",
    "\n",
    "* `Encoder`: \n",
    "  * **輸入**: 德文句子 (`source sentence`) 進行 `word embedding` 之後形成詞向量矩陣 $X\\in R^{N\\times d_{model}}$ ($d_{model}$ 表示詞向量維度)，最主要的目的是將 `source sentence` 作為 `Query,Key,Value(Q,K,V)` 進行 `self-attention`。\n",
    "  * **輸出**: `hidden representation` (矩陣)，維度與轉為 word embedding 的輸入一樣 ($\\in R^{N\\times d_{model}}$)。\n",
    "\n",
    "* `Source mask`\n",
    "  \n",
    "  在前處理中，需要把輸入的每個 batch 的句子做 padding 變成統一長度，主要是希望計算 loss 時不被算進去，padding 的地方也不希望模型注意到。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1626971864409,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "y1Z8ci1RR7Bo",
    "outputId": "c511e27b-4400-4b62-9fd6-0b6059d05073"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source mask example\n",
    "src_sample = torch.ByteTensor([[7,6,1,1,1],[5,4,3,1,1]])\n",
    "src_pad_idx = 1\n",
    "\n",
    "def make_src_mask(src):\n",
    "    # src.shape = (batch_size, src_len)\n",
    "    src_mask = (src != src_pad_idx)\n",
    "    \n",
    "    return src_mask\n",
    "\n",
    "src_mask = make_src_mask(src_sample)\n",
    "src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "P2ODB3APpFfr"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_layers, n_heads, pf_dim, dropout, learnable_pos= True, max_length=500):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        # input_dim = dictionary size of the src language\n",
    "        # nn.Embedding: 吃進一個 token (整數), 吐出一個 d_model 維度的向量\n",
    "        self.tok_embedding = nn.Embedding(input_dim, d_model)\n",
    "        if learnable_pos:\n",
    "            self.pos_embedding = nn.Embedding(max_length, d_model)\n",
    "        else:\n",
    "            self.pos_embedding = PositionalEncoding(max_length, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_seq_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_seq_len)\n",
    "        Output:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "#         print(src.shape)\n",
    "#         print(src_mask.shape)\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "        # pos.shape = (batch_size, src_len)\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        # src.shape = (batch_size, src_len, d_model)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "                \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "U7ziUhUmpFkz"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.ff_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.self_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(d_model, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_seq_len)\n",
    "        Output:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"        \n",
    "        # self-attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "               \n",
    "        # positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        # dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCJN-lXspy7W"
   },
   "source": [
    "## b. Positional Encoding\n",
    "\n",
    "Word Embedding 所表達的是所有詞向量之間的相似關係，而 Transformer 的做法是透過內積解決RNN的長距離依賴問題 (long-range dependenices) ，但是 Transformer 這樣做卻沒有考慮到句子中的詞先後順序關係，透過 Positional Encoding ，讓詞向量之間不只因為 word embedding 語義關係而靠近，也可以因為詞之間的位置相互靠近而靠近。Positional Encoding 的公式如下：\n",
    "\n",
    "$$\n",
    "PE_{(pos,2i)} = \\sin(pos/10000^{\\frac{2i}{d_{model}}}) \\\\\n",
    "PE_{(pos,2i+1)} = \\cos(pos/10000^{\\frac{2i}{d_{model}}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xeB8XWvHpFqo"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_length, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        # position.shape = (max_length, 1)\n",
    "        angle_rates = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # angle_rates.shape = (d_model/2,)\n",
    "        angle_rads = position * angle_rates\n",
    "        # angle_rads.shape = (max_length, d_model/2)\n",
    "        pe[:, 0::2] = torch.sin(angle_rads)   # 取偶數\n",
    "        pe[:, 1::2] = torch.cos(angle_rads)   # 取奇數\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # pe.shape = (1, max_length, d_model)\n",
    "        self.register_buffer('pe', pe) # register a constant tensor (not updated by optim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x.shape = (batch_size, src_len)\n",
    "        Output:\n",
    "            x.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"\n",
    "        x = torch.zeros(x.size(0), x.size(1), self.d_model) + self.pe[:, :x.size(1)]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYus1Z4vqAmM"
   },
   "source": [
    "舉個例子來理解一下 positional encoding (non-learnable)。此例拿第 25 個 token 的 positional encoding 來跟其餘 50 個字 (包含自己) 的 positional encoding 計算內積 (`np.dot`)，能夠發現越靠近 token 25 的內積越大，反之，越遠則內積越小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1NSyANLqI0N"
   },
   "source": [
    "可以看到確實發現越靠近 token 25 的內積越大，反之，越遠則內積越小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "se-KLX5ZqOcd"
   },
   "source": [
    "## c. Multi-head self-attention\n",
    "\n",
    "* Self-attention\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1M1XWNp8ywQrqQtjMJ4TKtRK2q9xssoL2' width=\"600\"/>\n",
    "<figcaption>Multi-Head Attention structure</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "架構如上圖，`Self-attention` 又稱自注意機制，在論文中被稱為 `Scaled Dot-Product Attention`\b，這個架構是 `Transformer` 的核心架構，在這模型會學到句子中詞與詞之間的關係。考慮一個句子，要關注的詞稱為 `Query`，被關注的詞則稱為 `Key`。通常都是一個 `Query` 去關注多個 `Key`。Self-attention 基本上有以下幾個步驟：\n",
    "\n",
    "1. `Word embedding`: 假設句子斷詞後(n 個詞)，每個詞已轉為詞向量且詞向量維度為 $d_k$，則句子就能表示為 $X\\in R^{n\\times d_k}$。\n",
    "2. `Q,K,V`: 將 $X$ 分別通過三個不同的全連接層 $W_Q,W_K,W_V$ 得到 $Q,K,V\\in R^{n\\times d_k}$。\n",
    "3. `Self-attention`: 將 $Q$ 和 $K$ 做矩陣相乘 ($QK^\\top$) 得到注意力矩陣，如同下圖中的方陣一樣，每個詞之間都會有一個注意力的值 $M_{i,j}$ (在此教學最後我們會視覺化這個注意力矩陣)，最後再將注意力矩陣與 $V$ 做矩陣相乘，得到輸出 $\\in R^{n\\times d_k}$。\n",
    "\n",
    "$$\n",
    "\\mathrm{Attention}(Q,K,V) = \\mathrm{Softmax}(\\frac{QK^\\top}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "`Transformer` 的精髓就在此，使用矩陣相乘讓詞與詞之間計算注意力，而矩陣相乘本質上就是多個內積，所以 `self-attention` 就是使用內積來實現注意力機制。\n",
    "\n",
    "* Multi-head\n",
    "\n",
    "  流程: 將 `Q,K,V` 分成 num_heads 份，各自做 self-attention，然後再 Concat ，接著通過 dense 輸出。\n",
    "\n",
    "  分成 num_heads 的優點最主要是希望讓每個 head 各自注意到序列中不同的地方，而且切分成較小的矩陣還能加速訓練過程。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EA9cREuDp0Hx"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 確保d_model可以被num_heads整除\n",
    "        assert d_model % self.n_heads == 0\n",
    "        self.head_dim = d_model // n_heads  # 將 d_model dimension 分成 n_heads 份\n",
    "        \n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            query.shape = (batch_size, query_len, d_model)\n",
    "            key.shape = (batch_size, key_len, d_model)\n",
    "            value.shape = (batch_size, value_len, d_model)\n",
    "            mask.shape = (batch_size, 1, query_len, key_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, query_len, d_model)\n",
    "            attention_weights.shape = (batch_size, n_heads, query_len, key_len)\n",
    "        \"\"\"   \n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # 通過全連結層形成 Q,K,V        \n",
    "        Q = self.wq(query)\n",
    "        K = self.wk(key)\n",
    "        V = self.wv(value)\n",
    "        # Q.shape = (batch_size, query_len, d_model)\n",
    "        # K.shape = (batch_size, key_len, d_model)\n",
    "        # V.shape = (batch_size, value_len, d_model)\n",
    "\n",
    "        # 將 q,k,v 等份切成 num_heads 份        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # Q.shape = (batch_size, n_heads, query_len, head_dim)\n",
    "        # K.shape = (batch_size, n_heads, key_len, head_dim)\n",
    "        # V.shape = (batch_size, n_heads, value_len, head_dim)\n",
    "\n",
    "        # 每個 heads 分別做 Q,K 內積        \n",
    "        scaled_attention_logits = torch.einsum(\"ijkl,ijml->ijkm\", [Q, K]) / self.scale\n",
    "        # scaled_attention_logits.shape = (batch_size, n_heads, query_len, key_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_attention_logits = scaled_attention_logits.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # 得到每個 heads 的 self-attention matrix\n",
    "        attention_weights = torch.softmax(scaled_attention_logits, dim=-1)       \n",
    "        # attention_weights.shape = (batch_size, n_heads, query_len, key_len)\n",
    "                \n",
    "        output = torch.matmul(self.dropout(attention_weights), V)\n",
    "        # output.shape = (batch_size, n_heads, query_len, head_dim)\n",
    "        \n",
    "        output = output.permute(0, 2, 1, 3).contiguous()\n",
    "        # output.shape = (batch_size, query_len, n_heads, head_dim)\n",
    "        \n",
    "        # concat 所有 heads\n",
    "        output = output.view(batch_size, -1, self.d_model)\n",
    "        # output.shape = (batch_size, query_len, d_model)\n",
    "        \n",
    "        output = self.fc_out(output)\n",
    "        # output.shape = (batch_size, query_len, d_model)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f6GbXYBvObj"
   },
   "source": [
    "## d. Point-wise feed forward network\n",
    "\n",
    "通過兩個全連接層：\n",
    "\n",
    "* 第一層 $W_1\\in R^{pf_{dim}\\times d_{model}}$ : 將 $d_{model}$ 變為 $pf_{dim}$。\n",
    "\n",
    "* 第二層 $W_2\\in R^{d_{model}\\times pf_{dim}}$ : 將 $pf_{dim}$ 變回 $d_{model}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mJ8qPh23p0Ll"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, d_model, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(d_model, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x.shape = (batch_size, seq_len, d_model)\n",
    "        Output:\n",
    "            x.shape = (batch_size, seq_len, d_model)\n",
    "        \"\"\"   \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVhEz_hrqjjt"
   },
   "source": [
    "## e. Decoder\n",
    "\n",
    "* `Decoder` 的輸入與輸出分別是什麼:\n",
    "\n",
    "  * **輸入**: Encoder output 與英文句子 (`target sentence`)，一開始在預測時是不會有真實答案的，所以會在中文句子開頭加上 `<BOS>` token 形成 `target sentence`(`shifted right`)，後來使用 `Auto regressive` 的方式進行預測，每當預測完一個詞，就將該預測詞與原始輸入詞接在一起再輸入給模型直到預測出 `<EOS>` 為止。\n",
    "  * **輸出**: 英文句子 (`target sentence`)，使用 `Auto regressive` 方式來預測，例如: 輸入 `<BOS>`，預測 $p_1$；輸入 $p_1$，預測 $p_2$，直到預測出 `<EOS>` 為止。\n",
    "\n",
    "* `Masked multi-head attention`\n",
    "  * Masked self-attention，後面需要觀察的 attention weight matrix\n",
    "  * 使用 trg_mask，讓 decoder 輸入只能往前看\n",
    "\n",
    "    source mask 前面談過，這裡來談談 target mask：\n",
    "    \n",
    "    這個在 (masked) self-attention 會使用到，簡單來說就是不讓當前的字去注意到之後字，如下圖。每個詞只能夠往前注意，會這麼做的原因是因為 `Transformer` 在訓練時是一次將正確答案 (整個句子) 輸入給 Decoder，因為預測時是一個詞一個詞依序往後預測，所以不能讓模型先看到答案後面的詞。\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1-v6WsgedF7s7ooo8xIm2sVfhL5lF_zvK' width=\"300\"/>\n",
    "<figcaption>Masked attention matrix</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3943,
     "status": "ok",
     "timestamp": 1626971888274,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "nKUoppjQbpUx",
    "outputId": "6df970bc-298b-46fb-cade-f38a5229bee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trg_pad_mask: tensor([[[ True,  True,  True, False, False]]], device='cuda:0')\n",
      "tensor([[ True, False, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True]], device='cuda:0')\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False],\n",
       "         [ True,  True, False, False, False],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [ True,  True,  True, False, False]]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target mask example\n",
    "trg_sample = torch.ByteTensor([[5,4,3,1,1]])\n",
    "trg_pad_idx = 1\n",
    "\n",
    "def make_trg_mask(trg):\n",
    "    # trg.shape = (batch_size, trg_len)   \n",
    "    trg_pad_mask = (trg != trg_pad_idx).unsqueeze(1).to(device)\n",
    "    print('trg_pad_mask:', trg_pad_mask)\n",
    "    # trg_pad_mask.shape = (batch_size, 1, trg_len)\n",
    "        \n",
    "    trg_len = trg.shape[1]\n",
    "    # 製造上三角矩陣\n",
    "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()\n",
    "    print(trg_sub_mask)\n",
    "    # trg_sub_mask.shape = (trg_len, trg_len)\n",
    "            \n",
    "    trg_mask = trg_pad_mask & trg_sub_mask # AND operation: T & T = T, T & F = F, F & F = F\n",
    "    # trg_mask.shape = (batch_size, trg_len, trg_len)\n",
    "        \n",
    "    return trg_mask\n",
    "\n",
    "trg_mask = make_trg_mask(trg_sample)\n",
    "print('------------')\n",
    "trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "T90SPpy-qbt-"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, d_model, n_layers, n_heads, pf_dim, dropout, max_length=500):\n",
    "        super().__init__()\n",
    "        # output_dim = dictionary size of the trg language\n",
    "        self.tok_embedding = nn.Embedding(output_dim, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_length, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            trg.shape = (batch_size, trg_len)\n",
    "            enc_src.shape = (batch_size, src_len, d_model)\n",
    "            trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, trg_len, output_dim)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"   \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(device)                    \n",
    "        # pos.shape = (batch_size, trg_len)\n",
    "            \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))        \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention_weights = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            # trg.shape = (batch_size, trg_len, d_model)\n",
    "            # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        # output.shape = (batch_size, trg_len, output_dim)    \n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KksYEetwqb2c"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.ff_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.self_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(d_model, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            trg.shape = (batch_size, trg_len, d_model)\n",
    "            enc_src.shape = (batch_size, src_len, d_model)\n",
    "            trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        Output:\n",
    "            trg.shape = (batch_size, trg_len, d_model)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"  \n",
    "        # self-attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))    \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "            \n",
    "        # encoder attention\n",
    "        _trg, attention_weights = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))            \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        return trg, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anYr1sJh2Ahq"
   },
   "source": [
    "## g. Transformer\n",
    "將編碼器 (Encoder) 與解碼器 (Decoder) 結合起來成為 Transformer：\n",
    "\n",
    "  * **輸入**: 德文句子 (`target sentence`) 與英文句子 (`target sentence`)。\n",
    "  * **輸出**: word embedded 的輸出句子以及每個 head 英文對德文句子的注意力矩陣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fioZQwQdqoeW"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        # src.shape = (batch_size, src_len)\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        # trg.shape = (batch_size, trg_len)\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # trg_pad_mask.shape = (batch_size, 1, 1, trg_len)\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        # # 製造上三角矩陣\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()\n",
    "        # trg_sub_mask.shape = (trg_len, trg_len)\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask # AND operation: T & T = T, T & F = F, F & F = F\n",
    "        # trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_len)\n",
    "            trg.shape = (batch_size, trg_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, trg_len, output_dim)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"     \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        # src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        # trg_mask.shape = (batch size, 1, trg_len, trg_len)\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        # enc_src.shape = (batch_size, src_len, d_model)\n",
    "                \n",
    "        output, attention_weights = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        # output.shape = (batch_size, trg_len, output_dim)\n",
    "        # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5TjPmxvzKkq"
   },
   "source": [
    "# 建模的另一種方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BQOLXl-vzJw-"
   },
   "outputs": [],
   "source": [
    "# Reference: https://andrewpeng.dev/transformer-pytorch/\n",
    "# pytorch_TR = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)\n",
    "# output = pytorch_TR(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask, \n",
    "#                    tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuBz0w3Kqysn"
   },
   "source": [
    "# 訓練模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBb-d8fXmV8J"
   },
   "source": [
    "## a. 超參數設定與建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1TG1-_zHqojR"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "D_MODEL = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "N_EPOCHS = 15\n",
    "CLIP = 1\n",
    "\n",
    "# 建立 encoder 和 decoder class\n",
    "enc = Encoder(INPUT_DIM, D_MODEL, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, MAX_LENGTH)\n",
    "dec = Decoder(OUTPUT_DIM, D_MODEL, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, MAX_LENGTH)\n",
    "\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token] # PAD_IDX=1\n",
    "\n",
    "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1626971899422,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "Gcx2KKyuqoop",
    "outputId": "a2457abf-d53d-48f8-9b7c-e05e5756b471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (tok_embedding): Embedding(1470, 256)\n",
      "    (pos_embedding): Embedding(500, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (tok_embedding): Embedding(863, 256)\n",
      "    (pos_embedding): Embedding(500, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=256, out_features=863, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 5,028,703 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "        \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "print(model.apply(init_weights))\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ibc1pj62GzT-"
   },
   "source": [
    "## b. 選擇優化器與損失函數\n",
    "這邊使用分類任務的損失函數`CrossEntropyLoss`。但部分句子為因為 `padding` 而有許多的 `1`，但是那並不是真實的標籤，所以必須忽略 `padding` 位置的損失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "qxdJojzaJspa"
   },
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    '''\n",
    "    A wrapper class for optimizer \n",
    "    From https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Optim.py\n",
    "    '''\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1IyVu2asq6d9"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "optimizer = ScheduledOptim(torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-09),\n",
    "                           d_model=D_MODEL, n_warmup_steps=4000)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_T-O_UDLa5u"
   },
   "source": [
    "## c. 定義訓練與驗證迴圈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "DG2rhg1tq6kV"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip):  \n",
    "    # train mode\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        # src.shape = (batch_size, src_len)\n",
    "        \n",
    "        trg = batch.trg\n",
    "        # trg.shape = (batch_size, trg_len)\n",
    "        \n",
    "        # 梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(src, trg[:,:-1]) # full teacher forcing\n",
    "        # output.shape = (batch_size, trg_len-1, output_dim)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        # output.shape = ((trg_len-1) * batch_ize, output_dim)\n",
    "        # trg.shape = ((trg_len-1) * batch_size)\n",
    "        \n",
    "        # 計算 loss\n",
    "        loss = criterion(output, trg) # outputs by default are from logits; trg no need to do one-hot encoding\n",
    "        # 反向傳播，計算梯度\n",
    "        loss.backward()\n",
    "        # 做 regularization，使得整體梯度 norm 不超過 1，以防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # 更新優化器\n",
    "        optimizer.step_and_update_lr()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ofL2Y0iRq-sM"
   },
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, iterator, criterion):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            # src.shape = (src_len, batch_size)\n",
    "            \n",
    "            trg = batch.trg\n",
    "            # trg.shape = (trg_len, batch_size)\n",
    "            \n",
    "            output, _ = model(src, trg[:,:-1]) # turn off teacher forcing\n",
    "            # output.shape = (trg_len, batch_size, output_dim)\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            # output.shape = ((trg_len-1) * batch_ize, output_dim)\n",
    "            # trg.shape = ((trg_len-1) * batch_size)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "LIDSwaOYq-uQ"
   },
   "outputs": [],
   "source": [
    "# 計算跑一個 Epoch 的時間\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    \n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "# 將 loss vs. Epoch 畫出來\n",
    "def showPlot(tr_points, va_points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(tr_points, label='train loss')\n",
    "    plt.plot(va_points, label='validation loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "qoI4hMTSq-0h"
   },
   "outputs": [],
   "source": [
    "def train(model, train_iterator, valid_iterator, optimizer, criterion):\n",
    "    best_valid_loss = float('inf')\n",
    "    plot_tr_loss = []\n",
    "    plot_va_loss = []\n",
    "    for epoch in range(N_EPOCHS):\n",
    "    \n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate_epoch(model, valid_iterator, criterion)\n",
    "        plot_tr_loss.append(train_loss)\n",
    "        plot_va_loss.append(valid_loss)\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            # 儲存模型 (只存權重)\n",
    "            torch.save(model.state_dict(), 'SavedModel/tr-model_0720.pt')\n",
    "    \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        # PPL 是 perplexity 的縮寫，基本上就是 cross-entropy 指數化；其值越小越好 (minimize probability likelyhood)\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    showPlot(plot_tr_loss, plot_va_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "D7M1IJOrq6rG",
    "outputId": "76845460-a0af-4c72-9644-cf97100e7591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 863.007\n",
      "\t Val. Loss: 6.761 |  Val. PPL: 863.076\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 863.007\n",
      "\t Val. Loss: 6.760 |  Val. PPL: 863.074\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 863.020\n",
      "\t Val. Loss: 6.760 |  Val. PPL: 863.071\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 863.005\n",
      "\t Val. Loss: 6.760 |  Val. PPL: 863.066\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 863.003\n",
      "\t Val. Loss: 6.760 |  Val. PPL: 863.059\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 862.988\n",
      "\t Val. Loss: 6.760 |  Val. PPL: 863.052\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 862.993\n",
      "\t Val. Loss: 6.760 |  Val. PPL: 863.042\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 862.977\n",
      "\t Val. Loss: 6.760 |  Val. PPL: 863.031\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 862.976\n",
      "\t Val. Loss: 6.760 |  Val. PPL: 863.020\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 862.959\n",
      "\t Val. Loss: 6.760 |  Val. PPL: 863.006\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 862.954\n",
      "\t Val. Loss: 6.760 |  Val. PPL: 862.992\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 862.946\n",
      "\t Val. Loss: 6.760 |  Val. PPL: 862.975\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 862.939\n",
      "\t Val. Loss: 6.760 |  Val. PPL: 862.958\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 862.932\n",
      "\t Val. Loss: 6.760 |  Val. PPL: 862.939\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 6.760 | Train PPL: 862.916\n",
      "\t Val. Loss: 6.760 |  Val. PPL: 862.918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA3kUZzVrkhA"
   },
   "source": [
    "# 推論: Seq2seq 德翻英翻譯機"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "yO02ygIjrZaO"
   },
   "outputs": [],
   "source": [
    "def inference(sentence, src_field, trg_field, model, device, max_len=100):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # 首先將想翻譯的句子轉成 tensor \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de_core_news_sm')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "    \n",
    "    # 句子頭尾加上 <BOS>, <EOS>\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    # tokens 透過字典轉成整數  \n",
    "    src_indices = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n",
    "    # src_tensor.shape = (batch_size, src_len)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.encoder(src_tensor, src_mask) \n",
    "        \n",
    "    trg_indices = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "    \n",
    "    # To record attention weights\n",
    "    #attn_plot = np.zeros((max_len, len(tokens)))\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device)\n",
    "        #print(trg_tensor.shape)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        #print(trg_mask.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention_weights = model.decoder(trg_tensor, encoder_outputs, trg_mask, src_mask) \n",
    "            # output.shape = (batch_size, trg_len, output_dim)\n",
    "            # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:, -1].item()\n",
    "        trg_indices.append(pred_token)\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    # 預測出來的'整數們'轉回 tokens\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indices]\n",
    "    attention = attention_weights.squeeze(0).detach().cpu().numpy()\n",
    "    # attention.shape = (n_heads, trg_len, srg_len)\n",
    "    \n",
    "    # 只回傳除了 <BOS> 以外的 tokens與attention weights\n",
    "    return trg_tokens[1:], attention         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhg0GZuuPgBh"
   },
   "source": [
    "* 舉個例子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['中', '新', '网', '3', '月', '4', '日', '电', '', '', '国', '台', '办', '发', '言', '人', '朱', '凤', '莲', '3', '月', '4', '日', '表', '示', '，', '由', '于', '民', '进', '党', '当', '局', '一', '再', '阻', '挠', '，', '1148', '名', '急', '需', '返', '乡', '的', '滞', '鄂', '台', '胞', '迄', '今', '无', '法', '回', '家', '。', '苏', '贞', '昌', '日', '前', '又', '公', '开', '散', '布', '“', '苏', '式', '谎', '言', '”', '，', '继', '续', '罔', '顾', '事', '实', '、', '颠', '倒', '黑', '白', '，', '谎', '称', '“', '卡', '关', '就', '卡', '在', '大', '陆', '”', '，', '“', '真', '不', '知', '人', '间', '还', '有', '羞', '耻', '二', '字', '。', '”', '朱', '凤', '莲', '说', '，', '疫', '情', '发', '生', '以', '来', '，', '大', '陆', '方', '面', '一', '方', '面', '全', '力', '照', '顾', '在', '大', '陆', '台', '胞', '的', '生', '活', '和', '疫', '情', '防', '控', '需', '要', '，', '另', '一', '方', '面', '充', '分', '考', '虑', '滞', '鄂', '台', '胞', '的', '实', '际', '需', '求', '和', '回', '家', '心', '愿', '，', '积', '极', '安', '排', '东', '航', '于', '2', '月', '3', '日', '运', '送', '首', '批', '247', '名', '台', '胞', '返', '回', '台', '湾', '，', '并', '于', '2', '月', '5', '日', '和', '此', '后', '多', '次', '提', '出', '尽', '快', '运', '送', '其', '他', '提', '出', '返', '乡', '要', '求', '台', '胞', '的', '合', '理', '安', '排', '，', '包', '括', '提', '出', '由', '两', '岸', '航', '空', '公', '司', '共', '同', '执', '飞', '临', '时', '航', '班']\n",
      "trg = ['朱', '凤', '莲', '表', '示', '']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 0\n",
    "src = vars(test_data.examples[example_idx])['src']\n",
    "trg = vars(test_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1626972115820,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "SMIrXon7rh0Y",
    "outputId": "54edaa0b-6a65-48de-f03f-797e1e6c5c69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted trg = ['功', '件', '兴', '韩', '驻', '韩', '驻', '韩', '驻', '韩', '驻', '韩', '驻', '韩', '驻', '韩', '驻', '韩', '驻', '韩', '会', '韩', '驻', '韩', '驻', '韩', '驻', '韩', '驻', '韩', '驻', '韩', '驻', '韩', '驻', '韩', '驻', '韩', '驻', '韩', '驻', '韩', '晶', '依', '二', '视', '紧', '及', '有', '二', '视', '紧', '信', '二', '视', '紧', '信', '二', '视', '紧', '信', '二', '视', '紧', '信', '二', '视', '紧', '信', '二', '视', '紧', '信', '二', '视', '紧', '信', '二', '视', '紧', '信', '二', '视', '紧', '信', '二', '视', '紧', '信', '二', '视', '紧', '信', '二', '素', '镜', '它', '年', '回', '菲']\n"
     ]
    }
   ],
   "source": [
    "translation, attention = inference(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "C-YbwWWXrh5l"
   },
   "outputs": [],
   "source": [
    "# def plot_attention(attention, sentence, predicted_sentence, n_heads=8, n_rows=4, n_cols=2):\n",
    "#     assert n_rows * n_cols == n_heads\n",
    "    \n",
    "#     fig = plt.figure(figsize=(15,25))\n",
    "#     #print(attention.shape)\n",
    "#     for i in range(n_heads):\n",
    "#         ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "#         _attention = attention[i]\n",
    "\n",
    "#         cax = ax.matshow(_attention, cmap='viridis')\n",
    "\n",
    "#         ax.tick_params(labelsize=12)\n",
    "#         x_loc = [i for i in range(len(sentence)+2)]\n",
    "#         ax.set_xticks(x_loc)\n",
    "#         ax.set_xticklabels(['<bos>']+[t.lower() for t in sentence]+['<eos>'], rotation=90)\n",
    "#         y_loc = [j for j in range(len(predicted_sentence))]\n",
    "#         ax.set_yticks(y_loc)\n",
    "#         ax.set_yticklabels(predicted_sentence)\n",
    "\n",
    "#     plt.show()\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1739,
     "status": "ok",
     "timestamp": 1626972222521,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "Wld8beUGrZfT",
    "outputId": "b0a8b39c-ed65-4629-b73a-1f4e9b01dfab"
   },
   "outputs": [],
   "source": [
    "# plot_attention(attention, src, translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJAlQCSTqb9A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer_v1_colab_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
