{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3409fd25-5092-4172-b8e7-e14aae41330d",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853b0745-ef0a-4f4c-9f7e-cbf5b6c1722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "from torchtext.legacy import data\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "TRAIN_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\train.src\"\n",
    "TRAIN_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\train.tgt\"\n",
    "VALID_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\valid.src\"\n",
    "VALID_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\valid.tgt\"\n",
    "TEST_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\test.src\"\n",
    "TEST_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\test.tgt\"\n",
    "\n",
    "SEED = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4334a21f-0e89-40f2-a4ee-b8dc196d8d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148318 148318 20394 20394 16689 16688\n",
      "非常时期，行非常之策。抗疫期间，中央银行频繁释放流动性，有效对冲了疫情带来的负面影响，但也产生了新的潜在风险。3月末，广义货币（M2）同比增长10.1%，增速分别上年同期高1.5个百分点；本外币贷款余额同比增长12.3%，一季度人民币贷款增加7.1万亿元，同比多增1.29万亿元。3月末，社会融资规模存量同比增长11.5%，增速比上年末高0.8个百分点。一季度，社融规模增加11.08万亿元，同比多2.47万亿元。这些资金引发了人们对房地产强劲反弹的担忧。一季度，为防止企业资金链断裂，金融机构提供的短期贷款新增了2.3万亿元，同比多增1.25万亿元；支持企业复工复产，中长期贷款新增了3.04万亿元，多增4766亿元。住户贷款总量同比少增了约6000亿元，这是受疫情影响，居民消费和购房大幅减少所致。但是，3月份住户贷款明显好转，新增贷款9865亿元，同比多增961亿元，比2月大幅多增。一季度，全国房地产开发投资同比下降7.7%，降幅比1-2月份收窄8.6个百分点。其中，住宅投资下降7.2%，降幅收窄8.8个百分点。商品房销售面积下降26.3%，销售额下降24.7%，降幅分别比1-2月收窄13.6和11.2个百分点。这些都说明，随着经济恢复正常化，房地产也在加快恢复。当前流动性比较充裕，迫切需要引导资金合理流动。由于存在“复工不达产”的现象，疫情导致供应链、产业链受到一定冲击，企业生产订单减少，企业增加资金投入的风险加大；加之疫情全球蔓延、外部形势仍不明朗，未来外贸形势严峻，使得资金流向实体经济受阻。受预期影响，金融市场动荡加剧，不会成为吸纳资金的很好去处。疫情期间，各地暂停房屋建设和销售，这些“延后性需求”可能会在疫情后爆发出来，加之前期疫情导致“供给不足”，房屋供求矛盾将变得更加突出。常言道：“按下葫芦浮起瓢”，前期投放的资金是否会趁机流入房地产市场兴风作浪？确实令人担忧。面对当下异常复杂的国内外经济形势，中央再次强调“要坚持房子是用来住的、不是用来炒的定位，促进房地产市场平稳健康发展”，表明中央始终保持着清醒头脑和战略定力，及时提示要防范房地产炒作风险。（文丨徐洪才中国政策科学研究会经济政策委员会副主任欧美同学会留美分会副会长） （原题为《热评丨重申“房住不炒”具有重要现实意义》）(本文来自澎湃新闻，更多原创资讯请下载“澎湃新闻”APP) \n",
      "\n",
      " 中央再次强调“要坚持房子是用来住的、不是用来炒的定位，促进房地产市场平稳健康发展”，表明中央始终保持着清醒头脑和战略定力，及时提示要防范房地产炒作风险。\n"
     ]
    }
   ],
   "source": [
    "train_x_list = []\n",
    "train_y_list = []\n",
    "valid_x_list = []\n",
    "valid_y_list = []\n",
    "test_x_list = []\n",
    "test_y_list = []\n",
    "\n",
    "file_train_x = codecs.open(TRAIN_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_train_y = codecs.open(TRAIN_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_valid_x = codecs.open(VALID_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_valid_y = codecs.open(VALID_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_test_x = codecs.open(TEST_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_test_y = codecs.open(TEST_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "\n",
    "# create list for training, validation, testing set\n",
    "while True:\n",
    "    line = file_train_x.readline()\n",
    "    train_x_list.append(line.replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "    \n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "while True:\n",
    "    line = file_train_y.readline()\n",
    "    train_y_list.append(line.replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "    \n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "while True:\n",
    "    line = file_valid_x.readline()\n",
    "    valid_x_list.append(line.replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "    \n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "while True:\n",
    "    line = file_valid_y.readline()\n",
    "    valid_y_list.append(line.replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "    \n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "while True:\n",
    "    line = file_test_x.readline()\n",
    "    test_x_list.append(line.replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "    \n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "while True:\n",
    "    line = file_test_y.readline()\n",
    "    test_y_list.append(line.replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "    \n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "print(len(train_x_list), len(train_y_list), len(valid_x_list), len(valid_y_list), len(test_x_list), len(test_y_list))\n",
    "print(train_x_list[10], \"\\n\\n\", train_y_list[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b36d6e9-13a8-4173-acab-339de7a19cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenization function\n",
    "def tokenize(text):\n",
    "    return [char for char in text]\n",
    "\n",
    "SRC = Field(tokenize = tokenize, \n",
    "            init_token = '<bos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True,                 \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize, \n",
    "            init_token = '<bos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True,                 \n",
    "            batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12622806-ae73-4fe5-b770-a3f7155d208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148318it [01:02, 2371.14it/s]\n",
      "20394it [00:07, 2616.32it/s]\n",
      "100%|██████████| 16689/16689 [00:08<00:00, 1882.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# building customize dataset\n",
    "# reference: https://www.programmersought.com/article/7283735573/\n",
    "\n",
    "# get_dataset constructs and returns the examples and fields required by the Dataset\n",
    "def get_dataset(input_data, output_data, input_data_field, output_data_field, test=False):\n",
    "\t# idData pair training is useless during training, use None to specify its corresponding field\n",
    "    fields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"src\", input_data_field), (\"trg\", output_data_field)]       \n",
    "    examples = []\n",
    "\n",
    "    if test:\n",
    "        # If it is a test set, the label is not loaded\n",
    "        for text in tqdm(input_data):\n",
    "            examples.append(data.Example.fromlist([None, text, None], fields))\n",
    "    else:\n",
    "        for text, label in tqdm(zip(input_data, output_data)):\n",
    "            examples.append(data.Example.fromlist([None, text, label], fields))\n",
    "    return examples, fields\n",
    "\n",
    "# Get the examples and fields needed to build the Dataset\n",
    "train_examples, train_fields = get_dataset(train_x_list, train_y_list, SRC, TRG)\n",
    "valid_examples, valid_fields = get_dataset(valid_x_list, valid_y_list, SRC, TRG)\n",
    "test_examples, test_fields = get_dataset(test_x_list, test_y_list, SRC, None, test=True)\n",
    "\n",
    "#Build Dataset dataset\n",
    "train_data = data.Dataset(train_examples, train_fields)\n",
    "valid_data = data.Dataset(valid_examples, valid_fields)\n",
    "test_data = data.Dataset(test_examples, test_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34881da-1823-4fb8-b703-630aeed639fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 10008\n",
      "Unique tokens in target (en) vocabulary: 5504\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f8a66d3-6ae2-45d0-aa4a-34180f98b45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activate gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2f6235f-4f7c-42f0-89e8-45ca2401c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "# Use BucketIterator instead of the usual one to automatically deal with padding problem\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n",
    "                                                                      batch_size = BATCH_SIZE, \n",
    "                                                                      device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6754f6f4-b43b-43fa-9f25-8b7102df568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_layers, n_heads, pf_dim, dropout, learnable_pos= True, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        # input_dim = dictionary size of the src language\n",
    "        # nn.Embedding: 吃進一個 token (整數), 吐出一個 d_model 維度的向量\n",
    "        self.tok_embedding = nn.Embedding(input_dim, d_model)\n",
    "        if learnable_pos:\n",
    "            self.pos_embedding = nn.Embedding(max_length, d_model)\n",
    "        else:\n",
    "            self.pos_embedding = PositionalEncoding(max_length, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_seq_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_seq_len)\n",
    "        Output:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "        # pos.shape = (batch_size, src_len)\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        # src.shape = (batch_size, src_len, d_model)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "                \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf1424aa-9b24-44db-928e-b54d4fa0c1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.ff_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.self_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(d_model, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_seq_len)\n",
    "        Output:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"        \n",
    "        # self-attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "               \n",
    "        # positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        # dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff6fe5e-87bc-47ab-8c5d-97a6c7f7c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_length, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        # position.shape = (max_length, 1)\n",
    "        angle_rates = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # angle_rates.shape = (d_model/2,)\n",
    "        angle_rads = position * angle_rates\n",
    "        # angle_rads.shape = (max_length, d_model/2)\n",
    "        pe[:, 0::2] = torch.sin(angle_rads)   # 取偶數\n",
    "        pe[:, 1::2] = torch.cos(angle_rads)   # 取奇數\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # pe.shape = (1, max_length, d_model)\n",
    "        self.register_buffer('pe', pe) # register a constant tensor (not updated by optim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x.shape = (batch_size, src_len)\n",
    "        Output:\n",
    "            x.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"\n",
    "        x = torch.zeros(x.size(0), x.size(1), self.d_model) + self.pe[:, :x.size(1)]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ec770b7-0070-4d14-8b75-70fcf97a2cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 50])\n",
      "After positional encoding: torch.Size([2, 50, 512])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 500\n",
    "src_length = 50\n",
    "input = torch.arange(0, src_length).unsqueeze(0).repeat(2, 1)\n",
    "print(\"Input shape:\", input.shape)\n",
    "d_model = 512\n",
    "pos_encoding = PositionalEncoding(max_length, d_model)(input) # (batch_size, src_length, d_model)\n",
    "print(\"After positional encoding:\", pos_encoding.shape)\n",
    "\n",
    "# 考慮第一筆資料第 25 個 token\n",
    "inp = pos_encoding[0][25].numpy()\n",
    "\n",
    "distance_list = list()\n",
    "for i in range(50):\n",
    "    target_word = pos_encoding[0][i].numpy()\n",
    "    dot_product = np.dot(inp, target_word)\n",
    "    distance_list.append(dot_product)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(distance_list)\n",
    "plt.xticks(list(range(50)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9b9136b-ab04-4745-901f-ef5e33493989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 確保d_model可以被num_heads整除\n",
    "        assert d_model % self.n_heads == 0\n",
    "        self.head_dim = d_model // n_heads  # 將 d_model dimension 分成 n_heads 份\n",
    "        \n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            query.shape = (batch_size, query_len, d_model)\n",
    "            key.shape = (batch_size, key_len, d_model)\n",
    "            value.shape = (batch_size, value_len, d_model)\n",
    "            mask.shape = (batch_size, 1, query_len, key_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, query_len, d_model)\n",
    "            attention_weights.shape = (batch_size, n_heads, query_len, key_len)\n",
    "        \"\"\"   \n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # 通過全連結層形成 Q,K,V        \n",
    "        Q = self.wq(query)\n",
    "        K = self.wk(key)\n",
    "        V = self.wv(value)\n",
    "        # Q.shape = (batch_size, query_len, d_model)\n",
    "        # K.shape = (batch_size, key_len, d_model)\n",
    "        # V.shape = (batch_size, value_len, d_model)\n",
    "\n",
    "        # 將 q,k,v 等份切成 num_heads 份        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # Q.shape = (batch_size, n_heads, query_len, head_dim)\n",
    "        # K.shape = (batch_size, n_heads, key_len, head_dim)\n",
    "        # V.shape = (batch_size, n_heads, value_len, head_dim)\n",
    "\n",
    "        # 每個 heads 分別做 Q,K 內積        \n",
    "        scaled_attention_logits = torch.einsum(\"ijkl,ijml->ijkm\", [Q, K]) / self.scale\n",
    "        # scaled_attention_logits.shape = (batch_size, n_heads, query_len, key_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_attention_logits = scaled_attention_logits.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # 得到每個 heads 的 self-attention matrix\n",
    "        attention_weights = torch.softmax(scaled_attention_logits, dim=-1)       \n",
    "        # attention_weights.shape = (batch_size, n_heads, query_len, key_len)\n",
    "                \n",
    "        output = torch.matmul(self.dropout(attention_weights), V)\n",
    "        # output.shape = (batch_size, n_heads, query_len, head_dim)\n",
    "        \n",
    "        output = output.permute(0, 2, 1, 3).contiguous()\n",
    "        # output.shape = (batch_size, query_len, n_heads, head_dim)\n",
    "        \n",
    "        # concat 所有 heads\n",
    "        output = output.view(batch_size, -1, self.d_model)\n",
    "        # output.shape = (batch_size, query_len, d_model)\n",
    "        \n",
    "        output = self.fc_out(output)\n",
    "        # output.shape = (batch_size, query_len, d_model)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d9b2aaf-8456-4dbe-a6e0-9cf7a53d7128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, d_model, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(d_model, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x.shape = (batch_size, seq_len, d_model)\n",
    "        Output:\n",
    "            x.shape = (batch_size, seq_len, d_model)\n",
    "        \"\"\"   \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b93437d-8f99-49e8-8187-22dda49b1cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, d_model, n_layers, n_heads, pf_dim, dropout, max_length=100):\n",
    "        super().__init__()\n",
    "        # output_dim = dictionary size of the trg language\n",
    "        self.tok_embedding = nn.Embedding(output_dim, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_length, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            trg.shape = (batch_size, trg_len)\n",
    "            enc_src.shape = (batch_size, src_len, d_model)\n",
    "            trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, trg_len, output_dim)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"   \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(device)                    \n",
    "        # pos.shape = (batch_size, trg_len)\n",
    "            \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))        \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention_weights = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            # trg.shape = (batch_size, trg_len, d_model)\n",
    "            # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        # output.shape = (batch_size, trg_len, output_dim)    \n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4c02123-ad26-4bee-ba13-4afbc1c28fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.ff_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.self_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(d_model, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            trg.shape = (batch_size, trg_len, d_model)\n",
    "            enc_src.shape = (batch_size, src_len, d_model)\n",
    "            trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        Output:\n",
    "            trg.shape = (batch_size, trg_len, d_model)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"  \n",
    "        # self-attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))    \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "            \n",
    "        # encoder attention\n",
    "        _trg, attention_weights = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))            \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        return trg, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15019d4b-9d20-4b2b-88ed-370d0d4fdbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        # src.shape = (batch_size, src_len)\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        # trg.shape = (batch_size, trg_len)\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # trg_pad_mask.shape = (batch_size, 1, 1, trg_len)\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        # # 製造上三角矩陣\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()\n",
    "        # trg_sub_mask.shape = (trg_len, trg_len)\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask # AND operation: T & T = T, T & F = F, F & F = F\n",
    "        # trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_len)\n",
    "            trg.shape = (batch_size, trg_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, trg_len, output_dim)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"     \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        # src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        # trg_mask.shape = (batch size, 1, trg_len, trg_len)\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        # enc_src.shape = (batch_size, src_len, d_model)\n",
    "                \n",
    "        output, attention_weights = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        # output.shape = (batch_size, trg_len, output_dim)\n",
    "        # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6de281d-6e68-48bd-87ff-d23db366d033",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "D_MODEL = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "N_EPOCHS = 15\n",
    "CLIP = 1\n",
    "\n",
    "# 建立 encoder 和 decoder class\n",
    "enc = Encoder(INPUT_DIM, D_MODEL, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, D_MODEL, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT)\n",
    "\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token] # PAD_IDX=1\n",
    "\n",
    "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaed6381-31e6-432f-aa92-ebc709688714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (tok_embedding): Embedding(10008, 256)\n",
      "    (pos_embedding): Embedding(100, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (tok_embedding): Embedding(5504, 256)\n",
      "    (pos_embedding): Embedding(100, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=256, out_features=5504, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 9,390,464 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "        \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "print(model.apply(init_weights))\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e2ed6aa-b3b1-4871-b181-77cf76c832b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    '''\n",
    "    A wrapper class for optimizer \n",
    "    From https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Optim.py\n",
    "    '''\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4273f308-4b97-4675-8d66-0c6d9c9caea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "optimizer = ScheduledOptim(torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-09),\n",
    "                           d_model=D_MODEL, n_warmup_steps=4000)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f68b1fb0-a982-45e0-831d-0a3410bafaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip):  \n",
    "    # train mode\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        # src.shape = (batch_size, src_len)\n",
    "        \n",
    "        trg = batch.trg\n",
    "        # trg.shape = (batch_size, trg_len)\n",
    "        \n",
    "        # 梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(src, trg[:,:-1]) # full teacher forcing\n",
    "        # output.shape = (batch_size, trg_len-1, output_dim)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        # output.shape = ((trg_len-1) * batch_ize, output_dim)\n",
    "        # trg.shape = ((trg_len-1) * batch_size)\n",
    "        \n",
    "        # 計算 loss\n",
    "        loss = criterion(output, trg) # outputs by default are from logits; trg no need to do one-hot encoding\n",
    "        # 反向傳播，計算梯度\n",
    "        loss.backward()\n",
    "        # 做 regularization，使得整體梯度 norm 不超過 1，以防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # 更新優化器\n",
    "        optimizer.step_and_update_lr()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c55ea5d1-0d89-407f-9846-a341591ffeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, iterator, criterion):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            # src.shape = (src_len, batch_size)\n",
    "            \n",
    "            trg = batch.trg\n",
    "            # trg.shape = (trg_len, batch_size)\n",
    "            \n",
    "            output, _ = model(src, trg[:,:-1]) # turn off teacher forcing\n",
    "            # output.shape = (trg_len, batch_size, output_dim)\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            # output.shape = ((trg_len-1) * batch_ize, output_dim)\n",
    "            # trg.shape = ((trg_len-1) * batch_size)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "788efdfa-f938-495b-a0ff-64bba8ed9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算跑一個 Epoch 的時間\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    \n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "# 將 loss vs. Epoch 畫出來\n",
    "def showPlot(tr_points, va_points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(tr_points, label='train loss')\n",
    "    plt.plot(va_points, label='validation loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1c01cd5-569c-4a45-af24-290cd42228b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iterator, valid_iterator, optimizer, criterion):\n",
    "    best_valid_loss = float('inf')\n",
    "    plot_tr_loss = []\n",
    "    plot_va_loss = []\n",
    "    for epoch in range(N_EPOCHS):\n",
    "    \n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate_epoch(model, valid_iterator, criterion)\n",
    "        plot_tr_loss.append(train_loss)\n",
    "        plot_va_loss.append(valid_loss)\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            # 儲存模型 (只存權重)\n",
    "            torch.save(model.state_dict(), 'SavedModel/tr-model_0720.pt')\n",
    "    \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        # PPL 是 perplexity 的縮寫，基本上就是 cross-entropy 指數化；其值越小越好 (minimize probability likelyhood)\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    showPlot(plot_tr_loss, plot_va_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40e45693-1fb0-40f2-a004-1b943ea365e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'trg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d0f14dae6647>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-4fa4ee5fe4fc>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_iterator, valid_iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mplot_tr_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-d825988303c3>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# src.shape = (batch_size, src_len)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtrg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;31m# trg.shape = (batch_size, trg_len)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'trg'"
     ]
    }
   ],
   "source": [
    "train(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
