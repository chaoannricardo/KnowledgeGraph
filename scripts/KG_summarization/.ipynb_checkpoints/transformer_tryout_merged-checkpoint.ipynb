{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGypE_zDn2xe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10001it [00:00, 11078.03it/s]\n",
      "1001it [00:00, 12102.00it/s]\n",
      "101it [00:00, 12658.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 31s\n",
      "\tTrain Loss: 7.251 | Train PPL: 1410.184\n",
      "\t Val. Loss: 6.462 |  Val. PPL: 640.532\n",
      "Epoch: 02 | Time: 0m 31s\n",
      "\tTrain Loss: 6.096 | Train PPL: 443.935\n",
      "\t Val. Loss: 5.628 |  Val. PPL: 278.067\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Bug Fixing Reference:\n",
    "* TorchText usage example and complete code: https://www.programmersought.com/article/7283735573/\n",
    "* https://colab.research.google.com/github/bentrevett/pytorch-sentiment-analysis/blob/master/A%20-%20Using%20TorchText%20with%20Your%20Own%20Datasets.ipynb#scrollTo=9vXi5qGV9v7h\n",
    "* https://wangjiosw.github.io/2020/02/29/deep-learning/torchtext_use/\n",
    "* https://github.com/pytorch/text/issues/664\n",
    "* https://github.com/pytorch/text/issues/474\n",
    "* https://stackoverflow.com/questions/56010551/pytorch-embedding-index-out-of-range\n",
    "* https://discuss.pytorch.org/t/embeddings-index-out-of-range-error/12582\n",
    "* https://www.programmersought.com/article/97387644893/\n",
    "\n",
    "# Documentations:\n",
    "* TORCHTEXT.DATASETS: https://pytorch.org/text/stable/datasets.html#multi30k\n",
    "* SOURCE CODE FOR TORCHTEXT.DATA.FIEL: https://pytorch.org/text/_modules/torchtext/data/field.html\n",
    "* Embedding Error Index out of Range in self: https://discuss.pytorch.org/t/embedding-error-index-out-of-range-in-self/81550\n",
    "\n",
    "# Other Reference:\n",
    "* https://github.com/pytorch/text/issues/652\n",
    "\"\"\"\n",
    "\n",
    "import codecs\n",
    "import os\n",
    "import math, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import einops\n",
    "import gdown\n",
    "import spacy\n",
    "import random, math, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator, Dataset, Example\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 為了重複實驗方便，我們固定隨機種子\n",
    "SEED = 87\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "''' Data Source Configuration '''\n",
    "TRAIN_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\train.src\"\n",
    "TRAIN_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\train.tgt\"\n",
    "VALID_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\valid.src\"\n",
    "VALID_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\valid.tgt\"\n",
    "TEST_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\test.src\"\n",
    "TEST_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\test.tgt\"\n",
    "\n",
    "\n",
    "''' Data Configuration '''\n",
    "TRAIN_SAMPLE_NUM = 10000\n",
    "VALID_SAMPLE_NUM = 1000\n",
    "TEST_SAMPLE_NUM = 100\n",
    "WORD_MIN_FREQUENCY = 2\n",
    "# MAX_LENGTH_INPUT = int(np.max([len(data)+2 for data in train_x_list]))\n",
    "# MAX_LENGTH_OUTPUT = int(np.max([len(data)+2 for data in train_y_list]))\n",
    "MAX_LENGTH = 500\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "\n",
    "''' Model Parameters '''\n",
    "D_MODEL = 512\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "N_EPOCHS = 100\n",
    "CLIP = 1\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# 資料前處理\n",
    "def tokenize(text):\n",
    "    return [char for char in text.split(\" \")]\n",
    "\n",
    "## b. 添加`<BOS>`(begin of sequence),`<EOS>`(end of sequence) 在句子頭尾\n",
    "# 我們利用 torchtext 中的 [Field](https://torchtext.readthedocs.io/en/latest/data.html#fields) 物件來統一處理如何將斷詞後的文本轉為 torch tensors。\n",
    "\n",
    "''' Process Begins '''\n",
    "train_x_list = []\n",
    "train_y_list = []\n",
    "valid_x_list = []\n",
    "valid_y_list = []\n",
    "test_x_list = []\n",
    "test_y_list = []\n",
    "\n",
    "file_train_x = codecs.open(TRAIN_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_train_y = codecs.open(TRAIN_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_valid_x = codecs.open(VALID_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_valid_y = codecs.open(VALID_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_test_x = codecs.open(TEST_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_test_y = codecs.open(TEST_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "\n",
    "# create list for training, validation, testing set\n",
    "temp_index = 0\n",
    "\n",
    "while True or temp_index == TRAIN_SAMPLE_NUM:\n",
    "    line = file_train_x.readline()\n",
    "    train_x_list.append(line.replace(\"\\n\", \"\"))\n",
    "    line = file_train_y.readline()\n",
    "    train_y_list.append(line.replace(\"\\n\", \"\"))\n",
    "\n",
    "\n",
    "    if not line or temp_index == TRAIN_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "\n",
    "while True or temp_index == VALID_SAMPLE_NUM:\n",
    "    line = file_valid_x.readline()\n",
    "    valid_x_list.append(line.replace(\"\\n\", \"\"))\n",
    "    line = file_valid_y.readline()\n",
    "    valid_y_list.append(line.replace(\"\\n\", \"\"))\n",
    "\n",
    "    if not line or temp_index == VALID_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "\n",
    "while True or temp_index == TEST_SAMPLE_NUM:\n",
    "    line = file_test_x.readline()\n",
    "    test_x_list.append(line.replace(\"\\n\", \"\"))\n",
    "    line = file_test_y.readline()\n",
    "    test_y_list.append(line.replace(\"\\n\", \"\"))\n",
    "\n",
    "    if not line or temp_index == TEST_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "\n",
    "# convert samples to desired length\n",
    "for listItem in [train_x_list, train_y_list, valid_x_list, valid_y_list, test_x_list, test_y_list]:\n",
    "    for textIndex, textElement in enumerate(listItem):\n",
    "        listItem[textIndex] = textElement[:MAX_LENGTH]\n",
    "        \n",
    "\n",
    "# print(len(train_x_list), len(train_y_list), len(valid_x_list), len(valid_y_list), len(test_x_list),\n",
    "#           len(test_y_list))\n",
    "# print(train_x_list[:2], \"\\n\\n\", train_y_list[0:2])\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "SRC = Field(sequential=True,\n",
    "                  tokenize=tokenize,\n",
    "                  init_token = '<bos>', \n",
    "                  eos_token = '<eos>',\n",
    "                  lower=True,\n",
    "                  batch_first = True) \n",
    "\n",
    "TRG = Field(sequential=True,\n",
    "                  tokenize=tokenize,\n",
    "                  init_token = '<bos>', \n",
    "                  eos_token = '<eos>',\n",
    "                  lower=True,\n",
    "                  batch_first = True) \n",
    "\n",
    "def get_dataset(input_list, output_list, text_field, label_field, test=False):\n",
    "\t# idData pair training is useless during training, use None to specify its corresponding field\n",
    "    fields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"src\", text_field), (\"trg\", label_field)]       \n",
    "    examples = []\n",
    "            \n",
    "    for text, label in tqdm(zip(input_list, output_list)):\n",
    "            examples.append(Example.fromlist([None, text, label], fields))\n",
    "    \n",
    "    return examples, fields\n",
    "\n",
    "# Get the examples and fields needed to build the Dataset\n",
    "train_examples, train_fields = get_dataset(train_x_list, train_y_list, SRC, TRG)\n",
    "valid_examples, valid_fields = get_dataset(valid_x_list, valid_y_list, SRC, TRG)\n",
    "test_examples, test_fields = get_dataset(test_x_list, test_y_list, SRC, TRG, test=True)\n",
    "\n",
    "#Build Dataset dataset\n",
    "train_data = Dataset(train_examples, train_fields)\n",
    "valid_data = Dataset(valid_examples, valid_fields)\n",
    "test_data = Dataset(test_examples, test_fields)\n",
    "\n",
    "\n",
    "# print(train_data)\n",
    "# print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "# print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "# print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "\n",
    "# print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "# print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")\n",
    "\n",
    "# print(train_data[1].src)\n",
    "\n",
    "# Use BucketIterator instead of the usual one to automatically deal with padding problem\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n",
    "                                                                      batch_size = BATCH_SIZE, \n",
    "                                                                      device = device, # If you use gpu, here -1 is replaced with the GPU number.\n",
    "        sort_key=lambda x: len(x.src), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "        sort_within_batch=False,\n",
    "        repeat=False # we pass repeat=False because we want to wrap this Iterator layer.)\n",
    "                                                                     )\n",
    "\n",
    "\n",
    "'''\n",
    "建立 transformer 模型\n",
    "\n",
    "`Transformer` 分為編碼器 (Encoder) 與解碼器 (Decoder)，中間會有一條線 (矩陣) 連接，在 `Transformer` 中，最核心的架構為 `Multi-head Attention` 這個架構，後續有許多論文都針對這個架構提出新的改進方式。\n",
    "\n",
    "\n",
    "a. Encoder\n",
    "\n",
    "這裡先讓大家有一個概觀，了解 `Encoder` 的輸入與輸出分別是什麼:\n",
    "\n",
    "目前進行的機器翻譯任務是德文翻譯成英文：\n",
    "\n",
    "* `Encoder`: \n",
    "  * **輸入**: 德文句子 (`source sentence`) 進行 `word embedding` 之後形成詞向量矩陣 $X\\in R^{N\\times d_{model}}$ ($d_{model}$ 表示詞向量維度)，最主要的目的是將 `source sentence` 作為 `Query,Key,Value(Q,K,V)` 進行 `self-attention`。\n",
    "  * **輸出**: `hidden representation` (矩陣)，維度與轉為 word embedding 的輸入一樣 ($\\in R^{N\\times d_{model}}$)。\n",
    "\n",
    "* `Source mask`\n",
    "  \n",
    "#   在前處理中，需要把輸入的每個 batch 的句子做 padding 變成統一長度，主要是希望計算 loss 時不被算進去，padding 的地方也不希望模型注意到\n",
    "\n",
    "'''\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_layers, n_heads, pf_dim, dropout, learnable_pos= True, max_length=500):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        # input_dim = dictionary size of the src language\n",
    "        # nn.Embedding: 吃進一個 token (整數), 吐出一個 d_model 維度的向量\n",
    "        self.tok_embedding = nn.Embedding(input_dim, d_model)\n",
    "        if learnable_pos:\n",
    "            self.pos_embedding = nn.Embedding(max_length, d_model)\n",
    "        else:\n",
    "            self.pos_embedding = PositionalEncoding(max_length, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_seq_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_seq_len)\n",
    "        Output:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "#         print(src.shape)\n",
    "#         print(src_mask.shape)\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "        # pos.shape = (batch_size, src_len)\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        # src.shape = (batch_size, src_len, d_model)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "                \n",
    "        return src\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.ff_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.self_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(d_model, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_seq_len)\n",
    "        Output:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"        \n",
    "        # self-attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "               \n",
    "        # positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        # dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        return src\n",
    "\n",
    "'''\n",
    "## b. Positional Encoding\n",
    "\n",
    "# Word Embedding 所表達的是所有詞向量之間的相似關係，而 Transformer 的做法是透過內積解決RNN的長距離依賴問題 (long-range dependenices) ，但是 Transformer 這樣做卻沒有考慮到句子中的詞先後順序關係，透過 Positional Encoding ，讓詞向量之間不只因為 word embedding 語義關係而靠近，也可以因為詞之間的位置相互靠近而靠近。Positional Encoding 的公式如下：\n",
    "\n",
    "# $$\n",
    "# PE_{(pos,2i)} = \\sin(pos/10000^{\\frac{2i}{d_{model}}}) \\\\\n",
    "# PE_{(pos,2i+1)} = \\cos(pos/10000^{\\frac{2i}{d_{model}}})\n",
    "# $$\n",
    "'''\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_length, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        # position.shape = (max_length, 1)\n",
    "        angle_rates = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # angle_rates.shape = (d_model/2,)\n",
    "        angle_rads = position * angle_rates\n",
    "        # angle_rads.shape = (max_length, d_model/2)\n",
    "        pe[:, 0::2] = torch.sin(angle_rads)   # 取偶數\n",
    "        pe[:, 1::2] = torch.cos(angle_rads)   # 取奇數\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # pe.shape = (1, max_length, d_model)\n",
    "        self.register_buffer('pe', pe) # register a constant tensor (not updated by optim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x.shape = (batch_size, src_len)\n",
    "        Output:\n",
    "            x.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"\n",
    "        x = torch.zeros(x.size(0), x.size(1), self.d_model) + self.pe[:, :x.size(1)]\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 舉個例子來理解一下 positional encoding (non-learnable)。此例拿第 25 個 token 的 positional encoding 來跟其餘 50 個字 (包含自己) 的 positional encoding 計算內積 (`np.dot`)，能夠發現越靠近 token 25 的內積越大，反之，越遠則內積越小。\n",
    "\n",
    "# 可以看到確實發現越靠近 token 25 的內積越大，反之，越遠則內積越小。\n",
    "\n",
    "# ## c. Multi-head self-attention\n",
    "\n",
    "\n",
    "\n",
    "# 架構如上圖，`Self-attention` 又稱自注意機制，在論文中被稱為 `Scaled Dot-Product Attention`\b，這個架構是 `Transformer` 的核心架構，在這模型會學到句子中詞與詞之間的關係。考慮一個句子，要關注的詞稱為 `Query`，被關注的詞則稱為 `Key`。通常都是一個 `Query` 去關注多個 `Key`。Self-attention 基本上有以下幾個步驟：\n",
    "\n",
    "# 1. `Word embedding`: 假設句子斷詞後(n 個詞)，每個詞已轉為詞向量且詞向量維度為 $d_k$，則句子就能表示為 $X\\in R^{n\\times d_k}$。\n",
    "# 2. `Q,K,V`: 將 $X$ 分別通過三個不同的全連接層 $W_Q,W_K,W_V$ 得到 $Q,K,V\\in R^{n\\times d_k}$。\n",
    "# 3. `Self-attention`: 將 $Q$ 和 $K$ 做矩陣相乘 ($QK^\\top$) 得到注意力矩陣，如同下圖中的方陣一樣，每個詞之間都會有一個注意力的值 $M_{i,j}$ (在此教學最後我們會視覺化這個注意力矩陣)，最後再將注意力矩陣與 $V$ 做矩陣相乘，得到輸出 $\\in R^{n\\times d_k}$。\n",
    "\n",
    "# $$\n",
    "# \\mathrm{Attention}(Q,K,V) = \\mathrm{Softmax}(\\frac{QK^\\top}{\\sqrt{d_k}})V\n",
    "# $$\n",
    "\n",
    "# `Transformer` 的精髓就在此，使用矩陣相乘讓詞與詞之間計算注意力，而矩陣相乘本質上就是多個內積，所以 `self-attention` 就是使用內積來實現注意力機制。\n",
    "\n",
    "# * Multi-head\n",
    "\n",
    "#   流程: 將 `Q,K,V` 分成 num_heads 份，各自做 self-attention，然後再 Concat ，接著通過 dense 輸出。\n",
    "\n",
    "#   分成 num_heads 的優點最主要是希望讓每個 head 各自注意到序列中不同的地方，而且切分成較小的矩陣還能加速訓練過程。\n",
    "\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 確保d_model可以被num_heads整除\n",
    "        assert d_model % self.n_heads == 0\n",
    "        self.head_dim = d_model // n_heads  # 將 d_model dimension 分成 n_heads 份\n",
    "        \n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            query.shape = (batch_size, query_len, d_model)\n",
    "            key.shape = (batch_size, key_len, d_model)\n",
    "            value.shape = (batch_size, value_len, d_model)\n",
    "            mask.shape = (batch_size, 1, query_len, key_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, query_len, d_model)\n",
    "            attention_weights.shape = (batch_size, n_heads, query_len, key_len)\n",
    "        \"\"\"   \n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # 通過全連結層形成 Q,K,V        \n",
    "        Q = self.wq(query)\n",
    "        K = self.wk(key)\n",
    "        V = self.wv(value)\n",
    "        # Q.shape = (batch_size, query_len, d_model)\n",
    "        # K.shape = (batch_size, key_len, d_model)\n",
    "        # V.shape = (batch_size, value_len, d_model)\n",
    "\n",
    "        # 將 q,k,v 等份切成 num_heads 份        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # Q.shape = (batch_size, n_heads, query_len, head_dim)\n",
    "        # K.shape = (batch_size, n_heads, key_len, head_dim)\n",
    "        # V.shape = (batch_size, n_heads, value_len, head_dim)\n",
    "\n",
    "        # 每個 heads 分別做 Q,K 內積        \n",
    "        scaled_attention_logits = torch.einsum(\"ijkl,ijml->ijkm\", [Q, K]) / self.scale\n",
    "        # scaled_attention_logits.shape = (batch_size, n_heads, query_len, key_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_attention_logits = scaled_attention_logits.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # 得到每個 heads 的 self-attention matrix\n",
    "        attention_weights = torch.softmax(scaled_attention_logits, dim=-1)       \n",
    "        # attention_weights.shape = (batch_size, n_heads, query_len, key_len)\n",
    "                \n",
    "        output = torch.matmul(self.dropout(attention_weights), V)\n",
    "        # output.shape = (batch_size, n_heads, query_len, head_dim)\n",
    "        \n",
    "        output = output.permute(0, 2, 1, 3).contiguous()\n",
    "        # output.shape = (batch_size, query_len, n_heads, head_dim)\n",
    "        \n",
    "        # concat 所有 heads\n",
    "        output = output.view(batch_size, -1, self.d_model)\n",
    "        # output.shape = (batch_size, query_len, d_model)\n",
    "        \n",
    "        output = self.fc_out(output)\n",
    "        # output.shape = (batch_size, query_len, d_model)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "## d. Point-wise feed forward network\n",
    "\n",
    "# 通過兩個全連接層：\n",
    "\n",
    "# * 第一層 $W_1\\in R^{pf_{dim}\\times d_{model}}$ : 將 $d_{model}$ 變為 $pf_{dim}$。\n",
    "\n",
    "# * 第二層 $W_2\\in R^{d_{model}\\times pf_{dim}}$ : 將 $pf_{dim}$ 變回 $d_{model}$。\n",
    "\n",
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, d_model, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(d_model, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x.shape = (batch_size, seq_len, d_model)\n",
    "        Output:\n",
    "            x.shape = (batch_size, seq_len, d_model)\n",
    "        \"\"\"   \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "## e. Decoder\n",
    "\n",
    "# * `Decoder` 的輸入與輸出分別是什麼:\n",
    "\n",
    "#   * **輸入**: Encoder output 與英文句子 (`target sentence`)，一開始在預測時是不會有真實答案的，所以會在中文句子開頭加上 `<BOS>` token 形成 `target sentence`(`shifted right`)，後來使用 `Auto regressive` 的方式進行預測，每當預測完一個詞，就將該預測詞與原始輸入詞接在一起再輸入給模型直到預測出 `<EOS>` 為止。\n",
    "#   * **輸出**: 英文句子 (`target sentence`)，使用 `Auto regressive` 方式來預測，例如: 輸入 `<BOS>`，預測 $p_1$；輸入 $p_1$，預測 $p_2$，直到預測出 `<EOS>` 為止。\n",
    "\n",
    "# * `Masked multi-head attention`\n",
    "#   * Masked self-attention，後面需要觀察的 attention weight matrix\n",
    "#   * 使用 trg_mask，讓 decoder 輸入只能往前看\n",
    "\n",
    "#     source mask 前面談過，這裡來談談 target mask：\n",
    "    \n",
    "#     這個在 (masked) self-attention 會使用到，簡單來說就是不讓當前的字去注意到之後字，如下圖。每個詞只能夠往前注意，會這麼做的原因是因為 `Transformer` 在訓練時是一次將正確答案 (整個句子) 輸入給 Decoder，因為預測時是一個詞一個詞依序往後預測，所以不能讓模型先看到答案後面的詞。\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, d_model, n_layers, n_heads, pf_dim, dropout, max_length=500):\n",
    "        super().__init__()\n",
    "        # output_dim = dictionary size of the trg language\n",
    "        self.tok_embedding = nn.Embedding(output_dim, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_length, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            trg.shape = (batch_size, trg_len)\n",
    "            enc_src.shape = (batch_size, src_len, d_model)\n",
    "            trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, trg_len, output_dim)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"   \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(device)                    \n",
    "        # pos.shape = (batch_size, trg_len)\n",
    "            \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))        \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention_weights = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            # trg.shape = (batch_size, trg_len, d_model)\n",
    "            # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        # output.shape = (batch_size, trg_len, output_dim)    \n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.ff_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.self_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(d_model, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            trg.shape = (batch_size, trg_len, d_model)\n",
    "            enc_src.shape = (batch_size, src_len, d_model)\n",
    "            trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        Output:\n",
    "            trg.shape = (batch_size, trg_len, d_model)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"  \n",
    "        # self-attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))    \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "            \n",
    "        # encoder attention\n",
    "        _trg, attention_weights = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))            \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        return trg, attention_weights\n",
    "\n",
    "'''\n",
    "g. Transformer\n",
    "編碼器 (Encoder) 與解碼器 (Decoder) 結合起來成為 Transformer：\n",
    "\n",
    "* **輸入**: 德文句子 (`target sentence`) 與英文句子 (`target sentence`)。\n",
    "* **輸出**: word embedded 的輸出句子以及每個 head 英文對德文句子的注意力矩陣。\n",
    "'''\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        # src.shape = (batch_size, src_len)\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        # trg.shape = (batch_size, trg_len)\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # trg_pad_mask.shape = (batch_size, 1, 1, trg_len)\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        # # 製造上三角矩陣\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()\n",
    "        # trg_sub_mask.shape = (trg_len, trg_len)\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask # AND operation: T & T = T, T & F = F, F & F = F\n",
    "        # trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_len)\n",
    "            trg.shape = (batch_size, trg_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, trg_len, output_dim)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"     \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        # src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        # trg_mask.shape = (batch size, 1, trg_len, trg_len)\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        # enc_src.shape = (batch_size, src_len, d_model)\n",
    "                \n",
    "        output, attention_weights = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        # output.shape = (batch_size, trg_len, output_dim)\n",
    "        # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "# 建立 encoder 和 decoder class\n",
    "enc = Encoder(INPUT_DIM, D_MODEL, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, MAX_LENGTH)\n",
    "dec = Decoder(OUTPUT_DIM, D_MODEL, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, MAX_LENGTH)\n",
    "\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token] # PAD_IDX=1\n",
    "\n",
    "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX).to(device)\n",
    "\n",
    "# Model summary\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "        \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "# print(model.apply(init_weights))\n",
    "# print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "'''\n",
    "b. 選擇優化器與損失函數\n",
    "這邊使用分類任務的損失函數`CrossEntropyLoss`。但部分句子為因為 `padding` 而有許多的 `1`，但是那並不是真實的標籤，所以必須忽略 `padding` 位置的損失。\n",
    "'''\n",
    "\n",
    "class ScheduledOptim():\n",
    "    '''\n",
    "    A wrapper class for optimizer \n",
    "    From https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Optim.py\n",
    "    '''\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "optimizer = ScheduledOptim(torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-09),\n",
    "                           d_model=D_MODEL, n_warmup_steps=4000)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) \n",
    "\n",
    "## c. 定義訓練與驗證迴圈\n",
    "\n",
    "def train_epoch(model, iterator, optimizer, criterion, clip):  \n",
    "    # train mode\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        # src.shape = (batch_size, src_len)\n",
    "        \n",
    "        trg = batch.trg\n",
    "        # trg.shape = (batch_size, trg_len)\n",
    "        \n",
    "        # 梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(src, trg[:,:-1]) # full teacher forcing\n",
    "        # output.shape = (batch_size, trg_len-1, output_dim)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        # output.shape = ((trg_len-1) * batch_ize, output_dim)\n",
    "        # trg.shape = ((trg_len-1) * batch_size)\n",
    "        \n",
    "        # 計算 loss\n",
    "        loss = criterion(output, trg) # outputs by default are from logits; trg no need to do one-hot encoding\n",
    "        # 反向傳播，計算梯度\n",
    "        loss.backward()\n",
    "        # 做 regularization，使得整體梯度 norm 不超過 1，以防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # 更新優化器\n",
    "        optimizer.step_and_update_lr()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate_epoch(model, iterator, criterion):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            # src.shape = (src_len, batch_size)\n",
    "            \n",
    "            trg = batch.trg\n",
    "            # trg.shape = (trg_len, batch_size)\n",
    "            \n",
    "            output, _ = model(src, trg[:,:-1]) # turn off teacher forcing\n",
    "            # output.shape = (trg_len, batch_size, output_dim)\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            # output.shape = ((trg_len-1) * batch_ize, output_dim)\n",
    "            # trg.shape = ((trg_len-1) * batch_size)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# 計算跑一個 Epoch 的時間\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    \n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "# 將 loss vs. Epoch 畫出來\n",
    "def showPlot(tr_points, va_points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(tr_points, label='train loss')\n",
    "    plt.plot(va_points, label='validation loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "\n",
    "def train(model, train_iterator, valid_iterator, optimizer, criterion):\n",
    "    best_valid_loss = float('inf')\n",
    "    plot_tr_loss = []\n",
    "    plot_va_loss = []\n",
    "    for epoch in range(N_EPOCHS):\n",
    "    \n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate_epoch(model, valid_iterator, criterion)\n",
    "        plot_tr_loss.append(train_loss)\n",
    "        plot_va_loss.append(valid_loss)\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            # 儲存模型 (只存權重)\n",
    "            torch.save(model.state_dict(), 'SavedModel/tr-model_0720.pt')\n",
    "    \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        # PPL 是 perplexity 的縮寫，基本上就是 cross-entropy 指數化；其值越小越好 (minimize probability likelyhood)\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    showPlot(plot_tr_loss, plot_va_loss)\n",
    "\n",
    "train(model, train_iterator, valid_iterator, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = 中新网3月4日电国台办发言人朱凤莲3月4日表示，由于民进党当局一再阻挠，1148名急需返乡的滞鄂台胞迄今无法回家。苏贞昌日前又公开散布“苏式谎言”，继续罔顾事实、颠倒黑白，谎称“卡关就卡在大陆”，“真不知人间还有羞耻二字。”朱凤莲说，疫情发生以来，大陆方面一方面全力照顾在大陆台胞的生活和疫情防控需要，另一方面充分考虑滞鄂台胞的实际需求和回家心愿，积极安排东航于2月3日运送首批247名台胞返回台湾，并于2月5日和此后多次提出尽快运送其他提出返乡要求台胞的合理安排，包括提出由两岸航空公司共同执飞临时航班\n",
      "trg = 朱凤莲表示，事实反复证明，民进党当局根本就不想让在湖北的台胞回家，滞鄂台胞返乡之路受阻，“卡关”就卡在民进党当局的这些政客手中。\n",
      "predicted trg = 陆续回复工，两台湾民警，两地方进入了相关人，称将继续提前提前提供更加利益。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 海外网9月11日消息，9月11日上午，国务院台湾事务办公室举行例行新闻发布会。国台办发言人马晓光就两岸政党交流、两岸军事互信和两岸文化往来等热点问题回答记者提问。有记者提问，随着台湾高中开学，根据民进党当局教育部门新课纲编写的历史教科书正式投入使用。岛内有舆论认为，这一新课纲处处“去中国化”，是在教育领域搞“台独”的体现，请问发言人有何评论？谢谢。马晓光表示，民进党当局上台以来，不断推进形形色色的“台独”活动，这次推出了充斥“去中国化”“台独”内容的新课纲，不仅荼毒台湾年轻一代，更进一步破坏两岸关\n",
      "trg = 马晓光表示，民进党当局上台以来，不断推进形形色色的“台独”活动，这次推出了充斥“去中国化”“台独”内容的新课纲，不仅荼毒台湾年轻一代，更进一步破坏两岸关系。\n",
      "predicted trg = 海军参谋长马祖党15日上午在台湾岛举行。他19届进行了台湾方案举和党当局党当局、校举举举举举举举举举揭话，这是台湾青年轻人利用激烈。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = “央视新闻”微信公号12月14日消息，今年是改革开放40周年。今年中国举办了四场主场外交活动，从博鳌亚洲论坛到首届中国国际进口博览会，几大主场外交活动始终贯穿着改革、开放、合作的主题。今年以来，习近平在多个场合还重点谈到开放。“中国开放的大门不会关闭，只会越开越大。”“人类社会发展的历史告诉我们，开放带来进步，封闭必然落后。”坚持对外开放是中国的基本国策，无论是在基层任职还是到中央工作，习近平一直积极践行开放发展理念，带领大家招商引资学技术，打开门户谋发展，将改革开放的大道越走越宽。改革开放初期：把\n",
      "trg = 坚持对外开放是中国的基本国策，无论是在基层任职还是到中央工作，习近平一直积极践行开放发展理念，带领大家招商引资学技术，打开门户谋发展，将改革开放的大道越走越宽。\n",
      "predicted trg = “为什么<unk>”，中国民主题对外开放证监会进行动经济论坛开放单，展。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 中新网客户端北京2月21日消息，21日凌晨，福州市公安局就赵宇“见义勇为反被刑拘”事件发布最新通报。通报显示，检方称，对赵某作出不起诉决定。对此，赵宇代理律师范辰接受中新网记者采访时表示，不予起诉不代表认定赵宇无罪。若是因调查证据不足而判定无罪，则将继续进行上诉，直到认定赵宇无罪。此外，赵宇一方或将申请国家赔偿。从“故意伤害”到正当防卫2月17日，在福州生活的21岁黑龙江小伙赵宇发微博求助称，自己见义勇为救助女邻居，却反被警方刑拘。事件一经曝出，引发网友广泛关注。据赵宇微博描述，2018年12月26日夜里\n",
      "trg = 赵宇代理律师范辰接受中新网记者采访时表示，不予起诉不代表认定赵宇无罪。若是因调查证据不足而判定无罪，则将继续进行上诉，直到认定赵宇无罪。\n",
      "predicted trg = 12月17日，官方微博发布的一起因斯因斯因斯因检：“登报警方调查”在广州某被害者称为在广州市公诉到“玩”涉嫌诈骗一起不实中，被害，被害者中将危害者的警方行。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 综合报道，美国总统特朗普当地时间7日离开爱尔兰，结束5天的访欧行程，期间他赴英国进行访问，纪念诺曼底登陆75周年。据报道，当天，特朗普在爱尔兰西部的香农机场(shannonairport)搭乘“空军一号”返回美国。这是他2017年1月就任总统以来，首度访问爱尔兰。 在对英国进行国事访问的不到72小时中，特朗普会见了英国女王伊丽莎白二世，和特蕾莎•梅共商英美贸易关系的未来，与脱欧派人士见面。当地时间5日，特朗普在英格兰南部港口城市朴茨茅斯(portsmouth)，参加诺曼底登陆75周年纪念活动。英女王伊丽莎白二世和英国首相特\n",
      "trg = 综合报道，美国总统特朗普当地时间7日离开爱尔兰，结束5天的访欧行程，期间他赴英国进行访问，纪念诺曼底登陆75周年。\n",
      "predicted trg = 当地时间1月6日，美国总统特朗普表示，他对伊朗普罗斯兰“这个人事情”将进行“阻”。在霍尔特朗普上都会上做个人进口”的愿意活动。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 《2018-2023年中国社交电商行业市场前景及投资机会研究报告》显示，社交电商作为一种基于社会化移动社交而迅速发展的新兴电子商务模式，自2013年出现后连续5年高速发展。80后和90后是我国移动社交网络发展的中坚力量，00后是移动社交网络界的新生代，而主打年轻一代的社交电商用户规模势必会随年轻人对社交网络、移动互联网使用率的增长而水涨船高。可以预见，未来社交电商还会有较大的发展空间和变数。但对于社交电商存在的问题及如何规范，《法制日报》记者采访了业内专家。社交电商身份不明虚假宣传涉嫌传销中国政法大学传播法\n",
      "trg = 社交电商属于电商毋庸置疑，但如果消费者从社交电商平台购买的产品涉及质量问题时，却无法享受无理由退货，这就侵犯了消费者权益。\n",
      "predicted trg = 如何进行动用户规模化社会的电子，设置平台实际空间也是电子产业的一个交行动驾驶题。如何交易行动算应用户的行动才流量子，以上投资移动才？<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 澎湃新闻记者2月6日上午从天津中医药大学第一附属医院以及阮士怡先生亲友处获悉，中国第二届国医大师阮士怡先生于2月5日上午因病去世，享年104岁。遵阮士怡先生本人遗愿丧事从简，不举行遗体告别仪式。据“天津中医一附院”官方微信公众号2014年10月刊文介绍：阮士怡教授，男，1917年2月生，河北省丰南县人，硕士研究生学历，天津中医药大学第一附属医院主任医师、教授、硕士生导师。阮士怡1946年6月从北京大学医学院毕业，系国家中医药管理局第一批中医药传承博士后合作导师，第五批全国老中医药专家学术经验继承工作指导老师\n",
      "trg = 记者从阮士怡先生亲友处获悉，中国第二届国医大师阮士怡先生于2月5日上午因病去世，享年104岁。遵阮士怡先生本人遗愿丧事从简，不举行遗体告别仪式。\n",
      "predicted trg = 我国兽医学院士研究所文学生于今年2月10日晚间中国家医院逝世，享年5月30日，享年左右臂（63），享年4日，享年83岁。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 狗年即将过去。作为十二生肖中“人类最好的朋友”，狗的历史其实就是人和狗伙伴关系的历史。狗的学名为“canislupusfamiliaris”，在灰狼的学名canislupus后，加上了意为“朋友”或“家庭成员”的拉丁文单词familiaris，就成了“家犬”，也就是我们平常所说的狗。从动物分类学上看，深受人们喜爱的熊猫，实际上从也是从狗形类动物演化而来的。狗属于食肉目的动物。狗的始祖是一种貌如小黄鼠狼、生活在四千万年前的细齿兽。食肉目动物从4000多万年前的始新世一直到现代都占据着优势，可以进一步分为三类，即狗形类、猫形类和鳍脚类。其\n",
      "trg = 大约15000年前，作为人类驯化的第一种动物，狗便开始了与人类共同狩猎、共同迁徙、共同揭开生命秘密的漫长旅程。\n",
      "predicted trg = “上去是人类的人类的口罩或者，类对家础上又称它们的经历史又称“古，仅是被撤的固然是被认知”。可是“梁食用纪念乘风”，还会想到这个<unk>理解。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 当地时间29日，泰国卫生部通报，新增143名新冠肺炎确诊病例和1名死亡病例。截止到当地时间29日上午，泰国累计确诊病例1388例，其中泰国籍1172例，非泰国籍216例。死亡病例累计7例。（原题为《泰国新增143例新冠肺炎确诊病例累计确诊1388例》）(本文来自澎湃新闻，更多原创资讯请下载“澎湃新闻”app)\n",
      "trg = 截止到当地时间29日上午，泰国累计确诊病例1388例，其中泰国籍1172例，非泰国籍216例。死亡病例累计7例。\n",
      "predicted trg = 泰国当地时间29日新增新冠肺炎确诊病例，泰国累计确诊<unk>例，其中1例死亡病例。此前确诊病例，泰国以<unk>例死亡病例。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 五粮液又曝出一起管理人员违纪事件。2月25日，四川省宜宾市纪委监委网站“廉洁宜宾”发布消息称，宜宾五粮液股份有限公司（000858.sz，五粮液）华东营销中心上海区域副经理陈成涉嫌严重违法，经市监委指定管辖，目前兴文县监委正对其进行检察调查。公开资料显示，陈成，男，1981年4月生，安徽省六安市人。2011年4月，陈成进入五粮液工作，至2012年2月任四川省宜宾五粮液集团有限公司华东营销中心浙江省湖州市城市经理；2018年1月至今，任宜宾五粮液股份有限公司华东营销中心上海区域副经理。近期，这已经不是第一起被通报的五\n",
      "trg = 澎湃新闻记者梳理“廉洁宜宾”发现，自2019年起，宜宾市纪委监委在2个月内已经通报了4起与五粮液相关的违纪调查。\n",
      "predicted trg = 上海市五川省委监委员会披露心一事，向广东省政厅长涉嫌严重违纪违法，目前正接受纪律审查。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 新华社北京10月15日电，国务院港澳事务办公室原副主任、党组成员，中央政府驻港联络办原副主任王凤超同志，因病于2019年9月28日在北京逝世，享年73岁。王凤超同志逝世后，中央有关领导同志以不同方式表示哀悼并向其亲属表示慰问。王凤超，1945年11月生于辽宁东港。1970年3月参加工作。1975年3月加入中国共产党。1981年9月起先后任中国社会科学院中国报刊史研究室副主任，新闻研究所副所长。1989年1月起先后任国务院港澳办公室一司、二司副司长，二司司长，1994年1月任国务院港澳事务办公室副主任、党组成员。1998年7月\n",
      "trg = 国务院港澳事务办公室原副主任、党组成员，中央政府驻港联络办原副主任王凤超同志，因病于2019年9月28日在北京逝世，享年73岁。\n",
      "predicted trg = 3月10日，北京航王世，享年3月必须因病医务院新任务，享年8月必须曾任务。据世，享年3月后将于2019年3月10日至21时长，因病医务。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = @央视新闻3月1日消息，2月28日上午10时左右，河北省张家口市察北管理区启奥能源管理服务有限公司的1台15蒸吨生物质锅炉发生上锅筒管孔区撕裂事故，造成3人死亡、7人受伤。目前伤者正在医院进行救治，事故原因正在调查。（原题为《河北张家口一公司锅炉发生事故，致3死7伤》）\n",
      "trg = 昨天上午10时左右，河北省张家口市察北管理区启奥能源管理服务有限公司的1台15蒸吨生物质锅炉发生上锅筒管孔区撕裂事故，造成3人死亡、7人受伤。\n",
      "predicted trg = 3月2日上午10时左右，在湖北省卫生健康委新闻记者发生事故现过程，造成2人死亡、母伤，其中3人受伤。3人被困难文件正在调查。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 最近，赵忠祥老师又上了一次头条，就因一只陪了他40年的铁锅。那么问题来了，铁锅烧菜到底能不能补铁？上海交通大学医学院附属同仁医院营养科主任张静表示，用铁锅炒菜的确可以增加一点菜肴中铁的含量。然而，这点微量的铁，并不能满足人体每日对铁的需求。更关键的是，铁锅中释出的铁是无机铁，本身就不容易被人体吸收。对普通人而言，只要膳食均衡，用什么锅做菜都行。如果你认为用铁锅更安心，那就继续用。但如果本身就已经贫血，想通过铁锅来改善，那根本不可能。“贫血最好的食物来源还是动物性的食品，比如红肉、猪肝等。”张静指出\n",
      "trg = 上海交通大学医学院附属同仁医院营养科主任张静表示，用铁锅炒菜的确可以增加一点菜肴中铁的含量。然而，这点微量的铁，并不能满足人体每日对铁的需求。\n",
      "predicted trg = 日本铁路及“食”字暖的食精神求，又来上海知识淡雅能够通用菌、制专业的方式。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 挂失补办身份证后仍被冒名注册公司，这些身份证来自哪里？发现“被老板”后如何“自证清白”，申请撤销登记？面对大量此类投诉举报，市场监管部门又该如何“破局”？澎湃新闻（www.thepaper.cn）梳理中国裁判文书网公开的此类案例和部分公开报道，发现身份证遗失补办后并不能阻止“被老板”情况发生，以“线下签”方式委托代理人注册登记公司，这种“人证不一致”的情况往往会造成虚假注册登记出现。在此过程中，来自身份证“黑市”的证件可能为不法分子提供注册所需要件。在非法购得身份证后，违法者再委托中介登记注册公司，干虚开\n",
      "trg = 澎湃新闻以“冒名注册公司”为关键词从中国裁判文书网检索到30起案例，其中有28起是将公司登记机关告上法庭，请求撤销登记。\n",
      "predicted trg = 今年“被告诉澎湃新闻提供获得、办理、办案，这些朋友圈等反映市“保证假诉”，已近两起来自己多新闻采取多是多是多新闻在发后仍然能去了获得的举报案。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 身处于自然之中，你是否曾经仔细欣赏过大自然？它包罗了世间的一切，而我们对它的真正意义上的了解，却不是那么多。在你观察不到的地方，蛙叫、蝉鸣、蜻蜓振翅……这些构成了静谧的夜晚中最悦耳的交响曲。云卷云舒，潮起潮落，日夜交替……自然中的一切故事，也都在不知不觉中悄然上演。在这个人与自然和谐共处的时代，若你对自然界的无穷奥秘感兴趣，在《斑纹》中或可窥见一二。万物有灵，美轮美奂——读《斑纹》本文原载于湄洲日报文|彭忠富办公区院子里的大树上，金蝉嘶鸣，此起彼伏，似乎在发泄对酷暑季节的不满。这\n",
      "trg = 大自然包罗了世间的一切，而我们对它的真正意义上的了解，却不是那么多。若你对自然界的无穷奥秘感兴趣，在《斑纹》中或可窥见一二。\n",
      "predicted trg = “<unk>子”不在本以及的身上，不了它们带来的一个地放在，演而成效、制和笑脸等着本就像的一道路上有着，甚至做一道……<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 5月27日，绿地控股（600606.sh）与贵州省黔晟国有资产经营有限责任公司（简称“贵州黔晟”，贵州省国资平台）签署贵州省药材有限责任公司（简称“贵州药材”）战略重组协议，绿地通过“股权收购+增资扩股”相结合的方式实现对贵州药材控股，绿地持股70%（收购60%老股+增资10%）、贵州黔晟持股30%。绿地透露，下一步将继续复制“绿地混改”模式，将部分股权转让给贵州药材管理团队及核心员工，实现由绿地控股、贵州黔晟、管理团队及核心员工共同持股的“三元结构”。贵州药材和贵州药业公司均系贵州黔晟二级子公司，为“一\n",
      "trg = 绿地通过“股权收购+增资扩股”相结合的方式实现对贵州药材控股，绿地持股70%（收购60%老股+增资10%）、贵州黔晟持股30%。\n",
      "predicted trg = 股合州地方对县资主要股，贵州茅台股份“县”微企业，盘、任观部资和利县资委员资援助，贵州省对县资援助，部署为了美元/股的资股。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = “在家里的窗台阳台上种种花，看上去是一件小事情，实际上却关系到整座城市的生态文明和城市品质。”上海市政协常委、民革上海市委副主委、上海社科院应用经济研究所文化创意产业研究室主任王慧敏感慨地说。她的感慨，源自一份由她负责的题为《关于以窗台阳台彩化为抓手，进一步激活花文化的基层治理功能的建议》的提案。这份看上去不够“热点”的提案，却有其特殊意义——这是一份跨界别的集体提案，由民革界别和民进界别共同提交。是什么促成了这份跨界别集体提案的诞生？王慧敏告诉澎湃新闻（www.thepaper.cn）记者，这是民革上海\n",
      "trg = 《关于以窗台阳台彩化为抓手，进一步激活花文化的基层治理功能的建议》这份提案有其特殊意义——这是一份跨界别的集体提案，由民革界别和民进界别共同提交。\n",
      "predicted trg = 《上海是世界的学者吉林松原副代理解读研究，这样一个试图案由于它的“大门诊”在某种可以看到，由于它的美的“大城市的“大城市”与分享一体系建而引发”。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 引子自从地球上诞生生命以来，生物的多样性在持续不断的进化，从而达到现有的复杂性。自然界，来自宇宙、太阳甚至是地球本身的电离辐射促使生物进行变异。大多数突变无法存活，但在极少数情况下，有益突变不仅能够产生新的物种，还能使新物种在野生环境下生存并茁壮生长。正是由于遗传与变异，生物才得以繁衍和进化。根据联合国粮农组织和国际原子能机构（fao/iaea）的数据库，截止2019年，有超过3200种作物新品种是由诱变育种方法获得的，其中80%左右是由物理诱变获得。亚洲国家更喜欢使用该技术对植物进行品种改良，例如中国、日\n",
      "trg = 根据联合国粮农组织和国际原子能机构的数据库，截止2019年，有超过3200种作物新品种是由诱变育种方法获得的，其中80%左右是由物理诱变获得。\n",
      "predicted trg = 经种种种种种种种种理性的种变得以下很难度，这种理念似乎已不能看到底是不能挣扎实生活水生命挑战，而是不能。在，中国生命挑战略上的种变得尤其他最大的的的的的的的的的的的的种价不能够吸着国家“新生命危害农业\n",
      "\n",
      "\n",
      "\n",
      "src = 电梯广告让人无处可躲。“我儿子前一阵跟电梯广告里学的，成天嘟嘟着嘴巴。”日前,家住泉景天沅雅园小区的吕女士向齐鲁晚报·齐鲁壹点记者反映，他们小区电梯内投放的广告有一些内容感觉不太适宜儿童观看。记者在采访中发现，不少市民有着同样的烦恼,有些电梯广告内容“简单粗暴”。电梯广告的发布有何标准，谁来审核?记者进行了采访。市民反映:坐了两天电梯，夸张口红广告轮番播前不久，吕女士发现自家小区电梯里的广告屏上开始播放一个品牌的口红广告，内容为一位年轻男子涂抹该品牌的各色口红，语调、动作很夸张。“第二天在业主群\n",
      "trg = 前不久,吕女士发现自家小区电梯里的广告屏上开始播放一个品牌的口红广告,六岁的儿子在回家时模仿起了广告里人物的动作。\n",
      "predicted trg = 团圆满老龄儿童内电子节前认，广告发布成立家，并拥有月<unk>个广告家，被广告位。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 新华社广州2月15日消息，2014年、2018年全国两会期间，习近平总书记两次参加广东代表团审议，要求广东“勇于先行先试，大胆实践探索”，“在构建推动经济高质量发展体制机制、建设现代化经济体系、形成全面开放新格局、营造共建共治共享社会治理格局上走在全国前列”。作为改革开放排头兵、先行地、实验区的广东，积极实践“发展是第一要务，人才是第一资源，创新是第一动力”理念，闻鸡起舞，日夜兼程，在新起点上再创新局。盯住市场打造高质量发展高地回乡创业的陈振柱，想把父亲在惠州市博罗县创立的乡村工厂升级成“广东某某公司”\n",
      "trg = 作为改革开放排头兵、先行地、实验区的广东，积极实践“发展是第一要务，人才是第一资源，创新是第一动力”理念，闻鸡起舞，日夜兼程，在新起点上再创新局。\n",
      "predicted trg = 广东关于“社会”在广东方当地举行。为，车辆<unk>开放军区，是广东路广东台，李永久记者，毕业能源汽车辆机构成果机构在全国首日路稳定位行。<eos>\n",
      "\n",
      "\n",
      "\n",
      "src = 综合韩媒消息，朝鲜、韩国与国际奥委会商定，在2020年东京奥运会上，朝韩将组建女子篮球、女子冰球、柔道和赛艇四个联队参加比赛。15日，韩国文化体育观光部长官都钟焕和朝鲜体育相金一国在瑞士洛桑与国际奥委会主席巴赫举行三方会晤，就朝韩在2020年东京奥运会上组建联队，参加女子篮球、女子冰球、赛艇和柔道4个项目比赛的方案达成协议。资料图韩国国际广播电台称，这将是朝韩第二次组建联队参加奥运会。去年，双方史上首次组建联队参加平昌冬奥会女子冰球项目比赛。此外，在去年的雅加达亚运会上，朝韩还共同出战女篮、皮划艇、赛\n",
      "trg = 朝鲜、韩国与国际奥委会商定，在2020年东京奥运会上，朝韩将组建女子篮球、女子冰球、柔道和赛艇四个联队参加比赛。\n",
      "predicted trg = 据韩联社4月15日韩国际协委员、韩国联社3项目负责人体育基金组织部队透露，比赛备训练制在韩国女子签订体育主席了一个比赛备训练项目，运会。<eos>\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Inferencing\n",
    "'''\n",
    "\n",
    "\n",
    "def inference(sentence, src_field, trg_field, model, device, max_len=100):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # 首先將想翻譯的句子轉成 tensor \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de_core_news_sm')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "    \n",
    "    # 句子頭尾加上 <BOS>, <EOS>\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    # tokens 透過字典轉成整數  \n",
    "    src_indices = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n",
    "    # src_tensor.shape = (batch_size, src_len)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.encoder(src_tensor, src_mask) \n",
    "        \n",
    "    trg_indices = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "    \n",
    "    # To record attention weights\n",
    "    #attn_plot = np.zeros((max_len, len(tokens)))\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device)\n",
    "        #print(trg_tensor.shape)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        #print(trg_mask.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention_weights = model.decoder(trg_tensor, encoder_outputs, trg_mask, src_mask) \n",
    "            # output.shape = (batch_size, trg_len, output_dim)\n",
    "            # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:, -1].item()\n",
    "        trg_indices.append(pred_token)\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    # 預測出來的'整數們'轉回 tokens\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indices]\n",
    "    attention = attention_weights.squeeze(0).detach().cpu().numpy()\n",
    "    # attention.shape = (n_heads, trg_len, srg_len)\n",
    "    \n",
    "    # 只回傳除了 <BOS> 以外的 tokens與attention weights\n",
    "    return trg_tokens[1:], attention \n",
    "\n",
    "# example_idx = 20\n",
    "\n",
    "# src = vars(test_data.examples[example_idx])['src']\n",
    "# trg = vars(test_data.examples[example_idx])['trg']\n",
    "\n",
    "# print(f'src = {\"\".join(src)}')\n",
    "# print(f'trg = {\"\".join(trg)}')\n",
    "\n",
    "# translation, attention = inference(src, SRC, TRG, model, device)\n",
    "\n",
    "# print(f'predicted trg = {\"\".join(translation)}')\n",
    "\n",
    "\n",
    "for i in range(21):\n",
    "    example_idx = i\n",
    "\n",
    "    src = vars(test_data.examples[example_idx])['src']\n",
    "    trg = vars(test_data.examples[example_idx])['trg']\n",
    "\n",
    "    print(f'src = {\"\".join(src)}')\n",
    "    print(f'trg = {\"\".join(trg)}')\n",
    "\n",
    "    translation, attention = inference(src, SRC, TRG, model, device)\n",
    "\n",
    "    print(f'predicted trg = {\"\".join(translation)}')\n",
    "    print(\"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence, n_heads=8, n_rows=4, n_cols=2):\n",
    "    assert n_rows * n_cols == n_heads\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,25))\n",
    "    #print(attention.shape)\n",
    "    for i in range(n_heads):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        _attention = attention[i]\n",
    "\n",
    "        cax = ax.matshow(_attention, cmap='viridis')\n",
    "\n",
    "        ax.tick_params(labelsize=12)\n",
    "        x_loc = [i for i in range(len(sentence)+2)]\n",
    "        ax.set_xticks(x_loc)\n",
    "        ax.set_xticklabels(['<bos>']+[t.lower() for t in sentence]+['<eos>'], rotation=90)\n",
    "        y_loc = [j for j in range(len(predicted_sentence))]\n",
    "        ax.set_yticks(y_loc)\n",
    "        ax.set_yticklabels(predicted_sentence)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_attention(attention, src, translation)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer_v1_colab_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
