{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9ba503-5bcf-49d0-9f4e-6f09e099242c",
   "metadata": {},
   "source": [
    "# Sequence 2 Sequence\n",
    "#### Reference:\n",
    "* https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa66df0b-e0b7-4395-a34d-27283bf25f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "import codecs\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# activate cuda\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "TSV_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\sequence2sequence\\\\Sentence pairs in English-Mandarin Chinese - 2021-06-27.tsv\"\n",
    "MAX_LENGTH = 100\n",
    "MODEL_SAVING_DIR = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\models_kg\\\\seq2seq\\\\\"\n",
    "RESULT_SAVING_DIR = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\results_kg\\\\seq2seq\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03157f68-3f1b-43ca-8c1e-d3c67dac2d4d",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc4ebc91-6702-4f0c-b644-a9c68bb3eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Let s try something .', '我們試試看！'], ['I have to go to sleep .', '我该去睡觉了。'], ['Today is June th and it is Muiriel s birthday !', '今天是６月１８号，也是Muiriel的生日！'], ['Muiriel is now .', 'Muiriel现在20岁了。'], ['The password is Muiriel .', '密码是\"Muiriel\"。'], ['I will be back soon .', '我很快就會回來。'], ['I m at a loss for words .', '我不知道應該說什麼才好。'], ['This is never going to end .', '這個永遠完不了了。'], ['This is never going to end .', '这将永远继续下去。'], ['I just don t know what to say .', '我只是不知道應該說什麼而已……']]\n"
     ]
    }
   ],
   "source": [
    "# parsing data\n",
    "\n",
    "file = codecs.open(TSV_PATH, mode=\"r\", errors=\"ignore\", encoding=\"utf8\")\n",
    "\n",
    "line_pairs = []\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "for line in file.readlines():\n",
    "    line_split_list = line.split(\"\\t\")\n",
    "    line_pairs.append([normalizeString(line_split_list[1].replace(\"\\r\\n\", \"\").replace(\"\\n\", \"\")),\n",
    "                       line_split_list[3].replace(\"\\r\\n\", \"\").replace(\"\\n\", \"\")])\n",
    "    \n",
    "    \n",
    "print(line_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3852f8c7-9969-4e0c-a724-aa21e9eaae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name, LNG):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence, LNG):\n",
    "        if LNG == \"EN\":\n",
    "            for word in sentence.split(' '):\n",
    "                self.addWord(word)\n",
    "        elif LNG == \"ZH\":\n",
    "            for word in sentence:\n",
    "                self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d539962-bf24-4104-a3a0-d4770566251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, line_pairs, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[line_element for line_element in l] for l in line_pairs]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2, \"ZH\")\n",
    "        output_lang = Lang(lang1, \"EN\")\n",
    "    else:\n",
    "        input_lang = Lang(lang1, \"EN\")\n",
    "        output_lang = Lang(lang2, \"ZH\")\n",
    "        \n",
    "    print(pairs[0:10])\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "465f193c-63e0-4c8a-af74-b7752b47c85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd1af53c-3ffd-4ad5-97dd-41aa7aa49524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "[['Let s try something .', '我們試試看！'], ['I have to go to sleep .', '我该去睡觉了。'], ['Today is June th and it is Muiriel s birthday !', '今天是６月１８号，也是Muiriel的生日！'], ['Muiriel is now .', 'Muiriel现在20岁了。'], ['The password is Muiriel .', '密码是\"Muiriel\"。'], ['I will be back soon .', '我很快就會回來。'], ['I m at a loss for words .', '我不知道應該說什麼才好。'], ['This is never going to end .', '這個永遠完不了了。'], ['This is never going to end .', '这将永远继续下去。'], ['I just don t know what to say .', '我只是不知道應該說什麼而已……']]\n",
      "Read 56574 sentence pairs\n",
      "Trimmed to 56571 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "EN 14702\n",
      "ZH 4547\n",
      "['I own a horse .', '我拥有一匹马。']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, line_pairs, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0], \"EN\")\n",
    "        output_lang.addSentence(pair[1], \"ZH\")\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('EN', 'ZH', False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340f85b-a11a-47bf-aca1-dcfb118c67ed",
   "metadata": {},
   "source": [
    "# Model Stage\n",
    "#### Documetations\n",
    "* Pytorch Embedding: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "* Pytorh GRU: https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "* Pytorch Softmax: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
    "* Pytorch View: https://pytorch.org/docs/stable/generated/torch.Tensor.view.html\n",
    "* Pytorch Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b42a5d4e-dafc-4ad9-a8f6-3f09997d41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "187dcc58-7bff-4d95-b5da-0d80e3c3a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc9567ee-fe93-465e-adbf-385c3e91e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence, LNG):\n",
    "    if LNG == \"EN\":\n",
    "        return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    elif LNG == \"ZH\":\n",
    "        return [lang.word2index[word] for word in sentence]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence, LNG):\n",
    "    indexes = indexesFromSentence(lang, sentence, LNG)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair, LNG_pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0], LNG_pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1], LNG_pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45d484fc-5568-4b77-9c43-4a4dade86cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60aa3ab6-e21c-45d4-a157-d2ac7a90b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11d922df-bffc-4bc1-96a3-c77eaa246f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, LNG_pair, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs), LNG_pair)\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85fc3266-5c0e-44b1-a831-7f0b658d7905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95c91a7f-ac81-43e5-8fa1-07f8cb8db2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m 6s (- 103m 39s) (5000 2%) 4.6258\n",
      "3m 51s (- 92m 24s) (10000 4%) 4.3789\n",
      "5m 38s (- 88m 20s) (15000 6%) 4.2040\n",
      "7m 26s (- 85m 30s) (20000 8%) 4.0454\n",
      "9m 14s (- 83m 10s) (25000 10%) 3.9528\n",
      "11m 2s (- 80m 55s) (30000 12%) 3.8119\n",
      "12m 50s (- 78m 52s) (35000 14%) 3.7262\n",
      "14m 38s (- 76m 53s) (40000 16%) 3.6921\n",
      "16m 28s (- 75m 3s) (45000 18%) 3.6569\n",
      "18m 18s (- 73m 12s) (50000 20%) 3.5803\n",
      "20m 7s (- 71m 19s) (55000 22%) 3.5139\n",
      "21m 56s (- 69m 29s) (60000 24%) 3.5140\n",
      "23m 45s (- 67m 37s) (65000 26%) 3.4625\n",
      "25m 35s (- 65m 47s) (70000 28%) 3.4554\n",
      "27m 24s (- 63m 57s) (75000 30%) 3.4252\n",
      "29m 14s (- 62m 9s) (80000 32%) 3.3885\n",
      "31m 3s (- 60m 17s) (85000 34%) 3.3592\n",
      "32m 53s (- 58m 28s) (90000 36%) 3.3893\n",
      "34m 44s (- 56m 40s) (95000 38%) 3.3465\n",
      "36m 33s (- 54m 50s) (100000 40%) 3.3350\n",
      "38m 24s (- 53m 2s) (105000 42%) 3.3161\n",
      "40m 13s (- 51m 12s) (110000 44%) 3.2806\n",
      "42m 3s (- 49m 21s) (115000 46%) 3.2676\n",
      "43m 53s (- 47m 32s) (120000 48%) 3.2789\n",
      "45m 44s (- 45m 44s) (125000 50%) 3.2610\n",
      "47m 33s (- 43m 54s) (130000 52%) 3.2688\n",
      "49m 22s (- 42m 3s) (135000 54%) 3.2174\n",
      "51m 12s (- 40m 13s) (140000 56%) 3.2326\n",
      "53m 2s (- 38m 24s) (145000 57%) 3.1957\n",
      "54m 51s (- 36m 34s) (150000 60%) 3.2086\n",
      "56m 42s (- 34m 45s) (155000 62%) 3.2298\n",
      "58m 33s (- 32m 56s) (160000 64%) 3.1855\n",
      "60m 22s (- 31m 6s) (165000 66%) 3.1990\n",
      "62m 13s (- 29m 16s) (170000 68%) 3.2021\n",
      "64m 2s (- 27m 26s) (175000 70%) 3.1633\n",
      "65m 56s (- 25m 38s) (180000 72%) 3.1432\n",
      "67m 48s (- 23m 49s) (185000 74%) 3.1566\n",
      "69m 38s (- 21m 59s) (190000 76%) 3.1538\n",
      "71m 33s (- 20m 10s) (195000 78%) 3.1203\n",
      "73m 28s (- 18m 22s) (200000 80%) 3.1303\n",
      "75m 20s (- 16m 32s) (205000 82%) 3.1291\n",
      "77m 14s (- 14m 42s) (210000 84%) 3.1288\n",
      "79m 7s (- 12m 52s) (215000 86%) 3.1187\n",
      "81m 1s (- 11m 2s) (220000 88%) 3.1373\n",
      "82m 55s (- 9m 12s) (225000 90%) 3.1189\n",
      "84m 51s (- 7m 22s) (230000 92%) 3.1074\n",
      "86m 48s (- 5m 32s) (235000 94%) 3.1089\n",
      "88m 44s (- 3m 41s) (240000 96%) 3.1014\n",
      "90m 37s (- 1m 50s) (245000 98%) 3.1098\n",
      "92m 31s (- 0m 0s) (250000 100%) 3.0663\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 250000, [\"EN\", \"ZH\"], print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff3313c4-aa38-4cb5-bd0d-febe7e0e8abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to path\n",
    "torch.save(encoder1.state_dict(), MODEL_SAVING_DIR + \"210718_encoder1_translation.pkl\")\n",
    "torch.save(attn_decoder1.state_dict(), MODEL_SAVING_DIR + \"210718_decoder_translation.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114bddcc-9edf-41f7-ac5c-14ff847cb136",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b272516a-3b9f-47ab-b3a7-08f388a83f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, LNG, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence, LNG)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "    \n",
    "def evaluateRandomly(encoder, decoder, LNG, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0], LNG)\n",
    "        output_sentence = ''.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0c88d5a-de9d-4be0-9833-5579bc4e6d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> True understanding requires the greatest of wisdoms .\n",
      "= 非上上智，无了了心。\n",
      "< 对于为的新的意的。生生。<EOS>\n",
      "\n",
      "> A dog followed me home .\n",
      "= 一隻狗跟隨我回家。\n",
      "< 只狗叫我我家。<EOS>\n",
      "\n",
      "> Flowers bloom .\n",
      "= 鮮花盛開。\n",
      "< 海一口叫花。<EOS>\n",
      "\n",
      "> Tom was shocked .\n",
      "= 湯姆吃了一驚。\n",
      "< 汤姆是人了。。<EOS>\n",
      "\n",
      "> I will try again thank you .\n",
      "= 我會再試一次, 謝謝您。\n",
      "< 我希望你再试试。<EOS>\n",
      "\n",
      "> The storm has died down .\n",
      "= 暴风雨平息下来了。\n",
      "< 死了了死了死了了。了。<EOS>\n",
      "\n",
      "> I don t give a rat s ass .\n",
      "= 我完全不管。\n",
      "< 我不有借。有。<EOS>\n",
      "\n",
      "> He is a comedian .\n",
      "= 他是喜剧演员。\n",
      "< 他是个人。<EOS>\n",
      "\n",
      "> Jack is the tallest boy in his class .\n",
      "= Jack是班里最高的男孩。\n",
      "< 他的的的班上上上上的<EOS>\n",
      "\n",
      "> Tom told Mary that she should stop pretending to be intoxicated .\n",
      "= 汤姆告诉玛丽她应该停止假装醉酒。\n",
      "< 汤姆告诉玛丽告诉玛丽会停止假。<EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1, \"EN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56eef4d9-607a-4904-abbc-2c21b5e1d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_file = codecs.open(RESULT_SAVING_DIR + \"ZH_EN_translation.txt\", mode=\"w\", encoding=\"utf8\")\n",
    "\n",
    "for pair in pairs:\n",
    "    export_file.write(\"Input: \" + str(pair[0]) + \"\\n\")\n",
    "    export_file.write(\"Ground Truth: \" + str(pair[1]) + \"\\n\")\n",
    "    output_words, attentions = evaluate(encoder1, attn_decoder1, pair[0], \"EN\")\n",
    "    export_file.write(\"Prediction: \" + ''.join(output_words) + \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
