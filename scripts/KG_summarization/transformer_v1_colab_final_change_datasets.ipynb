{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGypE_zDn2xe"
   },
   "source": [
    "# Transformer\n",
    "\n",
    "在 2017 年，Google Brain 團隊發布一篇論文 - [`Attention is all you need`](https://arxiv.org/abs/1706.03762)，此論文提出一個嶄新的架構 - `Transformer`，一舉轟動的自然語言處理領域，`Transformer` 是目前自然語言處理中所有 `SOTA Model (State-of-the-Arts)` 的基底，這篇論文一開始是將 `Transformer` 應用在機器翻譯任務上，其表現直接碾壓了 `Seq2seq` 系列模型，其貢獻可以總結為兩點：\n",
    "\n",
    "* 平行計算: `Seq2seq` 的編碼器以及解碼器都是 `RNN` ，序列輸入給 `RNN` 時必須先等 $x_t$ 運算完才能運算 $x_{t+1}$，這樣導致 `Seq2seq` 無法平行計算。而 `Transformer` 是一次輸入所有序列 $X=\\{x_1,\\cdots,x_n\\}$，不需要去等待前一個時間點。\n",
    "\n",
    "* `Bidirectional Representation`: `Seq2seq` 輸入序列的方式是單向的 ($x_1\\rightarrow x_2\\rightarrow \\cdots \\rightarrow x_n$)，而有時候要理解句子不能只由前往後，也要由後往前 (也就是上下文的資訊)，雖然後來有許多論文提出雙向 Seq2seq (Bidirectional LSTM) 來提升模型表現，但本質上兩個方向在計算過程中還是互相獨立的，效果有限。而 `Transformer` 是透過 `Self-attention` 來一次獲得上下文的資訊，計算過程中不使用 `RNN`。\n",
    "\n",
    "今日深度學習所用的主要套件版本為 (Google Colab 上都已安裝):\n",
    "* python 3.7\n",
    "* PyTorch 1.8\n",
    "* Torchtext 0.9\n",
    "\n",
    "Refs: \n",
    "1. https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb\n",
    "2. https://pytorch.org/tutorials/beginner/transformer_tutorial\n",
    "3. https://andrewpeng.dev/transformer-pytorch/\n",
    "4. https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FzBmx31B-zY"
   },
   "source": [
    "# 下載語言工具\n",
    "\n",
    "本次實作資料集工具來源為 [spaCy](https://spacy.io/usage/models) 中的英德文語言部份，首先需要下載語言工具，**下載完畢後請重新啟動執行階段 (執行階段 -> 重新啟動執行階段)** **Runtime -> Restart runtime**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21169,
     "status": "ok",
     "timestamp": 1626971828789,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "GHfJ0iOxCDcQ",
    "outputId": "e442d150-29ae-40a5-e956-b9e59917c850"
   },
   "source": [
    "!pip install -U spacy\n",
    "!pip install einops\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOzbvYHGoHLF"
   },
   "source": [
    "# 載入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7l1zAjLun06v"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import math, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import einops\n",
    "import gdown\n",
    "import spacy\n",
    "import random, math, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator, Dataset, Example\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i8UALqBXoO7M"
   },
   "outputs": [],
   "source": [
    "# 為了重複實驗方便，我們固定隨機種子\n",
    "SEED = 87\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YijVwAkoZw_"
   },
   "source": [
    "# 資料前處理\n",
    "\n",
    "今天我們將採用 torchtext 中的資料集 Multi30k 來做示範。這是擁有約 30,000 條相對應英文、德文與法文語句的資料集；每個句子約 12 個單字左右。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifd_EXaKodyd"
   },
   "source": [
    "## a. 德文與英文斷詞\n",
    "首先，我們需要將句子斷成合適的最小單位。所以，我們將利用 [spaCy](https://spacy.io/usage/models) 套件來做為斷詞工具 (tokenizer)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Xw9s3bDuoPF1"
   },
   "outputs": [],
   "source": [
    "# # 載入斷詞模型\n",
    "# spacy_de = spacy.load('de_core_news_sm') # for German\n",
    "# spacy_en = spacy.load('en_core_web_sm')  # for English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iZIsxVJEn1d6"
   },
   "outputs": [],
   "source": [
    "# 定義斷詞函數\n",
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    將給定的德文語句斷詞\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    將給定的英文語句斷詞\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize(text):\n",
    "    return [char for char in text.split(\" \")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wO895IT_oukU"
   },
   "source": [
    "## b. 添加`<BOS>`(begin of sequence),`<EOS>`(end of sequence) 在句子頭尾\n",
    "我們利用 torchtext 中的 [Field](https://torchtext.readthedocs.io/en/latest/data.html#fields) 物件來統一處理如何將斷詞後的文本轉為 torch tensors。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Data Source Configuration '''\n",
    "# TRAIN_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\train.src\"\n",
    "# TRAIN_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\train.tgt\"\n",
    "# VALID_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\valid.src\"\n",
    "# VALID_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\valid.tgt\"\n",
    "# TEST_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\test.src\"\n",
    "# TEST_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\test.tgt\"\n",
    "\n",
    "# ''' Data Configuration '''\n",
    "# TRAIN_SAMPLE_NUM = 10\n",
    "# VALID_SAMPLE_NUM = 200\n",
    "# TEST_SAMPLE_NUM = 100\n",
    "# WORD_MIN_FREQUENCY = 0\n",
    "# # MAX_LENGTH_INPUT = int(np.max([len(data)+2 for data in train_x_list]))\n",
    "# # MAX_LENGTH_OUTPUT = int(np.max([len(data)+2 for data in train_y_list]))\n",
    "# MAX_LENGTH_INPUT = 10\n",
    "# MAX_LENGTH_OUTPUT = 10\n",
    "# BOS_TOKEN = \"<bos>\"\n",
    "# EOS_TOKEN = \"<eos>\"\n",
    "\n",
    "# ''' Process Begins '''\n",
    "# train_x_list = []\n",
    "# train_y_list = []\n",
    "# valid_x_list = []\n",
    "# valid_y_list = []\n",
    "# test_x_list = []\n",
    "# test_y_list = []\n",
    "\n",
    "# file_train_x = codecs.open(TRAIN_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "# file_train_y = codecs.open(TRAIN_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "# file_valid_x = codecs.open(VALID_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "# file_valid_y = codecs.open(VALID_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "# file_test_x = codecs.open(TEST_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "# file_test_y = codecs.open(TEST_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "\n",
    "# # create list for training, validation, testing set\n",
    "# temp_index = 0\n",
    "# while True or temp_index == TRAIN_SAMPLE_NUM:\n",
    "#     line = file_train_x.readline()\n",
    "#     train_x_list.append(BOS_TOKEN + \" \" + line.replace(\"\\n\", \"\") + \" \" + EOS_TOKEN)\n",
    "\n",
    "#     if not line or temp_index == TRAIN_SAMPLE_NUM:\n",
    "#         temp_index = 0\n",
    "#         break\n",
    "#     else:\n",
    "#         temp_index += 1\n",
    "\n",
    "# while True or temp_index == TRAIN_SAMPLE_NUM:\n",
    "#     line = file_train_y.readline()\n",
    "#     train_y_list.append(BOS_TOKEN + \" \" + line.replace(\"\\n\", \"\") + \" \" + EOS_TOKEN)\n",
    "\n",
    "#     if not line or temp_index == TRAIN_SAMPLE_NUM:\n",
    "#         temp_index = 0\n",
    "#         break\n",
    "#     else:\n",
    "#         temp_index += 1\n",
    "\n",
    "# while True or temp_index == VALID_SAMPLE_NUM:\n",
    "#     line = file_valid_x.readline()\n",
    "#     valid_x_list.append(BOS_TOKEN + \" \" + line.replace(\"\\n\", \"\") + \" \" + EOS_TOKEN)\n",
    "\n",
    "#     if not line or temp_index == VALID_SAMPLE_NUM:\n",
    "#         temp_index = 0\n",
    "#         break\n",
    "#     else:\n",
    "#         temp_index += 1\n",
    "\n",
    "# while True or temp_index == VALID_SAMPLE_NUM:\n",
    "#     line = file_valid_y.readline()\n",
    "#     valid_y_list.append(BOS_TOKEN + \" \" + line.replace(\"\\n\", \"\") + \" \" + EOS_TOKEN)\n",
    "\n",
    "#     if not line or temp_index == VALID_SAMPLE_NUM:\n",
    "#         temp_index = 0\n",
    "#         break\n",
    "#     else:\n",
    "#         temp_index += 1\n",
    "\n",
    "# while True or temp_index == TEST_SAMPLE_NUM:\n",
    "#     line = file_test_x.readline()\n",
    "#     test_x_list.append(BOS_TOKEN + \" \" + line.replace(\"\\n\", \"\") + \" \" + EOS_TOKEN)\n",
    "\n",
    "#     if not line or temp_index == TEST_SAMPLE_NUM:\n",
    "#         temp_index = 0\n",
    "#         break\n",
    "#     else:\n",
    "#         temp_index += 1\n",
    "\n",
    "# while True or temp_index == TEST_SAMPLE_NUM:\n",
    "#     line = file_test_y.readline()\n",
    "#     test_y_list.append(BOS_TOKEN + \" \" + line.replace(\"\\n\", \"\") + \" \" + EOS_TOKEN)\n",
    "\n",
    "#     if not line or temp_index == TEST_SAMPLE_NUM:\n",
    "#         temp_index = 0\n",
    "#         break\n",
    "#     else:\n",
    "#         temp_index += 1\n",
    "\n",
    "# print(len(train_x_list), len(train_y_list), len(valid_x_list), len(valid_y_list), len(test_x_list),\n",
    "#           len(test_y_list))\n",
    "# print(train_x_list[:2], \"\\n\\n\", train_y_list[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 2 2 2 2\n",
      "['山 西 省 晋 中 ', '3 月 26 日 下'] \n",
      "\n",
      " ['赵 建 平 指 出 ', '田 以 堂 在 国 ']\n"
     ]
    }
   ],
   "source": [
    "''' Data Source Configuration '''\n",
    "TRAIN_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\train.src\"\n",
    "TRAIN_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\train.tgt\"\n",
    "VALID_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\valid.src\"\n",
    "VALID_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\valid.tgt\"\n",
    "TEST_DATA_X_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\test.src\"\n",
    "TEST_DATA_y_PATH = \"C:\\\\Users\\\\User\\\\Desktop\\\\Ricardo\\\\KnowledgeGraph_materials\\\\data_kg\\\\CLTS\\\\test.tgt\"\n",
    "\n",
    "''' Data Configuration '''\n",
    "TRAIN_SAMPLE_NUM = 1\n",
    "VALID_SAMPLE_NUM = 1\n",
    "TEST_SAMPLE_NUM = 1\n",
    "WORD_MIN_FREQUENCY = 0\n",
    "# MAX_LENGTH_INPUT = int(np.max([len(data)+2 for data in train_x_list]))\n",
    "# MAX_LENGTH_OUTPUT = int(np.max([len(data)+2 for data in train_y_list]))\n",
    "MAX_LENGTH_INPUT = 10\n",
    "MAX_LENGTH_OUTPUT = 10\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "\n",
    "''' Process Begins '''\n",
    "train_x_list = []\n",
    "train_y_list = []\n",
    "valid_x_list = []\n",
    "valid_y_list = []\n",
    "test_x_list = []\n",
    "test_y_list = []\n",
    "\n",
    "file_train_x = codecs.open(TRAIN_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_train_y = codecs.open(TRAIN_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_valid_x = codecs.open(VALID_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_valid_y = codecs.open(VALID_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_test_x = codecs.open(TEST_DATA_X_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "file_test_y = codecs.open(TEST_DATA_y_PATH, mode=\"r\", encoding=\"utf8\")\n",
    "\n",
    "# create list for training, validation, testing set\n",
    "temp_index = 0\n",
    "while True or temp_index == TRAIN_SAMPLE_NUM:\n",
    "    line = file_train_x.readline()\n",
    "    train_x_list.append(line.replace(\"\\n\", \"\")[:10])\n",
    "\n",
    "    if not line or temp_index == TRAIN_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "\n",
    "while True or temp_index == TRAIN_SAMPLE_NUM:\n",
    "    line = file_train_y.readline()\n",
    "    train_y_list.append(line.replace(\"\\n\", \"\")[:10])\n",
    "\n",
    "    if not line or temp_index == TRAIN_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "\n",
    "while True or temp_index == VALID_SAMPLE_NUM:\n",
    "    line = file_valid_x.readline()\n",
    "    valid_x_list.append(line.replace(\"\\n\", \"\")[:10])\n",
    "\n",
    "    if not line or temp_index == VALID_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "\n",
    "while True or temp_index == VALID_SAMPLE_NUM:\n",
    "    line = file_valid_y.readline()\n",
    "    valid_y_list.append(line.replace(\"\\n\", \"\")[:10])\n",
    "\n",
    "    if not line or temp_index == VALID_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "\n",
    "while True or temp_index == TEST_SAMPLE_NUM:\n",
    "    line = file_test_x.readline()\n",
    "    test_x_list.append(line.replace(\"\\n\", \"\")[:10])\n",
    "\n",
    "    if not line or temp_index == TEST_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "\n",
    "while True or temp_index == TEST_SAMPLE_NUM:\n",
    "    line = file_test_y.readline()\n",
    "    test_y_list.append(line.replace(\"\\n\", \"\")[:10])\n",
    "\n",
    "    if not line or temp_index == TEST_SAMPLE_NUM:\n",
    "        temp_index = 0\n",
    "        break\n",
    "    else:\n",
    "        temp_index += 1\n",
    "\n",
    "print(len(train_x_list), len(train_y_list), len(valid_x_list), len(valid_y_list), len(test_x_list),\n",
    "          len(test_y_list))\n",
    "print(train_x_list[:2], \"\\n\\n\", train_y_list[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 德文為 source\n",
    "# SRC = Field(tokenize = tokenize, \n",
    "#             init_token = '<bos>', \n",
    "#             eos_token = '<eos>', \n",
    "#             lower = True,                 # 全部轉為小寫\n",
    "#             batch_first = True)           # batch size (dimension) 放最前面\n",
    "\n",
    "# # 英文為 target\n",
    "# TRG = Field(tokenize = tokenize, \n",
    "#             init_token = '<bos>', \n",
    "#             eos_token = '<eos>', \n",
    "#             lower = True,                 # 全部轉為小寫\n",
    "#             batch_first = True)           # batch size (dimension) 放最前面\n",
    "\n",
    "\n",
    "SRC = Field(sequential=True,\n",
    "                  tokenize=tokenize,\n",
    "                  init_token = '<bos>', \n",
    "                  eos_token = '<eos>',\n",
    "                  lower=True,\n",
    "                  batch_first = True) \n",
    "\n",
    "TRG = Field(sequential=True,\n",
    "                  tokenize=tokenize,\n",
    "                  init_token = '<bos>', \n",
    "                  eos_token = '<eos>',\n",
    "                  lower=True,\n",
    "                  batch_first = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, ?it/s]\n",
      "2it [00:00, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.legacy.data.dataset.Dataset object at 0x0000025C3F865B88>\n",
      "Number of training examples: 2\n",
      "Number of validation examples: 2\n",
      "Number of testing examples: 2\n",
      "Unique tokens in source (de) vocabulary: 4\n",
      "Unique tokens in target (en) vocabulary: 5\n",
      "['3', '月', '26', '日', '下']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(input_list, output_list, text_field, label_field, test=False):\n",
    "\t# idData pair training is useless during training, use None to specify its corresponding field\n",
    "    fields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n",
    "                 (\"src\", text_field), (\"trg\", label_field)]       \n",
    "    examples = []\n",
    "\n",
    "    if test:\n",
    "        # If it is a test set, the label is not loaded\n",
    "        for text in tqdm(input_list):\n",
    "            examples.append(Example.fromlist([None, text, None], fields))\n",
    "    else:\n",
    "        for text, label in tqdm(zip(input_list, output_list)):\n",
    "            examples.append(Example.fromlist([None, text, label], fields))\n",
    "    return examples, fields\n",
    "\n",
    "# Get the examples and fields needed to build the Dataset\n",
    "train_examples, train_fields = get_dataset(train_x_list, train_y_list, SRC, TRG)\n",
    "valid_examples, valid_fields = get_dataset(valid_x_list, valid_y_list, SRC, TRG)\n",
    "test_examples, test_fields = get_dataset(test_x_list, test_y_list, SRC, None, test=True)\n",
    "\n",
    "#Build Dataset dataset\n",
    "train_data = Dataset(train_examples, train_fields)\n",
    "valid_data = Dataset(valid_examples, valid_fields)\n",
    "test_data = Dataset(test_examples, test_fields)\n",
    "\n",
    "\n",
    "print(train_data)\n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "\n",
    "\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")\n",
    "\n",
    "print(train_data[1].src)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Use BucketIterator instead of the usual one to automatically deal with padding problem\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n",
    "                                                                      batch_size = BATCH_SIZE, \n",
    "                                                                      device = device)\n",
    "\n",
    "# Constructing iterators for both training and validation sets\n",
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "        (train_data, valid_data), #Build the data set required by the dataset\n",
    "        batch_sizes=BATCH_SIZE,\n",
    "        device=device, # If you use gpu, here -1 is replaced with the GPU number.\n",
    "        sort_key=lambda x: len(x.src), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "        sort_within_batch=False,\n",
    "        repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n",
    ")\n",
    "\n",
    "test_iterator = Iterator(test_data, batch_size=BATCH_SIZE, device=device, sort=False, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtRkqvM2o4e3"
   },
   "source": [
    "## c. 切分資料集 Multi30k\n",
    "使用功能函數 splits 切分資料為訓練、驗證與測試集；參數 exts 是要告知切分時哪個語言是 source (前)，哪個是 target (後)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4888,
     "status": "ok",
     "timestamp": 1626971857433,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "J8iG1lLAowJ2",
    "outputId": "8a9dc91e-0036-4c1c-e38e-82afdbb9f757"
   },
   "outputs": [],
   "source": [
    "# train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (SRC, TRG))\n",
    "\n",
    "# print(train_data)\n",
    "# print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "# print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "# print(f\"Number of testing examples: {len(test_data.examples)}\")\n",
    "\n",
    "# SRC.build_vocab(train_data, min_freq = 2)\n",
    "# TRG.build_vocab(train_data, min_freq = 2)\n",
    "\n",
    "# print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "# print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")\n",
    "\n",
    "# print(train_data[1].src)\n",
    "\n",
    "# BATCH_SIZE = 128\n",
    "\n",
    "# # Use BucketIterator instead of the usual one to automatically deal with padding problem\n",
    "# train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), \n",
    "#                                                                       batch_size = BATCH_SIZE, \n",
    "#                                                                       device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyHz7LnYpPNw"
   },
   "source": [
    "# 建立 transformer 模型\n",
    "\n",
    "`Transformer` 分為編碼器 (Encoder) 與解碼器 (Decoder)，中間會有一條線 (矩陣) 連接，在 `Transformer` 中，最核心的架構為 `Multi-head Attention` 這個架構，後續有許多論文都針對這個架構提出新的改進方式。\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1BmuP7m3J0-jEdLtSGV6oBsSYGcne9llD' width=\"400\"/>\n",
    "<figcaption>Self-attention</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2eDzsF7pcpl"
   },
   "source": [
    "## a. Encoder\n",
    "\n",
    "這裡先讓大家有一個概觀，了解 `Encoder` 的輸入與輸出分別是什麼:\n",
    "\n",
    "目前進行的機器翻譯任務是德文翻譯成英文：\n",
    "\n",
    "* `Encoder`: \n",
    "  * **輸入**: 德文句子 (`source sentence`) 進行 `word embedding` 之後形成詞向量矩陣 $X\\in R^{N\\times d_{model}}$ ($d_{model}$ 表示詞向量維度)，最主要的目的是將 `source sentence` 作為 `Query,Key,Value(Q,K,V)` 進行 `self-attention`。\n",
    "  * **輸出**: `hidden representation` (矩陣)，維度與轉為 word embedding 的輸入一樣 ($\\in R^{N\\times d_{model}}$)。\n",
    "\n",
    "* `Source mask`\n",
    "  \n",
    "  在前處理中，需要把輸入的每個 batch 的句子做 padding 變成統一長度，主要是希望計算 loss 時不被算進去，padding 的地方也不希望模型注意到。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1626971864409,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "y1Z8ci1RR7Bo",
    "outputId": "c511e27b-4400-4b62-9fd6-0b6059d05073"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source mask example\n",
    "src_sample = torch.ByteTensor([[7,6,1,1,1],[5,4,3,1,1]])\n",
    "src_pad_idx = 1\n",
    "\n",
    "def make_src_mask(src):\n",
    "    # src.shape = (batch_size, src_len)\n",
    "    src_mask = (src != src_pad_idx)\n",
    "    \n",
    "    return src_mask\n",
    "\n",
    "src_mask = make_src_mask(src_sample)\n",
    "src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "P2ODB3APpFfr"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_layers, n_heads, pf_dim, dropout, learnable_pos= True, max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        # input_dim = dictionary size of the src language\n",
    "        # nn.Embedding: 吃進一個 token (整數), 吐出一個 d_model 維度的向量\n",
    "        self.tok_embedding = nn.Embedding(input_dim, d_model)\n",
    "        if learnable_pos:\n",
    "            self.pos_embedding = nn.Embedding(max_length, d_model)\n",
    "        else:\n",
    "            self.pos_embedding = PositionalEncoding(max_length, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_seq_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_seq_len)\n",
    "        Output:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        print(src.shape)\n",
    "        print(src_mask.shape)\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "        # pos.shape = (batch_size, src_len)\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        # src.shape = (batch_size, src_len, d_model)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "                \n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "U7ziUhUmpFkz"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.ff_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.self_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(d_model, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_seq_len)\n",
    "        Output:\n",
    "            src.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"        \n",
    "        # self-attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "               \n",
    "        # positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        # dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCJN-lXspy7W"
   },
   "source": [
    "## b. Positional Encoding\n",
    "\n",
    "Word Embedding 所表達的是所有詞向量之間的相似關係，而 Transformer 的做法是透過內積解決RNN的長距離依賴問題 (long-range dependenices) ，但是 Transformer 這樣做卻沒有考慮到句子中的詞先後順序關係，透過 Positional Encoding ，讓詞向量之間不只因為 word embedding 語義關係而靠近，也可以因為詞之間的位置相互靠近而靠近。Positional Encoding 的公式如下：\n",
    "\n",
    "$$\n",
    "PE_{(pos,2i)} = \\sin(pos/10000^{\\frac{2i}{d_{model}}}) \\\\\n",
    "PE_{(pos,2i+1)} = \\cos(pos/10000^{\\frac{2i}{d_{model}}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xeB8XWvHpFqo"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_length, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        # position.shape = (max_length, 1)\n",
    "        angle_rates = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # angle_rates.shape = (d_model/2,)\n",
    "        angle_rads = position * angle_rates\n",
    "        # angle_rads.shape = (max_length, d_model/2)\n",
    "        pe[:, 0::2] = torch.sin(angle_rads)   # 取偶數\n",
    "        pe[:, 1::2] = torch.cos(angle_rads)   # 取奇數\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # pe.shape = (1, max_length, d_model)\n",
    "        self.register_buffer('pe', pe) # register a constant tensor (not updated by optim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x.shape = (batch_size, src_len)\n",
    "        Output:\n",
    "            x.shape = (batch_size, src_len, d_model)\n",
    "        \"\"\"\n",
    "        x = torch.zeros(x.size(0), x.size(1), self.d_model) + self.pe[:, :x.size(1)]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYus1Z4vqAmM"
   },
   "source": [
    "舉個例子來理解一下 positional encoding (non-learnable)。此例拿第 25 個 token 的 positional encoding 來跟其餘 50 個字 (包含自己) 的 positional encoding 計算內積 (`np.dot`)，能夠發現越靠近 token 25 的內積越大，反之，越遠則內積越小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627
    },
    "executionInfo": {
     "elapsed": 932,
     "status": "ok",
     "timestamp": 1626971875048,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "0sEV7wDYp0Cd",
    "outputId": "b8629b88-b045-4ff1-9663-bdf52e8a39d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 50])\n",
      "After positional encoding: torch.Size([2, 50, 512])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 100\n",
    "src_length = 50\n",
    "input = torch.arange(0, src_length).unsqueeze(0).repeat(2, 1)\n",
    "print(\"Input shape:\", input.shape)\n",
    "d_model = 512\n",
    "pos_encoding = PositionalEncoding(max_length, d_model)(input) # (batch_size, src_length, d_model)\n",
    "print(\"After positional encoding:\", pos_encoding.shape)\n",
    "\n",
    "# 考慮第一筆資料第 25 個 token\n",
    "inp = pos_encoding[0][25].numpy()\n",
    "\n",
    "distance_list = list()\n",
    "for i in range(50):\n",
    "    target_word = pos_encoding[0][i].numpy()\n",
    "    dot_product = np.dot(inp, target_word)\n",
    "    distance_list.append(dot_product)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.plot(distance_list)\n",
    "plt.xticks(list(range(50)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1NSyANLqI0N"
   },
   "source": [
    "可以看到確實發現越靠近 token 25 的內積越大，反之，越遠則內積越小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "se-KLX5ZqOcd"
   },
   "source": [
    "## c. Multi-head self-attention\n",
    "\n",
    "* Self-attention\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1M1XWNp8ywQrqQtjMJ4TKtRK2q9xssoL2' width=\"600\"/>\n",
    "<figcaption>Multi-Head Attention structure</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "架構如上圖，`Self-attention` 又稱自注意機制，在論文中被稱為 `Scaled Dot-Product Attention`\b，這個架構是 `Transformer` 的核心架構，在這模型會學到句子中詞與詞之間的關係。考慮一個句子，要關注的詞稱為 `Query`，被關注的詞則稱為 `Key`。通常都是一個 `Query` 去關注多個 `Key`。Self-attention 基本上有以下幾個步驟：\n",
    "\n",
    "1. `Word embedding`: 假設句子斷詞後(n 個詞)，每個詞已轉為詞向量且詞向量維度為 $d_k$，則句子就能表示為 $X\\in R^{n\\times d_k}$。\n",
    "2. `Q,K,V`: 將 $X$ 分別通過三個不同的全連接層 $W_Q,W_K,W_V$ 得到 $Q,K,V\\in R^{n\\times d_k}$。\n",
    "3. `Self-attention`: 將 $Q$ 和 $K$ 做矩陣相乘 ($QK^\\top$) 得到注意力矩陣，如同下圖中的方陣一樣，每個詞之間都會有一個注意力的值 $M_{i,j}$ (在此教學最後我們會視覺化這個注意力矩陣)，最後再將注意力矩陣與 $V$ 做矩陣相乘，得到輸出 $\\in R^{n\\times d_k}$。\n",
    "\n",
    "$$\n",
    "\\mathrm{Attention}(Q,K,V) = \\mathrm{Softmax}(\\frac{QK^\\top}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "`Transformer` 的精髓就在此，使用矩陣相乘讓詞與詞之間計算注意力，而矩陣相乘本質上就是多個內積，所以 `self-attention` 就是使用內積來實現注意力機制。\n",
    "\n",
    "* Multi-head\n",
    "\n",
    "  流程: 將 `Q,K,V` 分成 num_heads 份，各自做 self-attention，然後再 Concat ，接著通過 dense 輸出。\n",
    "\n",
    "  分成 num_heads 的優點最主要是希望讓每個 head 各自注意到序列中不同的地方，而且切分成較小的矩陣還能加速訓練過程。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EA9cREuDp0Hx"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # 確保d_model可以被num_heads整除\n",
    "        assert d_model % self.n_heads == 0\n",
    "        self.head_dim = d_model // n_heads  # 將 d_model dimension 分成 n_heads 份\n",
    "        \n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            query.shape = (batch_size, query_len, d_model)\n",
    "            key.shape = (batch_size, key_len, d_model)\n",
    "            value.shape = (batch_size, value_len, d_model)\n",
    "            mask.shape = (batch_size, 1, query_len, key_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, query_len, d_model)\n",
    "            attention_weights.shape = (batch_size, n_heads, query_len, key_len)\n",
    "        \"\"\"   \n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # 通過全連結層形成 Q,K,V        \n",
    "        Q = self.wq(query)\n",
    "        K = self.wk(key)\n",
    "        V = self.wv(value)\n",
    "        # Q.shape = (batch_size, query_len, d_model)\n",
    "        # K.shape = (batch_size, key_len, d_model)\n",
    "        # V.shape = (batch_size, value_len, d_model)\n",
    "\n",
    "        # 將 q,k,v 等份切成 num_heads 份        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # Q.shape = (batch_size, n_heads, query_len, head_dim)\n",
    "        # K.shape = (batch_size, n_heads, key_len, head_dim)\n",
    "        # V.shape = (batch_size, n_heads, value_len, head_dim)\n",
    "\n",
    "        # 每個 heads 分別做 Q,K 內積        \n",
    "        scaled_attention_logits = torch.einsum(\"ijkl,ijml->ijkm\", [Q, K]) / self.scale\n",
    "        # scaled_attention_logits.shape = (batch_size, n_heads, query_len, key_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_attention_logits = scaled_attention_logits.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        # 得到每個 heads 的 self-attention matrix\n",
    "        attention_weights = torch.softmax(scaled_attention_logits, dim=-1)       \n",
    "        # attention_weights.shape = (batch_size, n_heads, query_len, key_len)\n",
    "                \n",
    "        output = torch.matmul(self.dropout(attention_weights), V)\n",
    "        # output.shape = (batch_size, n_heads, query_len, head_dim)\n",
    "        \n",
    "        output = output.permute(0, 2, 1, 3).contiguous()\n",
    "        # output.shape = (batch_size, query_len, n_heads, head_dim)\n",
    "        \n",
    "        # concat 所有 heads\n",
    "        output = output.view(batch_size, -1, self.d_model)\n",
    "        # output.shape = (batch_size, query_len, d_model)\n",
    "        \n",
    "        output = self.fc_out(output)\n",
    "        # output.shape = (batch_size, query_len, d_model)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f6GbXYBvObj"
   },
   "source": [
    "## d. Point-wise feed forward network\n",
    "\n",
    "通過兩個全連接層：\n",
    "\n",
    "* 第一層 $W_1\\in R^{pf_{dim}\\times d_{model}}$ : 將 $d_{model}$ 變為 $pf_{dim}$。\n",
    "\n",
    "* 第二層 $W_2\\in R^{d_{model}\\times pf_{dim}}$ : 將 $pf_{dim}$ 變回 $d_{model}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mJ8qPh23p0Ll"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, d_model, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(d_model, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x.shape = (batch_size, seq_len, d_model)\n",
    "        Output:\n",
    "            x.shape = (batch_size, seq_len, d_model)\n",
    "        \"\"\"   \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVhEz_hrqjjt"
   },
   "source": [
    "## e. Decoder\n",
    "\n",
    "* `Decoder` 的輸入與輸出分別是什麼:\n",
    "\n",
    "  * **輸入**: Encoder output 與英文句子 (`target sentence`)，一開始在預測時是不會有真實答案的，所以會在中文句子開頭加上 `<BOS>` token 形成 `target sentence`(`shifted right`)，後來使用 `Auto regressive` 的方式進行預測，每當預測完一個詞，就將該預測詞與原始輸入詞接在一起再輸入給模型直到預測出 `<EOS>` 為止。\n",
    "  * **輸出**: 英文句子 (`target sentence`)，使用 `Auto regressive` 方式來預測，例如: 輸入 `<BOS>`，預測 $p_1$；輸入 $p_1$，預測 $p_2$，直到預測出 `<EOS>` 為止。\n",
    "\n",
    "* `Masked multi-head attention`\n",
    "  * Masked self-attention，後面需要觀察的 attention weight matrix\n",
    "  * 使用 trg_mask，讓 decoder 輸入只能往前看\n",
    "\n",
    "    source mask 前面談過，這裡來談談 target mask：\n",
    "    \n",
    "    這個在 (masked) self-attention 會使用到，簡單來說就是不讓當前的字去注意到之後字，如下圖。每個詞只能夠往前注意，會這麼做的原因是因為 `Transformer` 在訓練時是一次將正確答案 (整個句子) 輸入給 Decoder，因為預測時是一個詞一個詞依序往後預測，所以不能讓模型先看到答案後面的詞。\n",
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src='https://drive.google.com/uc?export=view&id=1-v6WsgedF7s7ooo8xIm2sVfhL5lF_zvK' width=\"300\"/>\n",
    "<figcaption>Masked attention matrix</figcaption></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3943,
     "status": "ok",
     "timestamp": 1626971888274,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "nKUoppjQbpUx",
    "outputId": "6df970bc-298b-46fb-cade-f38a5229bee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trg_pad_mask: tensor([[[ True,  True,  True, False, False]]])\n",
      "tensor([[ True, False, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True]])\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False],\n",
       "         [ True,  True, False, False, False],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [ True,  True,  True, False, False]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target mask example\n",
    "trg_sample = torch.ByteTensor([[5,4,3,1,1]])\n",
    "trg_pad_idx = 1\n",
    "\n",
    "def make_trg_mask(trg):\n",
    "    # trg.shape = (batch_size, trg_len)   \n",
    "    trg_pad_mask = (trg != trg_pad_idx).unsqueeze(1).to(device)\n",
    "    print('trg_pad_mask:', trg_pad_mask)\n",
    "    # trg_pad_mask.shape = (batch_size, 1, trg_len)\n",
    "        \n",
    "    trg_len = trg.shape[1]\n",
    "    # 製造上三角矩陣\n",
    "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()\n",
    "    print(trg_sub_mask)\n",
    "    # trg_sub_mask.shape = (trg_len, trg_len)\n",
    "            \n",
    "    trg_mask = trg_pad_mask & trg_sub_mask # AND operation: T & T = T, T & F = F, F & F = F\n",
    "    # trg_mask.shape = (batch_size, trg_len, trg_len)\n",
    "        \n",
    "    return trg_mask\n",
    "\n",
    "trg_mask = make_trg_mask(trg_sample)\n",
    "print('------------')\n",
    "trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "T90SPpy-qbt-"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, d_model, n_layers, n_heads, pf_dim, dropout, max_length=100):\n",
    "        super().__init__()\n",
    "        # output_dim = dictionary size of the trg language\n",
    "        self.tok_embedding = nn.Embedding(output_dim, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_length, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            trg.shape = (batch_size, trg_len)\n",
    "            enc_src.shape = (batch_size, src_len, d_model)\n",
    "            trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, trg_len, output_dim)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"   \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(device)                    \n",
    "        # pos.shape = (batch_size, trg_len)\n",
    "            \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))        \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention_weights = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            # trg.shape = (batch_size, trg_len, d_model)\n",
    "            # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        # output.shape = (batch_size, trg_len, output_dim)    \n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "KksYEetwqb2c"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.ff_layer_norm = nn.LayerNorm(d_model)\n",
    "        self.self_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(d_model, n_heads, dropout)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(d_model, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            trg.shape = (batch_size, trg_len, d_model)\n",
    "            enc_src.shape = (batch_size, src_len, d_model)\n",
    "            trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "            src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        Output:\n",
    "            trg.shape = (batch_size, trg_len, d_model)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"  \n",
    "        # self-attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))    \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "            \n",
    "        # encoder attention\n",
    "        _trg, attention_weights = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))            \n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        # trg.shape = (batch_size, trg_len, d_model)\n",
    "        \n",
    "        return trg, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anYr1sJh2Ahq"
   },
   "source": [
    "## g. Transformer\n",
    "將編碼器 (Encoder) 與解碼器 (Decoder) 結合起來成為 Transformer：\n",
    "\n",
    "  * **輸入**: 德文句子 (`target sentence`) 與英文句子 (`target sentence`)。\n",
    "  * **輸出**: word embedded 的輸出句子以及每個 head 英文對德文句子的注意力矩陣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fioZQwQdqoeW"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        # src.shape = (batch_size, src_len)\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        # trg.shape = (batch_size, trg_len)\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # trg_pad_mask.shape = (batch_size, 1, 1, trg_len)\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        # # 製造上三角矩陣\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()\n",
    "        # trg_sub_mask.shape = (trg_len, trg_len)\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask # AND operation: T & T = T, T & F = F, F & F = F\n",
    "        # trg_mask.shape = (batch_size, 1, trg_len, trg_len)\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            src.shape = (batch_size, src_len)\n",
    "            trg.shape = (batch_size, trg_len)\n",
    "        Output:\n",
    "            output.shape = (batch_size, trg_len, output_dim)\n",
    "            attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \"\"\"     \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        # src_mask.shape = (batch_size, 1, 1, src_len)\n",
    "        # trg_mask.shape = (batch size, 1, trg_len, trg_len)\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        # enc_src.shape = (batch_size, src_len, d_model)\n",
    "                \n",
    "        output, attention_weights = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        # output.shape = (batch_size, trg_len, output_dim)\n",
    "        # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5TjPmxvzKkq"
   },
   "source": [
    "# 建模的另一種方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "BQOLXl-vzJw-"
   },
   "outputs": [],
   "source": [
    "# Reference: https://andrewpeng.dev/transformer-pytorch/\n",
    "# pytorch_TR = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, trans_dropout)\n",
    "# output = pytorch_TR(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_key_padding_mask, \n",
    "#                    tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuBz0w3Kqysn"
   },
   "source": [
    "# 訓練模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBb-d8fXmV8J"
   },
   "source": [
    "## a. 超參數設定與建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1TG1-_zHqojR"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "D_MODEL = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "N_EPOCHS = 15\n",
    "CLIP = 1\n",
    "\n",
    "# 建立 encoder 和 decoder class\n",
    "enc = Encoder(INPUT_DIM, D_MODEL, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, D_MODEL, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT)\n",
    "\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token] # PAD_IDX=1\n",
    "\n",
    "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1626971899422,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "Gcx2KKyuqoop",
    "outputId": "a2457abf-d53d-48f8-9b7c-e05e5756b471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (tok_embedding): Embedding(4, 256)\n",
      "    (pos_embedding): Embedding(100, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): EncoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (tok_embedding): Embedding(5, 256)\n",
      "    (pos_embedding): Embedding(100, 256)\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): DecoderLayer(\n",
      "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (self_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder_attention): MultiHeadAttentionLayer(\n",
      "          (wq): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wk): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (fc_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
      "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
      "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=256, out_features=5, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 4,008,453 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "        \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "print(model.apply(init_weights))\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ibc1pj62GzT-"
   },
   "source": [
    "## b. 選擇優化器與損失函數\n",
    "這邊使用分類任務的損失函數`CrossEntropyLoss`。但部分句子為因為 `padding` 而有許多的 `1`，但是那並不是真實的標籤，所以必須忽略 `padding` 位置的損失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qxdJojzaJspa"
   },
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    '''\n",
    "    A wrapper class for optimizer \n",
    "    From https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Optim.py\n",
    "    '''\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1IyVu2asq6d9"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "optimizer = ScheduledOptim(torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.98), eps=1e-09),\n",
    "                           d_model=D_MODEL, n_warmup_steps=4000)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_T-O_UDLa5u"
   },
   "source": [
    "## c. 定義訓練與驗證迴圈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "DG2rhg1tq6kV"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip):  \n",
    "    # train mode\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        # src.shape = (batch_size, src_len)\n",
    "        \n",
    "        trg = batch.trg\n",
    "        # trg.shape = (batch_size, trg_len)\n",
    "        \n",
    "        # 梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(src, trg[:,:-1]) # full teacher forcing\n",
    "        # output.shape = (batch_size, trg_len-1, output_dim)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "        # output.shape = ((trg_len-1) * batch_ize, output_dim)\n",
    "        # trg.shape = ((trg_len-1) * batch_size)\n",
    "        \n",
    "        # 計算 loss\n",
    "        loss = criterion(output, trg) # outputs by default are from logits; trg no need to do one-hot encoding\n",
    "        # 反向傳播，計算梯度\n",
    "        loss.backward()\n",
    "        # 做 regularization，使得整體梯度 norm 不超過 1，以防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        # 更新優化器\n",
    "        optimizer.step_and_update_lr()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ofL2Y0iRq-sM"
   },
   "outputs": [],
   "source": [
    "def evaluate_epoch(model, iterator, criterion):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            # src.shape = (src_len, batch_size)\n",
    "            \n",
    "            trg = batch.trg\n",
    "            # trg.shape = (trg_len, batch_size)\n",
    "            \n",
    "            output, _ = model(src, trg[:,:-1]) # turn off teacher forcing\n",
    "            # output.shape = (trg_len, batch_size, output_dim)\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            # output.shape = ((trg_len-1) * batch_ize, output_dim)\n",
    "            # trg.shape = ((trg_len-1) * batch_size)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "LIDSwaOYq-uQ"
   },
   "outputs": [],
   "source": [
    "# 計算跑一個 Epoch 的時間\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    \n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "# 將 loss vs. Epoch 畫出來\n",
    "def showPlot(tr_points, va_points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(tr_points, label='train loss')\n",
    "    plt.plot(va_points, label='validation loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "qoI4hMTSq-0h"
   },
   "outputs": [],
   "source": [
    "def train(model, train_iterator, valid_iterator, optimizer, criterion):\n",
    "    best_valid_loss = float('inf')\n",
    "    plot_tr_loss = []\n",
    "    plot_va_loss = []\n",
    "    for epoch in range(N_EPOCHS):\n",
    "    \n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "        valid_loss = evaluate_epoch(model, valid_iterator, criterion)\n",
    "        plot_tr_loss.append(train_loss)\n",
    "        plot_va_loss.append(valid_loss)\n",
    "        end_time = time.time()\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            # 儲存模型 (只存權重)\n",
    "            torch.save(model.state_dict(), 'SavedModel/tr-model_0720.pt')\n",
    "    \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        # PPL 是 perplexity 的縮寫，基本上就是 cross-entropy 指數化；其值越小越好 (minimize probability likelyhood)\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    showPlot(plot_tr_loss, plot_va_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "D7M1IJOrq6rG",
    "outputId": "76845460-a0af-4c72-9644-cf97100e7591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8])\n",
      "torch.Size([2, 1, 1, 8])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'Example' and 'Example'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-d0f14dae6647>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-4fa4ee5fe4fc>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_iterator, valid_iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mplot_tr_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mplot_va_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-8e16ee32f2a1>\u001b[0m in \u001b[0;36mevaluate_epoch\u001b[1;34m(model, iterator, criterion)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages\\torchtext\\legacy\\data\\iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[1;31m# fast-forward if loaded from state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages\\torchtext\\legacy\\data\\iterator.py\u001b[0m in \u001b[0;36minit_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_random_state_this_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restored_from_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages\\torchtext\\legacy\\data\\iterator.py\u001b[0m in \u001b[0;36mcreate_batches\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m             self.batches = batch(self.data(), self.batch_size,\n\u001b[0m\u001b[0;32m    250\u001b[0m                                  self.batch_size_fn)\n\u001b[0;32m    251\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages\\torchtext\\legacy\\data\\iterator.py\u001b[0m in \u001b[0;36mdata\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;34m\"\"\"Return the examples in the dataset in order, sorted, or shuffled.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'Example' and 'Example'"
     ]
    }
   ],
   "source": [
    "train(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA3kUZzVrkhA"
   },
   "source": [
    "# 推論: Seq2seq 德翻英翻譯機"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yO02ygIjrZaO"
   },
   "outputs": [],
   "source": [
    "def inference(sentence, src_field, trg_field, model, device, max_len=100):\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # 首先將想翻譯的句子轉成 tensor \n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('de_core_news_sm')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "    \n",
    "    # 句子頭尾加上 <BOS>, <EOS>\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    # tokens 透過字典轉成整數  \n",
    "    src_indices = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)\n",
    "    # src_tensor.shape = (batch_size, src_len)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_outputs = model.encoder(src_tensor, src_mask) \n",
    "        \n",
    "    trg_indices = [trg_field.vocab.stoi[trg_field.init_token]]\n",
    "    \n",
    "    # To record attention weights\n",
    "    #attn_plot = np.zeros((max_len, len(tokens)))\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device)\n",
    "        #print(trg_tensor.shape)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        #print(trg_mask.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, attention_weights = model.decoder(trg_tensor, encoder_outputs, trg_mask, src_mask) \n",
    "            # output.shape = (batch_size, trg_len, output_dim)\n",
    "            # attention_weights.shape = (batch_size, n_heads, trg_len, src_len)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:, -1].item()\n",
    "        trg_indices.append(pred_token)\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    \n",
    "    # 預測出來的'整數們'轉回 tokens\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indices]\n",
    "    attention = attention_weights.squeeze(0).detach().cpu().numpy()\n",
    "    # attention.shape = (n_heads, trg_len, srg_len)\n",
    "    \n",
    "    # 只回傳除了 <BOS> 以外的 tokens與attention weights\n",
    "    return trg_tokens[1:], attention         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhg0GZuuPgBh"
   },
   "source": [
    "* 舉個例子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1626972111647,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "wMUEWxUbrhv3",
    "outputId": "b04b7cca-0ca8-4066-ed3d-3478fb3ab1c7"
   },
   "outputs": [],
   "source": [
    "example_idx = 12\n",
    "src = vars(test_data.examples[example_idx])['src']\n",
    "trg = vars(test_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'trg = {trg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1626972115820,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "SMIrXon7rh0Y",
    "outputId": "54edaa0b-6a65-48de-f03f-797e1e6c5c69"
   },
   "outputs": [],
   "source": [
    "translation, attention = inference(src, SRC, TRG, model, device)\n",
    "\n",
    "print(f'predicted trg = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-YbwWWXrh5l"
   },
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence, n_heads=8, n_rows=4, n_cols=2):\n",
    "    assert n_rows * n_cols == n_heads\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,25))\n",
    "    #print(attention.shape)\n",
    "    for i in range(n_heads):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
    "        _attention = attention[i]\n",
    "\n",
    "        cax = ax.matshow(_attention, cmap='viridis')\n",
    "\n",
    "        ax.tick_params(labelsize=12)\n",
    "        x_loc = [i for i in range(len(sentence)+2)]\n",
    "        ax.set_xticks(x_loc)\n",
    "        ax.set_xticklabels(['<bos>']+[t.lower() for t in sentence]+['<eos>'], rotation=90)\n",
    "        y_loc = [j for j in range(len(predicted_sentence))]\n",
    "        ax.set_yticks(y_loc)\n",
    "        ax.set_yticklabels(predicted_sentence)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1739,
     "status": "ok",
     "timestamp": 1626972222521,
     "user": {
      "displayName": "Nelson Tsai",
      "photoUrl": "",
      "userId": "13165480496353898706"
     },
     "user_tz": -480
    },
    "id": "Wld8beUGrZfT",
    "outputId": "b0a8b39c-ed65-4629-b73a-1f4e9b01dfab"
   },
   "outputs": [],
   "source": [
    "plot_attention(attention, src, translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJAlQCSTqb9A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transformer_v1_colab_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
