{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accomplished-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import Tree\n",
    "import networkx as nx\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_trf\")\n",
    "# nlp = spacy.load(\"zh_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "separated-bride",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:\n",
      "[\"('吳宇森', 'nsubj')\", \"('導演', 'acl')\", \"('祕門', 'dobj')\", \"('殮房', 'nmod:assmod')\", \"('神探', 'nmod:assmod')\"]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"電影裡讓我直接想起來的還有——吳宇森導演的《辣手神探》的殮房的祕門啊\")\n",
    "entity1 = \"吳宇森\"\n",
    "entity2 = \"神探\"\n",
    "predicate = \"導演\"\n",
    "\n",
    "\n",
    "\n",
    "print('sentence:'.format(doc))\n",
    "# Load spacy's dependency tree into a networkx graph\n",
    "edges = []\n",
    "edges_and_dependencies = []\n",
    "edges_list = []\n",
    "dependencies_list = []\n",
    "index_1 = []\n",
    "index_2 = []\n",
    "index_3 = []\n",
    "\n",
    "for token in doc:\n",
    "    for child in token.children:\n",
    "        # append token to construct graph\n",
    "        edges.append(('{0}'.format(token.lower_),\n",
    "                      '{0}'.format(child.lower_)))\n",
    "        edges_and_dependencies.append(('{0}'.format((token.lower_, token.dep_)),\n",
    "                      '{0}'.format((child.lower_, child.dep_))))\n",
    "    \n",
    "    # append data for graph searchin\n",
    "    edges_list.append(token)\n",
    "    dependencies_list.append(token.dep_)\n",
    "        \n",
    "        \n",
    "graph = nx.Graph(edges)\n",
    "graph_test = nx.Graph(edges_and_dependencies)\n",
    "\n",
    "# print(edges_list)\n",
    "# print(dependencies_list)\n",
    "\n",
    "\n",
    "for index, element in enumerate(edges_list):\n",
    "    if str(element) == entity1:\n",
    "        index_1.append(index)\n",
    "    elif str(element) == entity2:\n",
    "        index_2.append(index)\n",
    "    elif str(element) == predicate:\n",
    "        index_3.append(index)\n",
    "        \n",
    "\n",
    "\n",
    "if len(index_1) == len(index_2) == len(index_3)== 1:\n",
    "    e_1 = \"(\\'\" + str(edges_list[index_1[0]]) + \"\\', \\'\" + dependencies_list[index_1[0]] + \"\\')\"\n",
    "    e_2 = \"(\\'\" + str(edges_list[index_2[0]]) + \"\\', \\'\" + dependencies_list[index_2[0]] + \"\\')\"\n",
    "    e_3 = \"(\\'\" + str(edges_list[index_3[0]]) + \"\\', \\'\" + dependencies_list[index_3[0]] + \"\\')\"\n",
    "    shortest_path_list = nx.shortest_path(graph_test, source=e_1, target=e_3) + nx.shortest_path(graph_test, source=e_3, target=e_2)[1:]\n",
    "else:\n",
    "    for indexA in index_1:\n",
    "        for indexB in index_2:\n",
    "            for indexC in index_3:\n",
    "                e_1 = \"(\\'\" + str(edges_list[indexA]) + \"\\', \\'\" + dependencies_list[indexA] + \"\\')\"\n",
    "                e_2 = \"(\\'\" + str(edges_list[indexB]) + \"\\', \\'\" + dependencies_list[indexB] + \"\\')\"\n",
    "                e_3 = \"(\\'\" + str(edges_list[indexC]) + \"\\', \\'\" + dependencies_list[indexC] + \"\\')\"\n",
    "                shortest_path_list = nx.shortest_path(graph_test, source=e_1, target=e_2)[1:]\n",
    "        \n",
    "print(shortest_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "humanitarian-survivor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('吳宇森', 'nsubj') <class 'str'>\n",
      "[\"('吳宇森', 'nsubj')\", \"('導演', 'acl')\", \"('祕門', 'dobj')\", \"('殮房', 'nmod:assmod')\", \"('神探', 'nmod:assmod')\"]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"電影裡讓我直接想起來的還有——吳宇森導演的《辣手神探》的殮房的祕門啊\")\n",
    "entity1 = '吳宇森'\n",
    "entity2 = '神探'\n",
    "\n",
    "\n",
    "def find_element_nested_list(element, list_source, index_list, index=\"start\"):\n",
    "    # find element recursively\n",
    "    for the_index, subList in enumerate(list_source):\n",
    "        if type(subList) is not list:\n",
    "            if list_source[the_index] == element:\n",
    "                if index != \"start\": index_list.append(str(index))\n",
    "                index_list.append(str(the_index))\n",
    "                index_list.append(\"@\")\n",
    "        else:\n",
    "            find_element_nested_list(element, subList, index_list, str(the_index))\n",
    "\n",
    "    index_result = \"-\".join(index_list)\n",
    "    index_result = re.split(\"|\".join([\"-@-\", \"-@\"]), index_result)[:-1]\n",
    "\n",
    "    return index_result\n",
    "\n",
    "\n",
    "edges = []\n",
    "edges_look_up = []\n",
    "edges_and_dependencies_look_up = []\n",
    "edges_and_dependencies = []\n",
    "\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    for child in token.children:\n",
    "        edges_look_up.append([token.lower_,\n",
    "                                      child.lower_])\n",
    "        edges_and_dependencies_look_up.append([(token.lower_, token.dep_),\n",
    "                                                       (child.lower_, child.dep_)])\n",
    "\n",
    "        edges.append(('{0}'.format(token.lower_),\n",
    "                              '{0}'.format(child.lower_)))\n",
    "        edges_and_dependencies.append(('{0}'.format((token.lower_, token.dep_)),\n",
    "                                               '{0}'.format((child.lower_, child.dep_))))\n",
    "\n",
    "graph = nx.Graph(edges)\n",
    "graph_test = nx.Graph(edges_and_dependencies)\n",
    "        # Get the length and path\n",
    "\n",
    "first_entity_index_list = find_element_nested_list(entity1, edges_look_up, [])\n",
    "second_entity_index_list = find_element_nested_list(entity2, edges_look_up, [])\n",
    "\n",
    "e_1 = edges_and_dependencies[int(first_entity_index_list[0].split(\"-\")[0])][\n",
    "            int(first_entity_index_list[0].split(\"-\")[1])]\n",
    "e_2 = edges_and_dependencies[int(second_entity_index_list[0].split(\"-\")[0])][\n",
    "            int(second_entity_index_list[0].split(\"-\")[1])]\n",
    "\n",
    "print(e_1, type(e_1))\n",
    "\n",
    "shortest_path_list = nx.shortest_path(graph_test, source=e_1, target=e_2)\n",
    "\n",
    "print(shortest_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(edges_look_up)\n",
    "\n",
    "\n",
    "# first_entity_index_list = find_element_nested_list(entity1, edges_look_up, [])\n",
    "# second_entity_index_list = find_element_nested_list(entity2, edges_look_up, [])\n",
    "# e_1 = edges_and_dependencies[int(first_entity_index_list[0].split(\"-\")[0])][int(first_entity_index_list[0].split(\"-\")[1])]\n",
    "# e_2 = edges_and_dependencies[int(second_entity_index_list[0].split(\"-\")[0])][int(second_entity_index_list[0].split(\"-\")[1])]\n",
    "\n",
    "# print(e_1, e_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monpa",
   "language": "python",
   "name": "monpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
