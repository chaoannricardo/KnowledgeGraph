{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "legislative-bidder",
   "metadata": {},
   "source": [
    "# Jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "# devide token by seperators\n",
    "total_entity_list = []\n",
    "sentence_entity_list = []\n",
    "total_label_list = []\n",
    "sentence_label_list = []\n",
    "\n",
    "for index, entityElement in enumerate(dataToken.iloc[:, 0]):\n",
    "    if entityElement not in [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\"]:\n",
    "        sentence_entity_list.append(entityElement)\n",
    "        sentence_label_list.append(dataToken.iloc[index, 1])\n",
    "    else:\n",
    "        total_entity_list.append(sentence_entity_list)\n",
    "        total_label_list.append(sentence_label_list)\n",
    "        sentence_entity_list = []\n",
    "        sentence_label_list = []\n",
    "    \n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list))\n",
    "\n",
    "\n",
    "# Clean up tokens and eliminate unneccesary tokens\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    indexPunct_0 = 0\n",
    "    indexPunct_1 = 0\n",
    "    for tokenIndex, tokens in enumerate(sentences):\n",
    "        # set up element indexes that are between two punctuation\n",
    "        if total_label_list[sentenceIndex][tokenIndex] in [\"x\"]:\n",
    "            if indexPunct_0 == 0:\n",
    "                indexPunct_0 = tokenIndex\n",
    "            else:\n",
    "                indexPunct_1 = tokenIndex\n",
    "    # remove tokens that are between two punctuations\n",
    "    if indexPunct_1 == 0:\n",
    "        total_entity_list[sentenceIndex][indexPunct_1] = \"\"\n",
    "        total_label_list[sentenceIndex][indexPunct_1] = \"\"\n",
    "    else:\n",
    "        for removeIndex in range(indexPunct_0, indexPunct_1 + 1):\n",
    "            total_entity_list[sentenceIndex][removeIndex] = \"\"\n",
    "            total_label_list[sentenceIndex][removeIndex] = \"\"\n",
    "            \n",
    "    total_entity_list[sentenceIndex] = list(filter((\"\").__ne__, total_entity_list[sentenceIndex]))\n",
    "    total_label_list[sentenceIndex]  = list(filter((\"\").__ne__, total_label_list[sentenceIndex]))\n",
    "                \n",
    "print(total_entity_list[0] , total_label_list[0])\n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get entity pairs and relations\n",
    "\n",
    "relation_pos_possible_list = [\"v\"]\n",
    "\n",
    "entity_pos_possible_list = [\"nr\", \"n\"]\n",
    "\n",
    "\n",
    "# get relations\n",
    "all_relations_list = []\n",
    "all_relations_index = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    relation_list = []\n",
    "    relation_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1: \n",
    "            if total_label_list[sentenceIndex][tokenIndex] in relation_pos_possible_list:\n",
    "                relation_list.append(token)\n",
    "                relation_index_list.append(tokenIndex)\n",
    "    all_relations_list.append(relation_list)\n",
    "    all_relations_index.append(relation_index_list)\n",
    "\n",
    "\n",
    "# get entities\n",
    "all_reformatted_entities = []\n",
    "all_reformatted_index_list = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    entity_index_list = []\n",
    "    possible_entities = []\n",
    "    reformatted_entities = []\n",
    "    reformatted_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1:\n",
    "            if total_label_list[sentenceIndex][tokenIndex] in entity_pos_possible_list:\n",
    "                entity_index_list.append(tokenIndex)\n",
    "                possible_entities.append(token)\n",
    "    # combine token if situate next to each other\n",
    "#     print(entity_index_list, possible_entities)\n",
    "    if len(possible_entities) > 0:\n",
    "        combine_entity_name = possible_entities[0]\n",
    "        for possibleIndex, possibleElement in enumerate(entity_index_list):\n",
    "#             print(possible_entities)\n",
    "            \n",
    "            isContinuous = False\n",
    "            if possibleIndex != 0:\n",
    "                if possibleElement == entity_index_list[possibleIndex - 1] + 1:\n",
    "                    isContinuous = True\n",
    "                    combine_entity_name += possible_entities[possibleIndex]\n",
    "                else:\n",
    "                    isContinuous = False\n",
    "            \n",
    "                if isContinuous == False:\n",
    "                    reformatted_entities.append(combine_entity_name)\n",
    "                    reformatted_index_list.append(entity_index_list[possibleIndex-1])\n",
    "                    combine_entity_name = possible_entities[possibleIndex]\n",
    "                    \n",
    "            if possibleIndex == (len(entity_index_list) - 1):\n",
    "                reformatted_entities.append(combine_entity_name)\n",
    "                reformatted_index_list.append(possibleElement)\n",
    "                \n",
    "#             print(combine_entity_name)     \n",
    "#     print(reformatted_entities)\n",
    "    all_reformatted_entities.append(reformatted_entities)\n",
    "    all_reformatted_index_list.append(reformatted_index_list)\n",
    "                \n",
    "        \n",
    "# print(all_relations_list, len(all_relations_list), \"\\n\")\n",
    "# print(all_reformatted_entities, len(all_reformatted_entities), )\n",
    "print(len(all_relations_list), len(all_relations_index), len(all_reformatted_entities), len(all_reformatted_index_list))\n",
    "\n",
    "# print out parsing result\n",
    "for index, element in enumerate(all_reformatted_entities):\n",
    "    print(element, all_reformatted_index_list[index], all_relations_list[index], all_relations_index[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monpa",
   "language": "python",
   "name": "monpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
