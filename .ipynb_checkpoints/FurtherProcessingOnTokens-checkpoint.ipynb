{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "authentic-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "generous-trustee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Windows_Storage\\Storage\\Github\\KnowledgeGraph\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "limiting-treasury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7547\n",
      "  Segmented Element  POS\n",
      "0              第二十三  Neu\n",
      "1                 章   Na\n",
      "2                     FW\n",
      "3               半導體   Na\n",
      "4                製造   VC\n",
      "5                概論   Na\n",
      "6                     FW\n",
      "7                     FW\n",
      "8                     FW\n",
      "9                     FW\n"
     ]
    }
   ],
   "source": [
    "# read in tokens\n",
    "\n",
    "dataToken = pd.read_csv(\"./results/第二十三章半導體製造概論_monpaResult.csv\", encoding=\"utf8\")\n",
    "\n",
    "# append a column for data if dependencies column does not exist (monpa case)\n",
    "if len(dataToken.columns) < 2:\n",
    "    dataToken.loc[:, \"Depe\"] = \"\"\n",
    "\n",
    "print(len(dataToken))\n",
    "print(dataToken.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "atlantic-principal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list length:  24 24 24\n",
      "['半導', '體工', '業常', '用', '的', '化學', '試劑', '可', '分為', '酸'] ['NOUN', 'NOUN', 'NOUN', 'VERB', 'PART', 'NOUN', 'NOUN', 'VERB', 'VERB', 'NOUN'] ['compound:nn', 'compound:nn', 'dep', 'acl', 'mark', 'compound:nn', 'nsubj', 'aux:modal', 'ROOT', 'conj']\n",
      "list length:  24 24 24\n"
     ]
    }
   ],
   "source": [
    "# devide token by seperators\n",
    "total_entity_list = []\n",
    "sentence_entity_list = []\n",
    "total_label_list = []\n",
    "sentence_label_list = []\n",
    "total_dependencies_list = []\n",
    "sentence_dependencies_list = []\n",
    "\n",
    "for index, entityElement in enumerate(dataToken.iloc[:, 0]):\n",
    "    if entityElement not in [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\"]:\n",
    "        sentence_entity_list.append(entityElement)\n",
    "        sentence_label_list.append(dataToken.iloc[index, 1])\n",
    "        sentence_dependencies_list.append(dataToken.iloc[index, 2])\n",
    "    else:\n",
    "        total_entity_list.append(sentence_entity_list)\n",
    "        total_label_list.append(sentence_label_list)\n",
    "        total_dependencies_list.append(sentence_dependencies_list)\n",
    "        sentence_entity_list = []\n",
    "        sentence_label_list = []\n",
    "        sentence_dependencies_list = []\n",
    "    \n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))\n",
    "\n",
    "\n",
    "# Clean up tokens and eliminate unneccesary tokens\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    indexPunct_0 = 0\n",
    "    indexPunct_1 = 0\n",
    "    for tokenIndex, tokens in enumerate(sentences):\n",
    "        # set up element indexes that are between two punctuation\n",
    "        if total_label_list[sentenceIndex][tokenIndex] in [\"PUNCT\"]:\n",
    "            if indexPunct_0 == 0:\n",
    "                indexPunct_0 = tokenIndex\n",
    "            else:\n",
    "                indexPunct_1 = tokenIndex\n",
    "    # remove tokens that are between two punctuations\n",
    "    if indexPunct_1 == 0:\n",
    "        total_entity_list[sentenceIndex][indexPunct_1] = \"\"\n",
    "        total_label_list[sentenceIndex][indexPunct_1] = \"\"\n",
    "        total_dependencies_list[sentenceIndex][indexPunct_1] = \"\"\n",
    "    else:\n",
    "        for removeIndex in range(indexPunct_0, indexPunct_1 + 1):\n",
    "            total_entity_list[sentenceIndex][removeIndex] = \"\"\n",
    "            total_label_list[sentenceIndex][removeIndex] = \"\"\n",
    "            total_dependencies_list[sentenceIndex][removeIndex] = \"\"\n",
    "            \n",
    "    total_entity_list[sentenceIndex] = list(filter((\"\").__ne__, total_entity_list[sentenceIndex]))\n",
    "    total_label_list[sentenceIndex]  = list(filter((\"\").__ne__, total_label_list[sentenceIndex]))\n",
    "    total_dependencies_list[sentenceIndex] = list(filter((\"\").__ne__, total_dependencies_list[sentenceIndex]))\n",
    "                \n",
    "print(total_entity_list[0] , total_label_list[0], total_dependencies_list[0])\n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ancient-zambia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 24 24 24\n",
      "['半導體工業常', '化學試劑', '酸'] [2, 6, 9] ['用', '可', '分為'] [3, 7, 8]\n",
      "['生產過程'] [4] ['用於', '過程'] [0, 4]\n",
      "['IC技術', '發展'] [1, 4] ['發展'] [4]\n",
      "['當今', '線寬', '.10', '奈米電子時代'] [0, 3, 8, 13] ['邁進', '小', '於', '0'] [2, 4, 5, 6]\n",
      "['矽晶圓單晶拋光片', '表面加工質量', '要求', '來'] [3, 7, 9, 12] ['高'] [14]\n",
      "['學試劑', '技術要求'] [5, 11] ['用', '提出', '高'] [2, 6, 8]\n",
      "['其顆粒', '雜質', '含量', '量級'] [1, 3, 5, 14] ['一般', '要', '減少'] [7, 8, 9]\n",
      "['其包裝', '要求'] [2, 6] [] []\n",
      "['矽晶圓片', '化學清洗製程', '度化學試劑', '標準', '技術指標見表4.2-2'] [1, 5, 12, 14, 21] ['用', '指標', '所示'] [7, 18, 22]\n",
      "['標準'] [1] ['標準'] [1]\n",
      "['標準', '試劑', '1.2', '線寬電路生產'] [1, 3, 6, 12] ['適用', '生產'] [4, 12]\n",
      "['標準', '試劑', '0.6', '0.2', '線寬電路製程'] [1, 3, 6, 8, 12] ['適用', '製程'] [4, 12]\n",
      "['標準', '試劑', '0.2', '0.', '線寬電路製程'] [1, 3, 6, 9, 14] ['適用', '製程'] [4, 14]\n",
      "['導體工廠', '化學試劑'] [3, 6] ['用', '於半', '工廠', '用', '較大'] [0, 1, 3, 7, 10]\n",
      "['系統', '污染'] [2, 4] ['避免'] [1]\n",
      "['中央系統管理', '供應方式', '化學試劑', '供給系統'] [3, 6, 9, 12] ['採用'] [0]\n",
      "['酸材質', '雙層管路系統'] [1, 5] [] []\n",
      "['內層鹼化學試劑'] [3] [] []\n",
      "['外層', '材質管'] [0, 6] ['採用', '透明'] [2, 3]\n",
      "['監視', '用', '內外管', '氮氣'] [1, 3, 7, 14] ['供泄漏', '外管', '通入', '經', '稀釋'] [0, 7, 10, 11, 12]\n",
      "['廢氣', '排放管路系統'] [1, 5] ['至'] [0]\n",
      "['機溶劑常', '不銹鋼管', '輸送管路'] [5, 8, 11] ['用', '用', '作為'] [1, 6, 9]\n",
      "['於酸', '鹼化學試劑', '機溶劑會對人體', '傷害'] [0, 3, 9, 14] ['體', '造成', '大'] [9, 10, 12]\n",
      "['安全輸送系統', '泄漏監視系統'] [6, 11] ['故應', '有', '嚴格', '中應', '配置'] [0, 1, 2, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# get entity pairs and relations\n",
    "\n",
    "relation_dependencies_possible_list = [\"ROOT\", \"nmod:prep\", \"prep\", \"agent\", ]\n",
    "relation_pos_possible_list = [\"VERB\"]\n",
    "\n",
    "entity_dependencies_possible_list = [\"compound:nn\", \"nsubj\", \"dep\", \"dobj\"]\n",
    "entity_pos_possible_list = [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "\n",
    "# get relations\n",
    "all_relations_list = []\n",
    "all_relations_index = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    relation_list = []\n",
    "    relation_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1: \n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in relation_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in relation_pos_possible_list:\n",
    "                relation_list.append(token)\n",
    "                relation_index_list.append(tokenIndex)\n",
    "    all_relations_list.append(relation_list)\n",
    "    all_relations_index.append(relation_index_list)\n",
    "\n",
    "\n",
    "# get entities\n",
    "all_reformatted_entities = []\n",
    "all_reformatted_index_list = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    entity_index_list = []\n",
    "    possible_entities = []\n",
    "    reformatted_entities = []\n",
    "    reformatted_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1:\n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in entity_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in entity_pos_possible_list:\n",
    "                entity_index_list.append(tokenIndex)\n",
    "                possible_entities.append(token)\n",
    "    # combine token if situate next to each other\n",
    "#     print(entity_index_list, possible_entities)\n",
    "    if len(possible_entities) > 0:\n",
    "        combine_entity_name = possible_entities[0]\n",
    "        for possibleIndex, possibleElement in enumerate(entity_index_list):\n",
    "#             print(possible_entities)\n",
    "            \n",
    "            isContinuous = False\n",
    "            if possibleIndex != 0:\n",
    "                if possibleElement == entity_index_list[possibleIndex - 1] + 1:\n",
    "                    isContinuous = True\n",
    "                    combine_entity_name += possible_entities[possibleIndex]\n",
    "                else:\n",
    "                    isContinuous = False\n",
    "            \n",
    "                if isContinuous == False:\n",
    "                    reformatted_entities.append(combine_entity_name)\n",
    "                    reformatted_index_list.append(entity_index_list[possibleIndex-1])\n",
    "                    combine_entity_name = possible_entities[possibleIndex]\n",
    "                    \n",
    "            if possibleIndex == (len(entity_index_list) - 1):\n",
    "                reformatted_entities.append(combine_entity_name)\n",
    "                reformatted_index_list.append(possibleElement)\n",
    "                \n",
    "#             print(combine_entity_name)     \n",
    "#     print(reformatted_entities)\n",
    "    all_reformatted_entities.append(reformatted_entities)\n",
    "    all_reformatted_index_list.append(reformatted_index_list)\n",
    "                \n",
    "        \n",
    "# print(all_relations_list, len(all_relations_list), \"\\n\")\n",
    "# print(all_reformatted_entities, len(all_reformatted_entities), )\n",
    "print(len(all_relations_list), len(all_relations_index), len(all_reformatted_entities), len(all_reformatted_index_list))\n",
    "\n",
    "# print out parsing result\n",
    "for index, element in enumerate(all_reformatted_entities):\n",
    "    print(element, all_reformatted_index_list[index], all_relations_list[index], all_relations_index[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-queens",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monpa",
   "language": "python",
   "name": "monpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
