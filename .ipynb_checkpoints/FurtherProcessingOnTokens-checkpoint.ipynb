{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "practical-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "attractive-cathedral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n",
      "   Segmented Element           Label\n",
      "0              半導體工業             NaN\n",
      "1                  常               D\n",
      "2                  用              VC\n",
      "3                  的              DE\n",
      "4                 化學              Na\n",
      "5                 試劑              Na\n",
      "6                  可               D\n",
      "7                 分為              VG\n",
      "8           酸、鹼、有機溶劑             NaN\n",
      "9                  和               P\n",
      "10              有機物質             NaN\n",
      "11                 及             Caa\n",
      "12               氧化物             NaN\n",
      "13                 等             Cab\n",
      "14                 五             Neu\n",
      "15                 大              VH\n",
      "16                 類              VC\n",
      "17                 ，   COMMACATEGORY\n",
      "18                分別               D\n",
      "19                用於              VC\n",
      "20                不同              VH\n",
      "21                 的              DE\n",
      "22                生產              Nv\n",
      "23                過程              Na\n",
      "24                 中              Ng\n",
      "25                 。  PERIODCATEGORY\n",
      "26                隨著               P\n",
      "27                                FW\n",
      "28                IC              FW\n",
      "29                                FW\n"
     ]
    }
   ],
   "source": [
    "# dataToken = pd.read_csv(\"../0311_ChineseTokenization/result/jiebaResult_2.csv\", encoding=\"utf8\")\n",
    "dataToken = pd.read_csv(\"./data/第二十三章半導體製造概論.txt\", encoding=\"utf8\")\n",
    "# dataToken = pd.read_csv(\"../0311_ChineseTokenization/result/spacyResult_2.csv\", encoding=\"utf8\")\n",
    "\n",
    "print(len(dataToken))\n",
    "print(dataToken.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-light",
   "metadata": {},
   "source": [
    "# Monpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "noticed-savannah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list length:  24 24\n",
      "['常', '用', '的', '化學', '試劑', '可', '分為', '酸、鹼、有機溶劑', '和', '有機物質', '及', '氧化物', '等', '五', '大', '類'] ['D', 'VC', 'DE', 'Na', 'Na', 'D', 'VG', nan, 'P', nan, 'Caa', nan, 'Cab', 'Neu', 'VH', 'VC']\n",
      "list length:  24 24\n"
     ]
    }
   ],
   "source": [
    "# devide token by seperators\n",
    "total_entity_list = []\n",
    "sentence_entity_list = []\n",
    "total_label_list = []\n",
    "sentence_label_list = []\n",
    "\n",
    "for index, entityElement in enumerate(dataToken.iloc[:, 0]):\n",
    "    if entityElement not in [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\"]:\n",
    "        sentence_entity_list.append(entityElement)\n",
    "        sentence_label_list.append(dataToken.iloc[index, 1])\n",
    "    else:\n",
    "        total_entity_list.append(sentence_entity_list)\n",
    "        total_label_list.append(sentence_label_list)\n",
    "        sentence_entity_list = []\n",
    "        sentence_label_list = []\n",
    "    \n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list))\n",
    "\n",
    "\n",
    "# Clean up tokens and eliminate unneccesary tokens\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    indexPunct_0 = 0\n",
    "    indexPunct_1 = 0\n",
    "    for tokenIndex, tokens in enumerate(sentences):\n",
    "        # set up element indexes that are between two punctuation\n",
    "        if total_label_list[sentenceIndex][tokenIndex] in [\"PARENTHESISCATEGORY\", \"COMMACATEGORY\", \"PERIODCATEGORY\"]:\n",
    "            if indexPunct_0 == 0:\n",
    "                indexPunct_0 = tokenIndex\n",
    "            else:\n",
    "                indexPunct_1 = tokenIndex\n",
    "    # remove tokens that are between two punctuations\n",
    "    if indexPunct_1 == 0:\n",
    "        total_entity_list[sentenceIndex][indexPunct_1] = \"\"\n",
    "        total_label_list[sentenceIndex][indexPunct_1] = \"\"\n",
    "    else:\n",
    "        for removeIndex in range(indexPunct_0, indexPunct_1 + 1):\n",
    "            total_entity_list[sentenceIndex][removeIndex] = \"\"\n",
    "            total_label_list[sentenceIndex][removeIndex] = \"\"\n",
    "            \n",
    "    total_entity_list[sentenceIndex] = list(filter((\"\").__ne__, total_entity_list[sentenceIndex]))\n",
    "    total_label_list[sentenceIndex]  = list(filter((\"\").__ne__, total_label_list[sentenceIndex]))\n",
    "                \n",
    "print(total_entity_list[0] , total_label_list[0])\n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "minimal-property",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 24 24 24\n",
      "['化學試劑', '酸、鹼、有機溶劑', '有機物質', '氧化物'] [4, 7, 9, 11] ['用', '大', '類'] [1, 14, 15]\n",
      "['生產過程'] [4] ['用於', '不同'] [0, 1]\n",
      "['技術', '發展'] [3, 6] ['不斷'] [5]\n",
      "['線', '奈米電子時代'] [3, 10] ['寬小於'] [4]\n",
      "['矽晶圓單晶', '光片', '表面', '質量', '要求'] [2, 4, 6, 8, 10] ['拋', '來', '高'] [3, 13, 15]\n",
      "['純化學試劑', '技術要求'] [6, 11] ['用', '超高', '提出', '了更高'] [2, 4, 7, 8]\n",
      "['顆粒', '雜質', '量', '數量級'] [2, 4, 7, 19] ['含'] [6]\n",
      "['包裝', '要求'] [2, 14] ['儲', '運', '提出', '相應', '新'] [4, 6, 9, 11, 13]\n",
      "['圓片', '化學清洗製程', '純度化學試劑', '標準', '技術指標見表'] [0, 4, 11, 16, 21] ['用', '超高'] [6, 8]\n",
      "['標準'] [3] [] []\n",
      "['標準', '試劑', '線', '電路生產'] [2, 4, 13, 16] ['適用', '寬'] [5, 14]\n",
      "[' 標準', '試劑', '線', '電路製程'] [4, 6, 15, 18] ['適用', '寬'] [7, 16]\n",
      "[' 標準', '試劑', '線', '電路製程'] [5, 7, 16, 19] ['適用', '寬'] [8, 17]\n",
      "['半導體', '化學試劑', '量'] [0, 4, 6] ['大'] [8]\n",
      "['系統', '污染'] [3, 5] ['安全'] [1]\n",
      "['中央系統管理', '方式', '化學試劑', '系統'] [2, 5, 8, 11] ['採用'] [0]\n",
      "['酸', '鹼化學試劑', '輸送', '氟', '高分子材料', '材質', '雙層管路系統'] [0, 3, 5, 9, 12, 15, 19] ['採用', '含'] [7, 8]\n",
      "['材質管路輸送酸', '鹼化學試劑'] [5, 8] [] []\n",
      "['材質管'] [7] ['採用', '透明'] [1, 2]\n",
      "['泄漏', '外管', '氮氣'] [1, 8, 14] ['供', '監視', '用', '稀釋'] [0, 2, 4, 12]\n",
      "['廢氣', '排放管路系統'] [1, 5] ['接至'] [0]\n",
      "['溶劑', '鋼管', '輸送管路'] [4, 8, 12] ['用', '用', '不銹'] [1, 6, 7]\n",
      "['酸', '鹼化學試劑', '溶劑', '人體', '傷害'] [0, 3, 6, 9, 14] ['大'] [12]\n",
      "['安全', '防護措施', '裝備', '管路輸送系統', '監視系統'] [3, 6, 8, 12, 19] ['嚴格', '配置', '泄'] [1, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "# get entity pairs and relations\n",
    "\n",
    "relation_pos_possible_list = [\"VH\", \"VC\", \"VJ\"]\n",
    "\n",
    "entity_pos_possible_list = [\"Na\", \"Nv\"]\n",
    "\n",
    "\n",
    "# get relations\n",
    "all_relations_list = []\n",
    "all_relations_index = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    relation_list = []\n",
    "    relation_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1: \n",
    "            if total_label_list[sentenceIndex][tokenIndex] in relation_pos_possible_list:\n",
    "                relation_list.append(token)\n",
    "                relation_index_list.append(tokenIndex)\n",
    "    all_relations_list.append(relation_list)\n",
    "    all_relations_index.append(relation_index_list)\n",
    "\n",
    "\n",
    "# get entities\n",
    "all_reformatted_entities = []\n",
    "all_reformatted_index_list = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    entity_index_list = []\n",
    "    possible_entities = []\n",
    "    reformatted_entities = []\n",
    "    reformatted_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1:\n",
    "            if total_label_list[sentenceIndex][tokenIndex] in entity_pos_possible_list:\n",
    "                entity_index_list.append(tokenIndex)\n",
    "                possible_entities.append(token)\n",
    "            elif type(total_label_list[sentenceIndex][tokenIndex]) is float:\n",
    "                if math.isnan(total_label_list[sentenceIndex][tokenIndex]):\n",
    "                    entity_index_list.append(tokenIndex)\n",
    "                    possible_entities.append(token)\n",
    "                \n",
    "    # combine token if situate next to each other\n",
    "#     print(entity_index_list, possible_entities)\n",
    "    if len(possible_entities) > 0:\n",
    "        combine_entity_name = possible_entities[0]\n",
    "        for possibleIndex, possibleElement in enumerate(entity_index_list):\n",
    "#             print(possible_entities)\n",
    "            \n",
    "            isContinuous = False\n",
    "            if possibleIndex != 0:\n",
    "                if possibleElement == entity_index_list[possibleIndex - 1] + 1:\n",
    "                    isContinuous = True\n",
    "                    combine_entity_name += possible_entities[possibleIndex]\n",
    "                else:\n",
    "                    isContinuous = False\n",
    "            \n",
    "                if isContinuous == False:\n",
    "                    reformatted_entities.append(combine_entity_name)\n",
    "                    reformatted_index_list.append(entity_index_list[possibleIndex-1])\n",
    "                    combine_entity_name = possible_entities[possibleIndex]\n",
    "                    \n",
    "            if possibleIndex == (len(entity_index_list) - 1):\n",
    "                reformatted_entities.append(combine_entity_name)\n",
    "                reformatted_index_list.append(possibleElement)\n",
    "                \n",
    "#             print(combine_entity_name)     \n",
    "#     print(reformatted_entities)\n",
    "    all_reformatted_entities.append(reformatted_entities)\n",
    "    all_reformatted_index_list.append(reformatted_index_list)\n",
    "                \n",
    "        \n",
    "# print(all_relations_list, len(all_relations_list), \"\\n\")\n",
    "# print(all_reformatted_entities, len(all_reformatted_entities), )\n",
    "print(len(all_relations_list), len(all_relations_index), len(all_reformatted_entities), len(all_reformatted_index_list))\n",
    "\n",
    "# print out parsing result\n",
    "for index, element in enumerate(all_reformatted_entities):\n",
    "    print(element, all_reformatted_index_list[index], all_relations_list[index], all_relations_index[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-astronomy",
   "metadata": {},
   "source": [
    "# Jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "attached-bleeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list length:  24 24\n",
      "['半導體', '工業', '常用', '的', '化學', '試劑', '可', '分為', '酸'] ['n', 'n', 'b', 'uj', 'n', 'n', 'v', 'v', 'n']\n",
      "list length:  24 24\n"
     ]
    }
   ],
   "source": [
    "# devide token by seperators\n",
    "total_entity_list = []\n",
    "sentence_entity_list = []\n",
    "total_label_list = []\n",
    "sentence_label_list = []\n",
    "\n",
    "for index, entityElement in enumerate(dataToken.iloc[:, 0]):\n",
    "    if entityElement not in [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\"]:\n",
    "        sentence_entity_list.append(entityElement)\n",
    "        sentence_label_list.append(dataToken.iloc[index, 1])\n",
    "    else:\n",
    "        total_entity_list.append(sentence_entity_list)\n",
    "        total_label_list.append(sentence_label_list)\n",
    "        sentence_entity_list = []\n",
    "        sentence_label_list = []\n",
    "    \n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list))\n",
    "\n",
    "\n",
    "# Clean up tokens and eliminate unneccesary tokens\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    indexPunct_0 = 0\n",
    "    indexPunct_1 = 0\n",
    "    for tokenIndex, tokens in enumerate(sentences):\n",
    "        # set up element indexes that are between two punctuation\n",
    "        if total_label_list[sentenceIndex][tokenIndex] in [\"x\"]:\n",
    "            if indexPunct_0 == 0:\n",
    "                indexPunct_0 = tokenIndex\n",
    "            else:\n",
    "                indexPunct_1 = tokenIndex\n",
    "    # remove tokens that are between two punctuations\n",
    "    if indexPunct_1 == 0:\n",
    "        total_entity_list[sentenceIndex][indexPunct_1] = \"\"\n",
    "        total_label_list[sentenceIndex][indexPunct_1] = \"\"\n",
    "    else:\n",
    "        for removeIndex in range(indexPunct_0, indexPunct_1 + 1):\n",
    "            total_entity_list[sentenceIndex][removeIndex] = \"\"\n",
    "            total_label_list[sentenceIndex][removeIndex] = \"\"\n",
    "            \n",
    "    total_entity_list[sentenceIndex] = list(filter((\"\").__ne__, total_entity_list[sentenceIndex]))\n",
    "    total_label_list[sentenceIndex]  = list(filter((\"\").__ne__, total_label_list[sentenceIndex]))\n",
    "                \n",
    "print(total_entity_list[0] , total_label_list[0])\n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "simplified-robert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 24 24 24\n",
      "['半導體工業', '化學試劑', '酸'] [1, 5, 8] ['可', '分為'] [6, 7]\n",
      "['於', '過程'] [1, 6] [] []\n",
      "[] [] ['隨著', '斷'] [0, 1]\n",
      "['線', '於', '電子時代'] [4, 7, 10] ['邁進'] [3]\n",
      "['矽晶', '光片', '表面'] [1, 4, 6] ['拋'] [3]\n",
      "['化學試劑', '技術'] [7, 11] ['超高', '提出', '要求'] [4, 8, 12]\n",
      "[] [] ['要求'] [0]\n",
      "['新'] [6] ['包裝', '相應', '要求'] [3, 4, 7]\n",
      "['矽晶圓片', '化學'] [1, 3] ['清洗', '超高', '所示'] [4, 9, 11]\n",
      "['標準'] [1] [] []\n",
      "[] [] [] []\n",
      "[] [] [] []\n",
      "[] [] [] []\n",
      "['於半導體工廠', '化學試劑'] [3, 6] [] []\n",
      "['系統'] [3] ['避免'] [2]\n",
      "['中央系統', '方式', '化學試劑', '系統'] [3, 6, 9, 12] ['採用'] [1]\n",
      "['酸系統'] [1] [] []\n",
      "['內層', '鹼化學試劑'] [0, 3] [] []\n",
      "['外層', '材質'] [0, 5] ['採用', '透明'] [2, 3]\n",
      "['氮氣'] [13] ['供泄', '監視', '管', '通入', '稀釋'] [0, 2, 7, 9, 11]\n",
      "['廢氣', '系統'] [0, 5] ['排放'] [2]\n",
      "['有機溶劑'] [3] [] []\n",
      "['於酸', '鹼化學試劑', '機溶劑', '人體'] [1, 4, 7, 10] ['有', '會', '造成'] [6, 8, 11]\n",
      "['系統'] [6] ['應有', '監視'] [1, 5]\n"
     ]
    }
   ],
   "source": [
    "# get entity pairs and relations\n",
    "\n",
    "relation_pos_possible_list = [\"v\"]\n",
    "\n",
    "entity_pos_possible_list = [\"nr\", \"n\"]\n",
    "\n",
    "\n",
    "# get relations\n",
    "all_relations_list = []\n",
    "all_relations_index = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    relation_list = []\n",
    "    relation_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1: \n",
    "            if total_label_list[sentenceIndex][tokenIndex] in relation_pos_possible_list:\n",
    "                relation_list.append(token)\n",
    "                relation_index_list.append(tokenIndex)\n",
    "    all_relations_list.append(relation_list)\n",
    "    all_relations_index.append(relation_index_list)\n",
    "\n",
    "\n",
    "# get entities\n",
    "all_reformatted_entities = []\n",
    "all_reformatted_index_list = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    entity_index_list = []\n",
    "    possible_entities = []\n",
    "    reformatted_entities = []\n",
    "    reformatted_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1:\n",
    "            if total_label_list[sentenceIndex][tokenIndex] in entity_pos_possible_list:\n",
    "                entity_index_list.append(tokenIndex)\n",
    "                possible_entities.append(token)\n",
    "    # combine token if situate next to each other\n",
    "#     print(entity_index_list, possible_entities)\n",
    "    if len(possible_entities) > 0:\n",
    "        combine_entity_name = possible_entities[0]\n",
    "        for possibleIndex, possibleElement in enumerate(entity_index_list):\n",
    "#             print(possible_entities)\n",
    "            \n",
    "            isContinuous = False\n",
    "            if possibleIndex != 0:\n",
    "                if possibleElement == entity_index_list[possibleIndex - 1] + 1:\n",
    "                    isContinuous = True\n",
    "                    combine_entity_name += possible_entities[possibleIndex]\n",
    "                else:\n",
    "                    isContinuous = False\n",
    "            \n",
    "                if isContinuous == False:\n",
    "                    reformatted_entities.append(combine_entity_name)\n",
    "                    reformatted_index_list.append(entity_index_list[possibleIndex-1])\n",
    "                    combine_entity_name = possible_entities[possibleIndex]\n",
    "                    \n",
    "            if possibleIndex == (len(entity_index_list) - 1):\n",
    "                reformatted_entities.append(combine_entity_name)\n",
    "                reformatted_index_list.append(possibleElement)\n",
    "                \n",
    "#             print(combine_entity_name)     \n",
    "#     print(reformatted_entities)\n",
    "    all_reformatted_entities.append(reformatted_entities)\n",
    "    all_reformatted_index_list.append(reformatted_index_list)\n",
    "                \n",
    "        \n",
    "# print(all_relations_list, len(all_relations_list), \"\\n\")\n",
    "# print(all_reformatted_entities, len(all_reformatted_entities), )\n",
    "print(len(all_relations_list), len(all_relations_index), len(all_reformatted_entities), len(all_reformatted_index_list))\n",
    "\n",
    "# print out parsing result\n",
    "for index, element in enumerate(all_reformatted_entities):\n",
    "    print(element, all_reformatted_index_list[index], all_relations_list[index], all_relations_index[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-black",
   "metadata": {},
   "source": [
    "# spaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "charged-paragraph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list length:  24 24 24\n",
      "['半導', '體工', '業常', '用', '的', '化學', '試劑', '可', '分為', '酸'] ['NOUN', 'NOUN', 'NOUN', 'VERB', 'PART', 'NOUN', 'NOUN', 'VERB', 'VERB', 'NOUN'] ['compound:nn', 'compound:nn', 'dep', 'acl', 'mark', 'compound:nn', 'nsubj', 'aux:modal', 'ROOT', 'conj']\n",
      "list length:  24 24 24\n"
     ]
    }
   ],
   "source": [
    "# devide token by seperators\n",
    "total_entity_list = []\n",
    "sentence_entity_list = []\n",
    "total_label_list = []\n",
    "sentence_label_list = []\n",
    "total_dependencies_list = []\n",
    "sentence_dependencies_list = []\n",
    "\n",
    "for index, entityElement in enumerate(dataToken.iloc[:, 0]):\n",
    "    if entityElement not in [\"，\", \"。\", \"！\", \"!\", \"？\", \"?\"]:\n",
    "        sentence_entity_list.append(entityElement)\n",
    "        sentence_label_list.append(dataToken.iloc[index, 1])\n",
    "        sentence_dependencies_list.append(dataToken.iloc[index, 2])\n",
    "    else:\n",
    "        total_entity_list.append(sentence_entity_list)\n",
    "        total_label_list.append(sentence_label_list)\n",
    "        total_dependencies_list.append(sentence_dependencies_list)\n",
    "        sentence_entity_list = []\n",
    "        sentence_label_list = []\n",
    "        sentence_dependencies_list = []\n",
    "    \n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))\n",
    "\n",
    "\n",
    "# Clean up tokens and eliminate unneccesary tokens\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    indexPunct_0 = 0\n",
    "    indexPunct_1 = 0\n",
    "    for tokenIndex, tokens in enumerate(sentences):\n",
    "        # set up element indexes that are between two punctuation\n",
    "        if total_label_list[sentenceIndex][tokenIndex] in [\"PUNCT\"]:\n",
    "            if indexPunct_0 == 0:\n",
    "                indexPunct_0 = tokenIndex\n",
    "            else:\n",
    "                indexPunct_1 = tokenIndex\n",
    "    # remove tokens that are between two punctuations\n",
    "    if indexPunct_1 == 0:\n",
    "        total_entity_list[sentenceIndex][indexPunct_1] = \"\"\n",
    "        total_label_list[sentenceIndex][indexPunct_1] = \"\"\n",
    "        total_dependencies_list[sentenceIndex][indexPunct_1] = \"\"\n",
    "    else:\n",
    "        for removeIndex in range(indexPunct_0, indexPunct_1 + 1):\n",
    "            total_entity_list[sentenceIndex][removeIndex] = \"\"\n",
    "            total_label_list[sentenceIndex][removeIndex] = \"\"\n",
    "            total_dependencies_list[sentenceIndex][removeIndex] = \"\"\n",
    "            \n",
    "    total_entity_list[sentenceIndex] = list(filter((\"\").__ne__, total_entity_list[sentenceIndex]))\n",
    "    total_label_list[sentenceIndex]  = list(filter((\"\").__ne__, total_label_list[sentenceIndex]))\n",
    "    total_dependencies_list[sentenceIndex] = list(filter((\"\").__ne__, total_dependencies_list[sentenceIndex]))\n",
    "                \n",
    "print(total_entity_list[0] , total_label_list[0], total_dependencies_list[0])\n",
    "print(\"list length: \", len(total_entity_list), len(total_label_list), len(total_dependencies_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "extraordinary-rally",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 24 24 24\n",
      "['半導體工業常', '化學試劑', '酸'] [2, 6, 9] ['用', '可', '分為'] [3, 7, 8]\n",
      "['生產過程'] [4] ['用於', '過程'] [0, 4]\n",
      "['IC技術', '發展'] [1, 4] ['發展'] [4]\n",
      "['當今', '線寬', '.10', '奈米電子時代'] [0, 3, 8, 13] ['邁進', '小', '於', '0'] [2, 4, 5, 6]\n",
      "['矽晶圓單晶拋光片', '表面加工質量', '要求', '來'] [3, 7, 9, 12] ['高'] [14]\n",
      "['學試劑', '技術要求'] [5, 11] ['用', '提出', '高'] [2, 6, 8]\n",
      "['其顆粒', '雜質', '含量', '量級'] [1, 3, 5, 14] ['一般', '要', '減少'] [7, 8, 9]\n",
      "['其包裝', '要求'] [2, 6] [] []\n",
      "['矽晶圓片', '化學清洗製程', '度化學試劑', '標準', '技術指標見表4.2-2'] [1, 5, 12, 14, 21] ['用', '指標', '所示'] [7, 18, 22]\n",
      "['標準'] [1] ['標準'] [1]\n",
      "['標準', '試劑', '1.2', '線寬電路生產'] [1, 3, 6, 12] ['適用', '生產'] [4, 12]\n",
      "['標準', '試劑', '0.6', '0.2', '線寬電路製程'] [1, 3, 6, 8, 12] ['適用', '製程'] [4, 12]\n",
      "['標準', '試劑', '0.2', '0.', '線寬電路製程'] [1, 3, 6, 9, 14] ['適用', '製程'] [4, 14]\n",
      "['導體工廠', '化學試劑'] [3, 6] ['用', '於半', '工廠', '用', '較大'] [0, 1, 3, 7, 10]\n",
      "['系統', '污染'] [2, 4] ['避免'] [1]\n",
      "['中央系統管理', '供應方式', '化學試劑', '供給系統'] [3, 6, 9, 12] ['採用'] [0]\n",
      "['酸材質', '雙層管路系統'] [1, 5] [] []\n",
      "['內層鹼化學試劑'] [3] [] []\n",
      "['外層', '材質管'] [0, 6] ['採用', '透明'] [2, 3]\n",
      "['監視', '用', '內外管', '氮氣'] [1, 3, 7, 14] ['供泄漏', '外管', '通入', '經', '稀釋'] [0, 7, 10, 11, 12]\n",
      "['廢氣', '排放管路系統'] [1, 5] ['至'] [0]\n",
      "['機溶劑常', '不銹鋼管', '輸送管路'] [5, 8, 11] ['用', '用', '作為'] [1, 6, 9]\n",
      "['於酸', '鹼化學試劑', '機溶劑會對人體', '傷害'] [0, 3, 9, 14] ['體', '造成', '大'] [9, 10, 12]\n",
      "['安全輸送系統', '泄漏監視系統'] [6, 11] ['故應', '有', '嚴格', '中應', '配置'] [0, 1, 2, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "# get entity pairs and relations\n",
    "\n",
    "relation_dependencies_possible_list = [\"ROOT\", \"nmod:prep\", \"prep\", \"agent\", ]\n",
    "relation_pos_possible_list = [\"VERB\"]\n",
    "\n",
    "entity_dependencies_possible_list = [\"compound:nn\", \"nsubj\", \"dep\", \"dobj\"]\n",
    "entity_pos_possible_list = [\"NOUN\", \"PROPN\"]\n",
    "\n",
    "\n",
    "# get relations\n",
    "all_relations_list = []\n",
    "all_relations_index = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    relation_list = []\n",
    "    relation_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1: \n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in relation_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in relation_pos_possible_list:\n",
    "                relation_list.append(token)\n",
    "                relation_index_list.append(tokenIndex)\n",
    "    all_relations_list.append(relation_list)\n",
    "    all_relations_index.append(relation_index_list)\n",
    "\n",
    "\n",
    "# get entities\n",
    "all_reformatted_entities = []\n",
    "all_reformatted_index_list = []\n",
    "for sentenceIndex, sentences in enumerate(total_entity_list):\n",
    "    entity_index_list = []\n",
    "    possible_entities = []\n",
    "    reformatted_entities = []\n",
    "    reformatted_index_list = []\n",
    "    for tokenIndex, token in enumerate(sentences):\n",
    "        if len(sentences) > 1:\n",
    "            if total_dependencies_list[sentenceIndex][tokenIndex] in entity_dependencies_possible_list or\\\n",
    "            total_label_list[sentenceIndex][tokenIndex] in entity_pos_possible_list:\n",
    "                entity_index_list.append(tokenIndex)\n",
    "                possible_entities.append(token)\n",
    "    # combine token if situate next to each other\n",
    "#     print(entity_index_list, possible_entities)\n",
    "    if len(possible_entities) > 0:\n",
    "        combine_entity_name = possible_entities[0]\n",
    "        for possibleIndex, possibleElement in enumerate(entity_index_list):\n",
    "#             print(possible_entities)\n",
    "            \n",
    "            isContinuous = False\n",
    "            if possibleIndex != 0:\n",
    "                if possibleElement == entity_index_list[possibleIndex - 1] + 1:\n",
    "                    isContinuous = True\n",
    "                    combine_entity_name += possible_entities[possibleIndex]\n",
    "                else:\n",
    "                    isContinuous = False\n",
    "            \n",
    "                if isContinuous == False:\n",
    "                    reformatted_entities.append(combine_entity_name)\n",
    "                    reformatted_index_list.append(entity_index_list[possibleIndex-1])\n",
    "                    combine_entity_name = possible_entities[possibleIndex]\n",
    "                    \n",
    "            if possibleIndex == (len(entity_index_list) - 1):\n",
    "                reformatted_entities.append(combine_entity_name)\n",
    "                reformatted_index_list.append(possibleElement)\n",
    "                \n",
    "#             print(combine_entity_name)     \n",
    "#     print(reformatted_entities)\n",
    "    all_reformatted_entities.append(reformatted_entities)\n",
    "    all_reformatted_index_list.append(reformatted_index_list)\n",
    "                \n",
    "        \n",
    "# print(all_relations_list, len(all_relations_list), \"\\n\")\n",
    "# print(all_reformatted_entities, len(all_reformatted_entities), )\n",
    "print(len(all_relations_list), len(all_relations_index), len(all_reformatted_entities), len(all_reformatted_index_list))\n",
    "\n",
    "# print out parsing result\n",
    "for index, element in enumerate(all_reformatted_entities):\n",
    "    print(element, all_reformatted_index_list[index], all_relations_list[index], all_relations_index[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-uncle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monpa",
   "language": "python",
   "name": "monpa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
